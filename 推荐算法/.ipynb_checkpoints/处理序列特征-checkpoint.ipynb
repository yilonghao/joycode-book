{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cdea23",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Base-Model\" data-toc-modified-id=\"Base-Model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Base Model</a></span></li><li><span><a href=\"#DIN(Deep-Interest-Network)\" data-toc-modified-id=\"DIN(Deep-Interest-Network)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>DIN(Deep Interest Network)</a></span><ul class=\"toc-item\"><li><span><a href=\"#DIN核心思想\" data-toc-modified-id=\"DIN核心思想-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>DIN核心思想</a></span></li><li><span><a href=\"#mini-batch-aware-regularization(MBAR)\" data-toc-modified-id=\"mini-batch-aware-regularization(MBAR)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>mini-batch aware regularization(MBAR)</a></span></li><li><span><a href=\"#自适应激活函数Dice\" data-toc-modified-id=\"自适应激活函数Dice-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>自适应激活函数Dice</a></span></li></ul></li><li><span><a href=\"#DIEN\" data-toc-modified-id=\"DIEN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>DIEN</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型结构\" data-toc-modified-id=\"模型结构-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>模型结构</a></span></li><li><span><a href=\"#Interest-Extractor-Layer\" data-toc-modified-id=\"Interest-Extractor-Layer-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Interest Extractor Layer</a></span></li><li><span><a href=\"#辅助损失函数\" data-toc-modified-id=\"辅助损失函数-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>辅助损失函数</a></span></li><li><span><a href=\"#Interest-Evolving-Layer\" data-toc-modified-id=\"Interest-Evolving-Layer-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Interest Evolving Layer</a></span></li><li><span><a href=\"#将attention机制融入GRU结构中(AUGRU)\" data-toc-modified-id=\"将attention机制融入GRU结构中(AUGRU)-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>将attention机制融入GRU结构中(AUGRU)</a></span></li></ul></li><li><span><a href=\"#DSIN\" data-toc-modified-id=\"DSIN-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>DSIN</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型结构（整体）\" data-toc-modified-id=\"模型结构（整体）-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>模型结构（整体）</a></span></li><li><span><a href=\"#embedding\" data-toc-modified-id=\"embedding-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>embedding</a></span></li><li><span><a href=\"#建模用户序列\" data-toc-modified-id=\"建模用户序列-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>建模用户序列</a></span><ul class=\"toc-item\"><li><span><a href=\"#session-division-layer\" data-toc-modified-id=\"session-division-layer-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>session division layer</a></span></li><li><span><a href=\"#session-interest-extractor-layer\" data-toc-modified-id=\"session-interest-extractor-layer-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>session interest extractor layer</a></span></li><li><span><a href=\"#Session-Interest-Interacting-Layer\" data-toc-modified-id=\"Session-Interest-Interacting-Layer-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Session Interest Interacting Layer</a></span></li><li><span><a href=\"#session-interest-activating-layer\" data-toc-modified-id=\"session-interest-activating-layer-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>session interest activating layer</a></span></li><li><span><a href=\"#session-interest-extractor-layer-相关-activation-unit\" data-toc-modified-id=\"session-interest-extractor-layer-相关-activation-unit-4.3.5\"><span class=\"toc-item-num\">4.3.5&nbsp;&nbsp;</span>session interest extractor layer 相关 activation unit</a></span></li><li><span><a href=\"#session-interest-interacting-layer-相关-activation-unit\" data-toc-modified-id=\"session-interest-interacting-layer-相关-activation-unit-4.3.6\"><span class=\"toc-item-num\">4.3.6&nbsp;&nbsp;</span>session interest interacting layer 相关 activation unit</a></span></li></ul></li><li><span><a href=\"#MLP\" data-toc-modified-id=\"MLP-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>MLP</a></span></li><li><span><a href=\"#实验数据\" data-toc-modified-id=\"实验数据-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>实验数据</a></span></li></ul></li><li><span><a href=\"#京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender\" data-toc-modified-id=\"京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a></span><ul class=\"toc-item\"><li><span><a href=\"#简介\" data-toc-modified-id=\"简介-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href=\"#Deep-Multifaceted-Transformers-Layer\" data-toc-modified-id=\"Deep-Multifaceted-Transformers-Layer-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Deep Multifaceted Transformers Layer</a></span></li><li><span><a href=\"#Bias-Deep-Neural-Network\" data-toc-modified-id=\"Bias-Deep-Neural-Network-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Bias Deep Neural Network</a></span></li><li><span><a href=\"#Multi-gate-Mixture-of-Experts-Layers\" data-toc-modified-id=\"Multi-gate-Mixture-of-Experts-Layers-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Multi-gate Mixture-of-Experts Layers</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad2e6b",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "\n",
    "传统基于用户行为序列描述用户兴趣的方法是把序列中各个item的embedding通过pooling的方式转化成一个固定维的embedding。如下图所示，其中红色节点表示商品ID，蓝色节点表示店铺iD，粉色节点表示类目ID，白色节点表示用户特征和上下文特征，Goods 1 ~ Goods N 用来描述用户的历史行为，候选广告（Candidate Ad）本身也是商品。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/dec039c0-ee95-4d4d-9246-4038e11812f2.png\" alt=\"image\" style=\"zoom: 33%;\" />\n",
    "\n",
    "网络结构具有的缺点是经过pooling以后的向量与候选广告无关，对于一个用户来说是固定不变的。对于不同的候选广告，与之对应的用户兴趣分布也应该不通。因为只有用户部分的兴趣会影响当前行为（对候选广告点击或不点击）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95366c34",
   "metadata": {},
   "source": [
    "## DIN(Deep Interest Network)\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/9223919b-9c70-4403-a519-54a0b10e8b3b.png\" alt=\"image\" style=\"zoom: 67%;\" />\n",
    "\n",
    "### DIN核心思想\n",
    "\n",
    "DIN的核心思想是，对每一个用户，不同的广告有不同的向量表示，结合用户行为特征与给定的广告为每个用户行为计算权重，引入local-activation机制有侧重的利用用户不同的行为特征，其中$e_j$表示用户行为向量，$v_A$为候选广告的向量，a表示激活单元，$a(e_j,v_A)$为权重。整体逻辑为SUM Pooling。\n",
    "\n",
    "![image](https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/c2eb0b1e-f6a3-4cfd-99ad-3b8537a0963b.png)\n",
    "\n",
    "相比基础的深度推荐网络，DIN在生成用户向量的时候加了一个activation unit单元，计算每个用户行为与候选广告之间的权重。在传统的attention机制中，给定两个向量，比如u和v，通常直接做点乘。这篇论文中做了进一步的改进：首先是把u和v以及uv的外积合并起来作为输入给全连接层，最后得到权重，这样可以减少信息损失。论文中还放宽了权重加和等于1的限制，这样更有利于体现用户行为特征之间的差异化程度。\n",
    "\n",
    "### mini-batch aware regularization(MBAR)\n",
    "\n",
    "为解决大规模稀疏场景下，采用SGD对引入L2正则的loss进行更新时计算开销过大的问题，该方法只对每一个mini-batch中参数不为0的进行梯度更新。\n",
    "\n",
    "### 自适应激活函数Dice\n",
    "\n",
    "PRelu激活函数如下所示：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/5f30f50e-8c69-4212-8692-940457ec8b7a.png\" style=\"zoom: 67%;\" />\n",
    "\n",
    "采用PRelu激活函数时，它的rectified point固定为0，这在每一层的输入分布发生变化时是不适用的，所以文章对该激活函数做了改进，平滑了rectified point附近曲线的同时，激活函数会根据每层输入数据的分布来自适应调整rectified point的位置，具体形式如下：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ccac1f77-49db-4cc2-ab87-186e5637c1f2.png\" style=\"zoom: 67%;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/81556967-3897-407e-8496-2d98aa7bc705.png\" style=\"zoom: 50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f365daf",
   "metadata": {},
   "source": [
    "## DIEN\n",
    "\n",
    "阿里妈妈的精准定向检索及基础算法团队以 Deep Interest Network(DIN) 算法为基础，进一步优化升级，产出了 Deep Interest Evolution Network(DIEN)算法，主要解决了以下两个问题：\n",
    "\n",
    "- 更加精确的刻画用户的长期兴趣和短期兴趣\n",
    "- 用户的兴趣是时刻变化的，需要更加准确的刻画用户兴趣的变化\n",
    "\n",
    "### 模型结构\n",
    "如图所示，DIEN 由 embedding 层、Interest Extractor Layer 层，Interest Evolving Layer 层组成。Interest Extractor Layer 根据行为序列提取兴趣序列，Interest Evolving Layer 基于 Target Ad 对兴趣序列进行建模，得到用户兴趣向量表示，最终将用户兴趣向量表示与其他特征向量 concat 到一起后送到 MLP 中以进行最终预测。\n",
    "\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1c958050-4694-4a23-8ca2-ac929d53453a.png\" style=\"zoom:50%;\" />\n",
    "\n",
    "\n",
    "### Interest Extractor Layer\n",
    "\n",
    "兴趣提取层的目的就是从用户的行为序列中提取出一系列的兴趣状态。为了平衡效率和性能，作者选择了GRU（Gated Recurrent Unit，门循环单元）网络来对用户行为之间的依赖进行建模。GRU既能够克服RNN梯度消失的问题，同时又比LSTM网络具有更少的参数，训练时收敛速度更快。GRU单元的表达式如下：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/225b086d-f739-42b0-bcb6-b5a1d6a2cbae.png\" style=\"zoom: 33%;\" />\n",
    "\n",
    "其中 $\\sigma$表示sigmoid激活函数，$\\circ$表示元素积(element-wise product)，$W$和$U$表示隐藏层参数，$i_t$表示GRU单元的输入（用户第t个行为的embedding向量），$h_{t}$表示第t个GRU单元的隐层状态。关于GRU结构的介绍可以参考文章：[GRU学习笔记](https://zhuanlan.zhihu.com/p/463521382)，主结构如下：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6ee03222-480d-457c-9509-a6270d8a4223.png\" style=\"zoom: 33%;\" />\n",
    "\n",
    "### 辅助损失函数\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a3592fc6-81c0-4461-b6fd-e98f90109bb6.png\" style=\"zoom:50%;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/44c87c81-7b82-4438-9b8c-3fddbdd69173.png\" alt=\"辅助Loss计算示意图\" style=\"zoom:50%;\" />\n",
    "\n",
    "其中 $\\{e_{b}^{i},\\hat e_{b}^{i}\\} \\in D_{\\beta},i=1,2,3,...,N$表示N对行为embedding序列，$e_{b}^{i} \\in \\mathbb{R}^{T  \\times n_E}$表示用户的点击行为序列，$\\hat e_{b}^{i}\\in \\mathbb{R}^{T  \\times n_E}$表示用户没有点击的行为序列。$T$表示序列中历史行为的数量，$n_E$表示embedding向量的维度。$e_{b}^{i}[t]$表示第i个用户第t次点击行为的embedding向量。其中：\n",
    "$$\n",
    "\\sigma(x_1, x_2) = \\frac{1}{exp(-[x_1, x_2])}\n",
    "$$\n",
    "\n",
    "DIEN使用的整体损失函数是：\n",
    "\n",
    "$$\n",
    "L = L_{target} + \\alpha * L_{aux}\n",
    "$$\n",
    "\n",
    "### Interest Evolving Layer\n",
    "\n",
    "兴趣进化层Interest Evolution Layer的主要目标是刻画用户兴趣的进化过程。兴趣进化层的数据就是兴趣提取层的输出，兴趣进化层结合了注意力机制中的局部激活能力和GRU的序列学习能力来建模。attention部分系数计算方式如下：\n",
    "$$\n",
    "a_t = \\frac{exp(h_{t}W_{e_{a}})}{\\sum_{j=1}^Texp(h_{j}W_{e_{a}})}\n",
    "$$\n",
    "\n",
    "### 将attention机制融入GRU结构中(AUGRU)\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/71b8130c-bdf8-4cdf-86f4-e5eccfcdd689.png\" style=\"zoom:50%;\" />\n",
    "\n",
    "通过将attention部分的计算结果作用到GRU结构的更新门上将attention机制融入到GRU结构中。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/664e6fa8-e51d-4fc0-8077-3c26c81319f6.png\" style=\"zoom: 33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5f143",
   "metadata": {},
   "source": [
    "## DSIN\n",
    "\n",
    "这篇文章主要介绍阿里在2019年发表的排序阶段模型：Deep Session Interest Network for Click-Through Rate Prediction。主要思路为将用户的历史点击行为划分为不同session，而后通过Transformer结构学习每个session的向量表示，最后通过BiLSTM结构对session序列进行建模，整体来说具有一定的参考意义。\n",
    "\n",
    "### 模型结构（整体）\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/11a806f7-3775-44b0-83db-77a199ef130d.png\" style=\"zoom:50%;\" />\n",
    "\n",
    "### embedding\n",
    "\n",
    "从图中可以看出模型主要由三大部分组成，其中第一部分（part1）主要处理用户画像特征（年龄、性别、所在城市等）和item侧特征（seller id, brand id等），处理方式也很简单，所有特征通过embedding的方式得到对应的向量表示，最终用户侧特征向量表示为$X^U \\in \\mathbb{R}^{N_u \\times d_{model}}$，其中$N_u$表示用户侧特征个数，item侧特征的向量表示为：$X^I \\in \\mathbb{R}^{N_i \\times d_{model}}$，其中$N_i$表示item侧特征个数。\n",
    "\n",
    "### 建模用户序列\n",
    "\n",
    "这一部分（part2）主要用来建模用户行为序列，主要包含4部分：\n",
    "\n",
    "- session division layer: 对用户行为序列进行划分\n",
    "- session interest extractor layer: 学习每个session的向量表示\n",
    "- session interest interacting layer: 对session序列进行建模\n",
    "- session interest activating layer: 引入attention机制\n",
    "\n",
    "#### session division layer\n",
    "\n",
    "主要作用就是将用户的历史点击行为序列划分为多个sessions。整个操作的符号表示为将用户的行为序列$S$划分为sessions $Q$，第$k$个session的表示形式为：\n",
    "\n",
    "$$\n",
    "Q_k = [b_1; ...; bi; ...; b_T] \\in \\mathbb{R}^{T \\times d_{model}}\n",
    "$$\n",
    "\n",
    "其中$T$为当前session对应的序列长度，$b_i$为用户在当前session中的第$i$次点击行为，文中session的划分是按照时间间隔来的，两次点击之间的间隔超过30min则将下一次点击行为计入下一个session。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/23182b90-adff-4fb1-8841-b0b18e78a747.jpg\" style=\"zoom:50%;\" />\n",
    "\n",
    "#### session interest extractor layer\n",
    "\n",
    "这部分的主要作用就是学习每个session的向量表示，文中使用了multi-head self-attention的结构对每个session建模。为了刻画不同session间的顺序，DSIN使用了Bias Encoding，其中$BE \\in \\mathbb{R}^{K \\times T \\times d_{model}}$。Bias Encoding 计算方式如下：\n",
    "\n",
    "$$\n",
    "BE_{(k,t,c)} = w_k^K + w_t^T + w_c^C\n",
    "$$\n",
    "\n",
    "其中$BE_{(k,t,c)}$表示第k个session中的第t个物品的embedding向量中的第c个位置对应的bias，加入bias encoding后，用户的session表示为：\n",
    "\n",
    "$$\n",
    "Q=Q+BE\n",
    "$$\n",
    "\n",
    "Multi-head Self-attention的计算逻辑为：\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0a020f15-557f-48a9-b49f-cd177c1af371.png\" style=\"zoom: 50%;\" />\n",
    "$$\n",
    "I_k^Q = FFN(Concat(head_1, ..., head_H)W^O)\n",
    "$$\n",
    "\n",
    "$$\n",
    "I_k = Avg(I_k^Q)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Session Interest Interacting Layer\n",
    "\n",
    "这部分主要通过Bi-LSTM结构对session序列进行建模。\n",
    "\n",
    "$$\n",
    "H_t = \\mathop{h_{ft}}\\limits ^{\\rightarrow} \\oplus \\mathop{h_{bt}}\\limits ^{\\leftarrow}\n",
    "$$\n",
    "\n",
    "其中$\\mathop{h_{ft}}\\limits ^{\\rightarrow}$表示forward隐藏层状态，$\\mathop{h_{bt}}\\limits ^{\\leftarrow}$表示backward隐藏层状态。\n",
    "\n",
    "#### session interest activating layer\n",
    "\n",
    "这部分主要是通过attention机制刻画目标item和session之间的相关性。主要思想是若session与目标item之间的相关性越高，则应该赋予越大的权重。模型结构中共有两个Activation Unit，结构是一致的。\n",
    "\n",
    "#### session interest extractor layer 相关 activation unit\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0f1a1c33-3507-4375-8fb3-8a7640b1d777.png\" style=\"zoom: 33%;\" />\n",
    "\n",
    "#### session interest interacting layer 相关 activation unit\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a5717872-e0b2-4240-8d6a-80b5d51b4183.png\" style=\"zoom: 33%;\" />\n",
    "\n",
    "### MLP\n",
    "\n",
    "这一部分（part3）为简单的MLP结构，首先将得到的所有向量表示concat到一起，而后通过2层前馈神经网络，最后通过一个softmax层得到输出。损失函数为：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/4e742bb1-d068-47c4-93c3-46351e119807.png\" style=\"zoom:33%;\" />\n",
    "\n",
    "其中 $\\mathbb{D}$表示训练集，$y \\in \\{0, 1\\}$，$p(\\cdot)$表示网络最终的输出结果，表示用户点击目标item的概率。\n",
    "\n",
    "### 实验数据\n",
    "\n",
    "从实验结果来看，AUC相比其他模型均有一定幅度的提升。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/f67a91a0-f19b-47b6-95ba-a1ae66ae1a0c.png\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5beb2",
   "metadata": {},
   "source": [
    "## 京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender\n",
    "\n",
    "\n",
    "> 推荐阅读⭐️⭐️⭐️⭐️⭐️\n",
    "> - paper: Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender\n",
    "> - [论文《Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender》](https://blog.csdn.net/whgyxy/article/details/126023948)\n",
    "> - [ctr预估：DMT模型](https://zhuanlan.zhihu.com/p/534414021)\n",
    "> - [详解谷歌之多任务学习模型MMoE(KDD 2018)](https://zhuanlan.zhihu.com/p/55752344)\n",
    "\n",
    "### 简介\n",
    "\n",
    "模型应用的业务场景商品搜索排序阶段，论文提出用多个Transfomer对用户多种类型的行为序列进行建模，在此基础上叠加MMOE建模多目标，最后使用一个消偏塔对数据进行消偏。\n",
    "\n",
    "![image-20221231142523288](https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221231142523288-2501956.png)\n",
    "\n",
    "### Deep Multifaceted Transformers Layer\n",
    "\n",
    "Embedding Layer对每个物料使用物料id、类目id、品牌id、商铺id分别映射成低维向量，然后concat起来，形成向量；Dense特征使用了Z-score归一化。模型使用了点击，加购，成交3个Item Sequence，分别表征短期，中期和长期兴趣；分别用3个Transformer来对点击、加入购物车、购买行为序列进行建模：\n",
    "\n",
    "- encoder：用序列的item-embedding作为self-attention的输入\n",
    "- decoder：使用target item的embedding作为query，encoder输出的结果作为key和value。\n",
    "\n",
    "### Bias Deep Neural Network\n",
    "\n",
    "Bais塔的输入都是偏差相关的特征，对于位置偏差输入就是展示位置索引编号或者网页索引编号；对于近邻偏差，输入就是目标物料的类目和邻近K个物料的类目。Bias建模部分使用Bias特征+MLP，输出的Logits与主网络Logits相加。\n",
    "\n",
    "### Multi-gate Mixture-of-Experts Layers\n",
    "\n",
    "多任务建模部分使用MMoE结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675dfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
