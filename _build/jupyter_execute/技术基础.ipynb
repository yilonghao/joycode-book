{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01952e22",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#技术基础\" data-toc-modified-id=\"技术基础-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>技术基础</a></span><ul class=\"toc-item\"><li><span><a href=\"#beam-search\" data-toc-modified-id=\"beam-search-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>beam search</a></span><ul class=\"toc-item\"><li><span><a href=\"#简介\" data-toc-modified-id=\"简介-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href=\"#贪心搜索(greedy-search)\" data-toc-modified-id=\"贪心搜索(greedy-search)-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>贪心搜索(greedy search)</a></span></li><li><span><a href=\"#集束搜索(beam-search)\" data-toc-modified-id=\"集束搜索(beam-search)-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>集束搜索(beam search)</a></span></li></ul></li><li><span><a href=\"#SimHash\" data-toc-modified-id=\"SimHash-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>SimHash</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>TF-IDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#词频-TF-计算方式\" data-toc-modified-id=\"词频-TF-计算方式-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>词频 TF 计算方式</a></span></li><li><span><a href=\"#逆文档频率-IDF-计算方式\" data-toc-modified-id=\"逆文档频率-IDF-计算方式-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>逆文档频率 IDF 计算方式</a></span></li><li><span><a href=\"#TF-IDF-计算方式\" data-toc-modified-id=\"TF-IDF-计算方式-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>TF-IDF 计算方式</a></span></li><li><span><a href=\"#代码实现\" data-toc-modified-id=\"代码实现-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>代码实现</a></span></li></ul></li><li><span><a href=\"#GRU\" data-toc-modified-id=\"GRU-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>GRU</a></span><ul class=\"toc-item\"><li><span><a href=\"#梯度消失和长距离依赖问题\" data-toc-modified-id=\"梯度消失和长距离依赖问题-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>梯度消失和长距离依赖问题</a></span></li><li><span><a href=\"#GRU结构简介\" data-toc-modified-id=\"GRU结构简介-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>GRU结构简介</a></span><ul class=\"toc-item\"><li><span><a href=\"#reset-gate-$r_t$-和-update-gate-$z_t$\" data-toc-modified-id=\"reset-gate-$r_t$-和-update-gate-$z_t$-1.4.2.1\"><span class=\"toc-item-num\">1.4.2.1&nbsp;&nbsp;</span>reset gate $r_t$ 和 update gate $z_t$</a></span></li><li><span><a href=\"#当前时刻的新信息\" data-toc-modified-id=\"当前时刻的新信息-1.4.2.2\"><span class=\"toc-item-num\">1.4.2.2&nbsp;&nbsp;</span>当前时刻的新信息</a></span></li></ul></li></ul></li><li><span><a href=\"#L-2-Norm\" data-toc-modified-id=\"L-2-Norm-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>L-2 Norm</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Batch Normalization</a></span></li><li><span><a href=\"#交叉熵损失函数\" data-toc-modified-id=\"交叉熵损失函数-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>交叉熵损失函数</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义\" data-toc-modified-id=\"定义-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href=\"#通俗解释\" data-toc-modified-id=\"通俗解释-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>通俗解释</a></span></li></ul></li><li><span><a href=\"#Focal-Loss\" data-toc-modified-id=\"Focal-Loss-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Focal Loss</a></span><ul class=\"toc-item\"><li><span><a href=\"#简介\" data-toc-modified-id=\"简介-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href=\"#推导过程（可跳过该章节）\" data-toc-modified-id=\"推导过程（可跳过该章节）-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>推导过程（可跳过该章节）</a></span></li></ul></li><li><span><a href=\"#OneEpoch现象\" data-toc-modified-id=\"OneEpoch现象-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>OneEpoch现象</a></span></li><li><span><a href=\"#温度系数\" data-toc-modified-id=\"温度系数-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>温度系数</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hard样本\" data-toc-modified-id=\"Hard样本-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span>Hard样本</a></span></li><li><span><a href=\"#温度系数与Hard样本的具体关系\" data-toc-modified-id=\"温度系数与Hard样本的具体关系-1.10.2\"><span class=\"toc-item-num\">1.10.2&nbsp;&nbsp;</span>温度系数与Hard样本的具体关系</a></span></li><li><span><a href=\"#温度系数对预估值的影响\" data-toc-modified-id=\"温度系数对预估值的影响-1.10.3\"><span class=\"toc-item-num\">1.10.3&nbsp;&nbsp;</span>温度系数对预估值的影响</a></span></li><li><span><a href=\"#温度系数对loss的影响\" data-toc-modified-id=\"温度系数对loss的影响-1.10.4\"><span class=\"toc-item-num\">1.10.4&nbsp;&nbsp;</span>温度系数对loss的影响</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4da6a",
   "metadata": {},
   "source": [
    "# 技术基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2c8ca",
   "metadata": {},
   "source": [
    "## beam search\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. [Beam Search（集束搜索/束搜索）](https://www.cnblogs.com/sddai/p/10552592.html)\n",
    "> 2. [十分钟读懂Beam Search(1/2)](https://blog.csdn.net/hecongqing/article/details/105040105)\n",
    "> 3. [9.8. Beam Search](https://d2l.ai/chapter_recurrent-modern/beam-search.html)\n",
    "\n",
    "\n",
    "### 简介\n",
    "\n",
    "Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。这样减少了空间消耗，并提高了时间效率，但缺点就是有可能存在潜在的最佳方案被丢弃，因此Beam Search算法是不完全的，一般用于解空间较大的系统中。\n",
    "\n",
    "### 贪心搜索(greedy search)\n",
    "\n",
    "贪心搜索最为简单，每一个时间步都取出了条件概率最大一个结果。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/84824679-64f7-4731-9eda-3df61e02dc87.png\" style=\"zoom: 67%; display: block; margin: auto;\" />\n",
    "\n",
    "### 集束搜索(beam search)\n",
    "\n",
    "而beam search是对贪心策略一个改进。思路也很简单，就是稍微放宽一些考察的范围。在每一个时间步，不再只保留当前分数最高的1个输出，而是保留num_beams个。当num_beams=1时集束搜索就退化成了贪心搜索。\n",
    "\n",
    "下图是一个实际的例子，每个时间步有A、B、C、D、E共5种可能的输出，图中的num_beams=2，也就是说每个时间步都会保留到当前步为止条件概率最优的2个序列。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/fa985ef1-40bc-4745-9e8b-fd6b4976f796.png\" style=\"zoom: 67%;\" />\n",
    "\n",
    "- 第 1 个时间步，A和C是最优的两个序列，因此得到了两个结果`[A],[C]`，其他三个就被丢弃了；\n",
    "- 第 2 个时间步，得到10个候选`[AA],[AB],[AC],[AD],[AE],[CA],[CB],[CC],[CD],[CE]`，对这10个序列进行统一排名，再保留最优的两个序列，即`[AB]`和`[CE]`；\n",
    "- 第 3 个时间步，得到10个候选`[ABA],[ABB],[ABC],[ABD],[ABE],[CEA],[CEB],[CEC],[CED],[CEE]`，对这10个序列进行统一排名，再保留最优的两个序列，即`[ABD],[CED]`。\n",
    "\n",
    "可以发现，beam search在每一步需要考察的候选人数量是贪心搜索的num_beams倍，因此是一种牺牲时间换性能的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aacaf93",
   "metadata": {},
   "source": [
    "## SimHash\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. [5分钟搞懂LSH之SimHash算法原理](https://zhuanlan.zhihu.com/p/444198579)\n",
    "\n",
    "在许多场景中，都会遇到海量数据相似度计算的问题，如：电商场景中根据商品embedding计算相似度，取出相似的topk个商品。然而，这种计算相似度需要笛卡尔积的时间复杂度，在数据量较小时，时间还可以接受，但是当数据量达到几十万甚至几百几千万时，是没有办法接受的，这个时候就需要想其他办法。本文主要介绍海量item之间相似度计算问题——局部敏感哈希(Locality-Sensitive Hashing, LSH)之SimHash算法原理。\n",
    "\n",
    "假设有3个商品，即：item1、item2和item3，每个商品用二维的embedding来表示，同时随机初始化6个超平面，即：s1、s2、s3、s4、s5和s6，每个超平面也是一个二维的embedding，这时可以在二维平面直角坐标系下表示，如下图：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-ace449e4d4c8a00b8d12cabbc9232ff3_1440w.webp\" alt=\"img\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "接下来，我们让每个item分别与6个超平面进行向量点积（相似度计算的一种方式），如果结果大于0，则结果为1，否则结果为0。因此会有如下结果表格：\n",
    "\n",
    "|       | s1   | s2   | s3   | s4   | s5   | s6   | SimHash |\n",
    "| ----- | ---- | ---- | ---- | ---- | ---- | ---- | ------- |\n",
    "| item1 | 1    | 1    | 1    | 0    | 0    | 0    | 111000  |\n",
    "| item2 | 1    | 1    | 1    | 0    | 0    | 0    | 111000  |\n",
    "| item3 | 0    | 0    | 0    | 1    | 1    | 1    | 000111  |\n",
    "\n",
    "通过上面的表格，item1、item2和超平面s1、s2、s3的相似度（向量点积）大于0，对应表格中的值1；与s4、s5、s6的相似度小于0，对应表格中的值0。同理，item3和超平面s1、s2、s3的相似度小于0，对应表格中的值0；与s4、s5、s6的相似度大于0，对应表格中的值1。这时，把每个超平面叫做哈希函数，SimHash值是每个item与各个超平面向量点积后的二进制结果。我们发现item1与item2的SimHash值是一样的，而与item3的SimHash值不同。这时把SimHash值相同的放在一个桶里面，如下图：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-9de151911806b2032c081b2ce88dbb9d_1440w.jpeg\" alt=\"img\" style=\"zoom: 67%;\" />\n",
    "\n",
    "如果有几十万这样的item，SimHash算法计算后，每个桶都会有一定数量的item。这时计算item的topk个相似item时，只需要将此item与对应桶中其他item进行相似度计算，然后找到其topk个相似的item。下面是SimHash算法伪代码：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221027082521020.png\" alt=\"image-20221027082521020\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d4ffe",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF算法通常用于提取一篇文章的关键词，算法的核心思想比较简单，我们也可以加以拓展应用到推荐系统中。\n",
    "\n",
    "### 词频 TF 计算方式\n",
    "\n",
    "词频的计算方式有多种，比较常见的为方式一，在提取文章关键词的场景下，方式一和方式二是等价的（计算每个词的TF-IDF值，方式二相对于方式一相当于都除了同一个常数，而后按照TF-IDF值倒排取TOPN个词作为文章的关键词）。\n",
    "\n",
    "方式一：\n",
    "\n",
    "$$\n",
    "词频(TF) = 某个词在文章中出现的次数\n",
    "$$\n",
    "\n",
    "方式二：\n",
    "\n",
    "$$\n",
    "词频(TF) = \\frac{某个词在文章中出现的次数}{文章的总词数}\n",
    "$$\n",
    "\n",
    "### 逆文档频率 IDF 计算方式\n",
    "\n",
    "$$\n",
    "逆文档频率(IDF) = log_2(\\frac{语料库的文档总数}{包含该词的文档总数 + 1})\n",
    "$$\n",
    "\n",
    "### TF-IDF 计算方式\n",
    "\n",
    "$$\n",
    "TF\\text{-}IDF = 词频TF \\times 逆文档频率IDF\n",
    "$$\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd38d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "文档 1: The quick brown fox jumps over the lazy dog.\n",
      "词: the, TF-IDF值: 0.2222\n",
      "词: jumps, TF-IDF值: 0.1562\n",
      "词: quick, TF-IDF值: 0.1111\n",
      "词: brown, TF-IDF值: 0.1111\n",
      "词: fox, TF-IDF值: 0.1111\n",
      "词: over, TF-IDF值: 0.1111\n",
      "词: lazy, TF-IDF值: 0.1111\n",
      "词: dog, TF-IDF值: 0.0791\n",
      "\n",
      "文档 2: Never jump over the lazy dog quickly.\n",
      "词: never, TF-IDF值: 0.2008\n",
      "词: jump, TF-IDF值: 0.2008\n",
      "词: quickly, TF-IDF值: 0.2008\n",
      "词: over, TF-IDF值: 0.1429\n",
      "词: the, TF-IDF值: 0.1429\n",
      "词: lazy, TF-IDF值: 0.1429\n",
      "词: dog, TF-IDF值: 0.1018\n",
      "\n",
      "文档 3: A quick brown dog outpaces a quick fox.\n",
      "词: a, TF-IDF值: 0.3514\n",
      "词: quick, TF-IDF值: 0.2500\n",
      "词: outpaces, TF-IDF值: 0.1757\n",
      "词: brown, TF-IDF值: 0.1250\n",
      "词: fox, TF-IDF值: 0.1250\n",
      "词: dog, TF-IDF值: 0.0890\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog quickly.\",\n",
    "    \"A quick brown dog outpaces a quick fox.\"\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    \"\"\"预处理文档\"\"\"\n",
    "    return doc.lower().replace('.', '').split()\n",
    "\n",
    "\n",
    "def compute_tf(doc):\n",
    "    \"\"\"计算词频TF\"\"\"\n",
    "    tf_dict = defaultdict(int)\n",
    "    for word in doc:\n",
    "        tf_dict[word] += 1\n",
    "    doc_len = len(doc)\n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] /= float(doc_len)\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def compute_idf(docs):\n",
    "    \"\"\"计算逆文档频率IDF\"\"\"\n",
    "    N = len(docs)\n",
    "    idf_dict = defaultdict(int)\n",
    "    all_words = set(word for doc in docs for word in doc)\n",
    "    for word in all_words:\n",
    "        containing_docs = sum(1 for doc in docs if word in doc)\n",
    "        # math.log 计算出来的值可能会是负数，尤其是当词频很高时（例如，词出现在所有文档中）\n",
    "        # 添加1到最终的IDF值：避免负值并确保所有IDF值都是正的\n",
    "        idf_dict[word] = math.log(N / (1 + containing_docs)) + 1\n",
    "    return idf_dict\n",
    "\n",
    "\n",
    "def compute_tfidf(tf, idf):\n",
    "    \"\"\"计算TF-IDF值\"\"\"\n",
    "    tfidf = {}\n",
    "    for word, tf_val in tf.items():\n",
    "        tfidf[word] = tf_val * idf[word]\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "preprocessed_docs = [preprocess(doc) for doc in documents]\n",
    "tf_list = [compute_tf(doc) for doc in preprocessed_docs]\n",
    "idf = compute_idf(preprocessed_docs)\n",
    "tfidf_list = [compute_tfidf(tf, idf) for tf in tf_list]\n",
    "\n",
    "\n",
    "for i, tfidf in enumerate(tfidf_list):\n",
    "    print(f\"\\n文档 {i + 1}: {documents[i]}\")\n",
    "    for word, score in sorted(tfidf.items(), key=lambda item: item[1], reverse=True):\n",
    "        print(f\"词: {word}, TF-IDF值: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ef81a",
   "metadata": {},
   "source": [
    "## GRU\n",
    "\n",
    "### 梯度消失和长距离依赖问题\n",
    "\n",
    "RNN 中存在的梯度消失问题会导致难以学习到长距离依赖的问题。由于梯度消失问题的存在，越早的时刻对参数的修正起到的作用就越小，也就是说模型很难捕捉到长距离依赖关系。\n",
    "\n",
    "### GRU结构简介\n",
    "\n",
    "GRU 引入了 **reset gate** 和 **update gate**。其结构图如下，其中 $*$ 表示按位乘。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6ee03222-480d-457c-9509-a6270d8a4223-20221007224201070.png\" style=\"zoom: 33%;\" />\n",
    "\n",
    "#### reset gate $r_t$ 和 update gate $z_t$\n",
    "\n",
    "reset gate 用来控制计算当前时刻的新信息时，保留多少之前的记忆。举个例子来说明一下，假设每个时刻输入的是一个词的话，那么如果 $r_t$ 为 0，那么 $\\widetilde{h}_t$ 中就会只包含当前词的信息。\n",
    "\n",
    "$$\n",
    "{r}_t = \\sigma (W_r \\cdot [h_{t-1}, x_t])\n",
    "$$\n",
    "\n",
    "update gate 控制需要从前一时刻的隐藏层状态 $h_{t-1}$ 中忘记多少信息，同时控制需要将多少当前时刻的新信息加入到隐藏层状态中。\n",
    "\n",
    "$$\n",
    "{z}_t = \\sigma (W_z \\cdot [h_{t-1}, x_t])\n",
    "$$\n",
    "\n",
    "reset gate 允许模型丢弃一些和未来无关的信息，如果reset gate接近0，那么之前的隐藏层信息就会丢弃。update gate 控制当前时刻的隐藏层输出 $h_t$ 需要保留多少之前的隐藏层信息，若 $z_t$ 接近于 1，相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。 一般来说那些具有短距离依赖的单元 reset gate 比较活跃，具有长距离依赖的单元 update gate 比较活跃。\n",
    "\n",
    "#### 当前时刻的新信息\n",
    "\n",
    "接下来计算当前时刻的新信息（candidate values, $\\widetilde{h}_t$）。这跟 LSTM 中的 candidates values($\\widetilde{C}_t$) 是类似的。计算方式如下：\n",
    "\n",
    "$$\n",
    "\\widetilde{h}_t = tanh(W_h \\cdot [r_t \\circ h_{t-1}, x_t])\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837408bc",
   "metadata": {},
   "source": [
    "## L-2 Norm\n",
    "\n",
    "**L-2 Norm的作用是把embedding的长度都归一化为1，也就是说把它们都映射到一个长度为1的单位超球面上去。如果把它投影到单位超平面上，会增加训练稳定性和投影空间的线性可分性**。增加线性可分性，意思也就是说你用简单算法也能得到比较好的效果。试想一下，一个单位超球面和一个不规则球面的向量空间，是不是前者更容易做到线性可分呢？这是目前图像领域里面得出的结论。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240313085555643.png\" alt=\"image-20240313085555643\" style=\"zoom: 67%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb615b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量 [3 4] 的归一化结果是: [0.6 0.8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def l2_normalize(vector):\n",
    "    norm = np.sqrt(np.sum(np.square(vector)))\n",
    "    if norm == 0:\n",
    "        return vector\n",
    "    return vector / norm\n",
    "\n",
    "\n",
    "vector = np.array([3, 4])\n",
    "normalized_vector = l2_normalize(vector)\n",
    "print(f\"向量 {vector} 的归一化结果是: {normalized_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bcf046",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "BN假设所有样本独立同分布，并且使用所有样本的共享统计量进行normalization，$E$和$Var$分别是滑动均值和方差，$\\gamma$和$\\beta$是可以学习的scale和bias。多场景建模时，一般为每个场景添加独立的BN层，确保每个场景的数据都能被正确归一化。\n",
    "\n",
    "$$\n",
    "z' = \\gamma \\frac{z-E}{\\sqrt{Var + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "Batch Normalization（批量归一化）的主要作用可以概括为以下几点：\n",
    "1. 稳定训练过程：训练神经网络时，数据在每一层都会经过一系列的变化，导致分布可能会变得不稳定。Batch Normalization通过将数据重新调整为标准正态分布，使训练过程更加平稳，减少训练的不确定性。\n",
    "2. 加速收敛：通过归一化处理，模型在训练过程中可以更快地达到最优状态。因为数据分布被稳定下来，模型可以用更高的学习率，从而减少训练的时间。\n",
    "3. 防止过拟合：Batch Normalization有助于防止模型过拟合。这种归一化方法有类似正则化的效果，减少对其他正则化方法（如Dropout）的依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2705a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798]\n",
      " [ 2.2408932   1.86755799 -0.97727788]\n",
      " [ 0.95008842 -0.15135721 -0.10321885]\n",
      " [ 0.4105985   0.14404357  1.45427351]\n",
      " [ 0.76103773  0.12167502  0.44386323]]\n",
      "训练模式下的归一化结果:\n",
      " [[ 0.79834163 -0.10633427  0.73103552]\n",
      " [ 1.50498534  1.93980876 -1.57728392]\n",
      " [-0.40789413 -0.87536574 -0.54579564]\n",
      " [-1.20737923 -0.46345902  1.29222109]\n",
      " [-0.68805361 -0.49464972  0.09982295]]\n",
      "测试模式下的归一化结果:\n",
      " [[ 1.6881266   0.36139964  0.95638524]\n",
      " [ 2.1785064   1.8657813  -1.02779382]\n",
      " [ 0.85105179 -0.20401383 -0.14114988]\n",
      " [ 0.29624413  0.09883153  1.43876764]\n",
      " [ 0.65663338  0.07589925  0.41380923]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones(num_features)  # 缩放参数γ\n",
    "        self.beta = np.zeros(num_features)  # 平移参数β\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        if self.training:\n",
    "            # 计算当前批次的均值和方差\n",
    "            batch_mean = np.mean(X, axis=0)\n",
    "            batch_var = np.var(X, axis=0)\n",
    "            # 归一化\n",
    "            self.X_norm = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            # 平滑的更新运行中的均值和方差\n",
    "            # 平衡了当前批次和历史批次的统计量，防止单个批次对整体统计量的剧烈影响\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            # 使用运行中的均值和方差进行归一化\n",
    "            self.X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        # 应用缩放和偏移\n",
    "        out = self.gamma * self.X_norm + self.beta\n",
    "        return out\n",
    "\n",
    "    def set_training(self, training):\n",
    "        self.training = training\n",
    "\n",
    "# 示例使用\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(5, 3)\n",
    "print(X)\n",
    "\n",
    "bn = BatchNormalization(num_features=3)\n",
    "\n",
    "# 训练模式下的前向传播\n",
    "bn.set_training(True)\n",
    "output_train = bn.forward(X)\n",
    "print(\"训练模式下的归一化结果:\\n\", output_train)\n",
    "\n",
    "# 测试模式下的前向传播\n",
    "bn.set_training(False)\n",
    "output_test = bn.forward(X)\n",
    "print(\"测试模式下的归一化结果:\\n\", output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593d68f",
   "metadata": {},
   "source": [
    "## 交叉熵损失函数\n",
    "\n",
    "### 定义\n",
    "交叉熵损失函数（Cross-Entropy Loss），在机器学习中特别是分类任务中非常常用。它衡量的是模型预测的概率分布与真实概率分布之间的差异。换句话说，交叉熵损失越小，表示模型的预测结果越接近真实标签。\n",
    "\n",
    "对于一个分类任务，交叉熵损失函数的定义如下：\n",
    "\n",
    "$$\n",
    "\\text{CE}(y, \\hat{y}) = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ y $ 是真实标签的独热编码（one-hot encoding）。\n",
    "- $ \\hat{y} $ 是模型预测的概率分布。\n",
    "- $ C $ 是类别的总数。\n",
    "- $ y_i $ 是真实标签中第 $i$ 类的值（对于独热编码，只有一个位置是1，其余为0）。\n",
    "- $ \\hat{y}_i $ 是模型预测的第 $i$ 类的概率。\n",
    "\n",
    "### 通俗解释\n",
    "可以用一个简单的例子来解释交叉熵损失函数。假设我们在做一个猫和狗的二分类任务：\n",
    "- 如果图片是猫，真实标签 $ y $ 可以表示为 [1, 0]。\n",
    "- 如果图片是狗，真实标签 $ y $ 可以表示为 [0, 1]。\n",
    "\n",
    "模型输出的是对这两个类别的预测概率，例如：\n",
    "- 模型预测图片是猫的概率为 0.9，是狗的概率为 0.1，预测结果为 [0.9, 0.1]。\n",
    "\n",
    "交叉熵损失函数计算真实标签和预测概率之间的差异，对于标签 [1, 0] 和预测 [0.9, 0.1]，交叉熵损失为：\n",
    "$$\n",
    "\\text{CE} = - (1 \\cdot \\log(0.9) + 0 \\cdot \\log(0.1)) = - \\log(0.9)\n",
    "$$\n",
    "\n",
    "损失值越小，表示预测结果越接近真实标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55218477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉熵损失: 0.7418746839526391\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算交叉熵损失\n",
    "\n",
    "    参数:\n",
    "    y_true -- 真实标签，形状为 (batch_size, num_classes)\n",
    "    y_pred -- 预测概率，形状为 (batch_size, num_classes)\n",
    "\n",
    "    返回:\n",
    "    loss -- 交叉熵损失值\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9  # 避免log(0)的情况\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "    return np.mean(cross_entropy)\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "y_true = np.array([[1, 0], [0, 1], [1, 0]])  # 真实标签 (one-hot encoded)\n",
    "y_pred = np.array([[0.9, 0.1], [0.4, 0.6], [0.2, 0.8]])  # 预测概率\n",
    "\n",
    "loss = cross_entropy_loss(y_true, y_pred)\n",
    "print(\"交叉熵损失:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712e890",
   "metadata": {},
   "source": [
    "## Focal Loss\n",
    "\n",
    "### 简介\n",
    "Focal Loss 是一种改进的交叉熵损失函数，旨在解决样本类别不均衡的问题。它通过降低对易分类样本的损失权重，增强对难分类样本的关注，从而改善模型在不平衡数据集上的表现。Focal Loss 的公式如下：\n",
    "\n",
    "$$\n",
    "Loss_{fl} =-\\alpha_{t}(1- p_{t})^\\gamma log(p_{t})\n",
    "$$\n",
    "\n",
    "其中公式中各个部分的含义如下：\n",
    "- $p_t$是模型对正确类别的预测概率。\n",
    "- $\\alpha_t$是平衡因子，用于平衡正负样本的影响（可选）。\n",
    "- $\\gamma$是调整因子，用于控制易分类样本的权重降低程度。\n",
    "\n",
    "\n",
    "Focal Loss是如何做到降低对易分类样本的损失权重，增强对难分类样本的关注的？\n",
    "- 对于易分类样本，$p_{t}$接近 1（模型对其分类的信心高），那么$1-p_{t}$接近 0，因此 $(1- p_{t})^\\gamma$ 也接近 0，这个调节项会显著降低易分类样本的损失值。\n",
    "- 对于难分类样本，$p_{t}$接近 0（模型对其分类的信心低），那么$1-p_{t}$接近 1，因此 $(1- p_{t})^\\gamma$ 仍接近 1，这个调节项对难分类样本的影响很小。\n",
    "\n",
    "### 推导过程（可跳过该章节）\n",
    "交叉熵损失函数（其中 $\\hat p$为预测概率大小）：\n",
    "\n",
    "$$\n",
    "Loss = L(y, \\hat p) =-ylog(\\hat p) - (1-y)log(1-\\hat p)\n",
    "$$\n",
    "\n",
    "对于二分类问题，先简化公式：\n",
    "\n",
    "$$\n",
    "Loss = L(y, \\hat p) =-log(\\hat p) - log(1-\\hat p)\n",
    "$$\n",
    "\n",
    "对于所有样本来说，假设N为总样本量，m为正样本量，n为负样本量，当m << n时，负样本就会在损失函数里占据主导地位，由于损失函数的倾斜，模型训练过程中会倾向于样本多的类别，造成模型对少样本类别的性能较差。\n",
    "\n",
    "$$\n",
    "Loss =\\frac{1}{N}(\\sum_{y_i=1}^{m}-log(\\hat p) + \\sum_{y_i=o}^{n}-log(1-\\hat p))\n",
    "$$\n",
    "\n",
    "focal loss具体形式：\n",
    "\n",
    "$$\n",
    "Loss =\\frac{1}{N}(\\sum_{y_i=1}^{m}-(1-\\hat p)^\\gamma log(\\hat p) + \\sum_{y_i=o}^{n}-\\hat p^{\\gamma} log(1-\\hat p))\n",
    "$$\n",
    "\n",
    "如果我做以下定义：\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240312223812014.png\" alt=\"image-20240312223812014\" style=\"zoom:50%;\" />\n",
    "\n",
    "**focal loss表达式：**\n",
    "\n",
    "$$\n",
    "Loss_{fl} =-(1- p_t)^\\gamma log(p_t)\n",
    "$$\n",
    "\n",
    "**交叉熵表达式：**\n",
    "\n",
    "$$\n",
    "Loss_{ce} =-log(p_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2654a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Loss: 0.09273549740974675\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    计算Focal Loss\n",
    "\n",
    "    参数:\n",
    "    y_true -- 真实标签，形状为 (batch_size, num_classes)\n",
    "    y_pred -- 预测概率，形状为 (batch_size, num_classes)\n",
    "    alpha -- 平衡因子，默认值为 0.25\n",
    "    gamma -- 调整因子，默认值为 2.0\n",
    "\n",
    "    返回:\n",
    "    loss -- Focal Loss 值\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # 避免log(0)情况\n",
    "    cross_entropy = -y_true * np.log(y_pred)\n",
    "    loss = alpha * (1 - y_pred) ** gamma * cross_entropy\n",
    "    return np.mean(np.sum(loss, axis=1))\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "y_true = np.array([[1, 0], [0, 1], [1, 0]])  # 真实标签\n",
    "y_pred = np.array([[0.9, 0.1], [0.4, 0.6], [0.2, 0.8]])  # 预测概率\n",
    "\n",
    "loss = focal_loss(y_true, y_pred)\n",
    "print(\"Focal Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3349aad",
   "metadata": {},
   "source": [
    "## OneEpoch现象\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. paper: **Multi-Epoch Learning** for Deep Click-Through Rate Prediction Models\n",
    "> 2. [阿里OneEpoch VS 快手MultiEpoch](https://zhuanlan.zhihu.com/p/669063912)\n",
    "\n",
    "**模型AUC在第一个epoch内逐步提升，但是从第二个epoch开始，AUC效果突然剧烈下降**。产生OneEpoch现象的原因：\n",
    "\n",
    "- embedding  + mlp的结构\n",
    "- 能使模型快速收敛的优化器算法（eg: 学习率较大的adam优化器）\n",
    "- 高维稀疏特征（eg: item_id等细粒度特征）\n",
    "- 其他不相关因素：模型参数量、激活函数、batch size、weight decay、dropout\n",
    "\n",
    "多Epoch探究：**每一轮训练都重置embedding，更新embedding和mlp**，避免embedding层过拟合，并让mlp层学的更充分。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-4a751d00853416d18182ef4d97f2f5b4_1440w-0067336.webp\" alt=\"img\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f844a3",
   "metadata": {},
   "source": [
    "## 温度系数\n",
    "温度系数常应用在召回/粗排等双塔模型结构中，点乘之后除以一个固定的系数（温度系数），τ是温度系数，一般来说加一个温度系数是有效的。原因是温度系数可以让模型更聚焦于hard负例，且τ越小越聚焦，也就不用花费大力气挖掘hard负例。\n",
    "\n",
    "### Hard样本\n",
    "Hard样本是指那些模型难以正确分类的样本。这些样本可能由于以下原因难以分类：\n",
    "- 数据不平衡\n",
    "- 特征不明显\n",
    "- 噪声或异常数据\n",
    "\n",
    "### 温度系数与Hard样本的具体关系\n",
    "\n",
    "1. **低温度系数（<1）**:\n",
    "   - **效果**: 使得模型的输出概率分布更尖锐（更接近于0或1）。\n",
    "   - **对hard样本的影响**: \n",
    "     - 模型对hard样本的预测可能会更不确定，因为这些样本本身难以分类，输出概率会更加极端（高概率的类别与低概率的类别差距更大）。\n",
    "     - 模型可能会更加确信自己的错误预测，从而难以在后续训练中纠正。\n",
    "\n",
    "2. **高温度系数（>1）**:\n",
    "   - **效果**: 使得模型的输出概率分布更平滑（更接近于均匀分布）。\n",
    "   - **对hard样本的影响**: \n",
    "     - 模型对hard样本的预测会变得更不确定，输出的概率更接近于均匀分布（各类别的概率差距缩小）。\n",
    "     - 这种不确定性可以提示模型在训练过程中对这些hard样本进行更多关注，从而帮助模型更好地学习和纠正错误。\n",
    "\n",
    "### 温度系数对预估值的影响\n",
    "\n",
    "假设一个分类模型在一个三类问题中输出如下分数（未归一化）：\n",
    "- 易分类样本：[10, 2, 1]\n",
    "- 难分类样本：[3, 3, 2.5]\n",
    "\n",
    "1. **无温度调整（温度=1）**:\n",
    "   - 易分类样本的Softmax概率：[0.999, 0.001, 0.000]\n",
    "   - 难分类样本的Softmax概率：[0.4, 0.4, 0.2]\n",
    "\n",
    "2. **降低温度（温度=0.5）**:\n",
    "   - 易分类样本的Softmax概率：[1.0, 0.0, 0.0]\n",
    "   - 难分类样本的Softmax概率：[0.42, 0.42, 0.16]（更尖锐）\n",
    "\n",
    "3. **升高温度（温度=2）**:\n",
    "   - 易分类样本的Softmax概率：[0.88, 0.06, 0.06]\n",
    "   - 难分类样本的Softmax概率：[0.34, 0.34, 0.32]（更平滑）\n",
    "\n",
    "### 温度系数对loss的影响\n",
    "在机器学习和深度学习中，损失函数（loss）是用于衡量模型预测与真实标签之间差异的指标。对于分类任务，常用的损失函数是交叉熵损失。温度系数的调整会影响模型的预测概率分布，从而对损失值产生影响。\n",
    "\n",
    "假设模型的输出分数为：[3.0, 1.0, 0.1]，且真实标签对应的类别为第一个类别（索引0）。\n",
    "\n",
    "1. **无温度调整（温度=1）**:\n",
    "   - 使用Softmax函数计算概率：\n",
    "     - $ P_0 = \\frac{e^{3.0}}{e^{3.0} + e^{1.0} + e^{0.1}} \\approx 0.84 $\n",
    "     - $ P_1 = \\frac{e^{1.0}}{e^{3.0} + e^{1.0} + e^{0.1}} \\approx 0.11 $\n",
    "     - $ P_2 = \\frac{e^{0.1}}{e^{3.0} + e^{1.0} + e^{0.1}} \\approx 0.05 $\n",
    "\n",
    "2. **降低温度（温度=0.5）**:\n",
    "   - 调整后的Softmax计算：\n",
    "     - 调整后的分数：[6.0, 2.0, 0.2]（分数被拉开）\n",
    "     - $ P_0 = \\frac{e^{6.0}}{e^{6.0} + e^{2.0} + e^{0.2}} \\approx 0.97 $\n",
    "     - $ P_1 = \\frac{e^{2.0}}{e^{6.0} + e^{2.0} + e^{0.2}} \\approx 0.03 $\n",
    "     - $ P_2 = \\frac{e^{0.2}}{e^{6.0} + e^{2.0} + e^{0.2}} \\approx 0.0007 $\n",
    "\n",
    "交叉熵损失的公式为：\n",
    "$$ L = - \\sum_{i} y_i \\log(p_i) $$\n",
    "其中 $y_i$ 是真实标签的one-hot编码，$p_i$ 是模型预测的概率。\n",
    "\n",
    "1. **无温度调整时的损失**:\n",
    "   - $ y = [1, 0, 0] $\n",
    "   - 损失：$ L = - \\log(0.84) \\approx 0.17 $\n",
    "\n",
    "2. **降低温度后的损失**:\n",
    "   - $ y = [1, 0, 0] $\n",
    "   - 损失：$ L = - \\log(0.97) \\approx 0.03 $\n",
    "\n",
    "当温度系数降低时，模型的输出概率分布变得更尖锐，预估值被拉开。如果模型预测正确，类别的概率接近1，交叉熵损失将会变小。相反，如果模型预测错误，高温度系数导致的平滑概率分布可能会导致较高的损失。因此，预估值被拉开不一定会导致损失变大。具体影响取决于模型预测的正确性。对于正确的预测，预估值被拉开会降低损失；对于错误的预测，预估值被拉开会增加损失。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}