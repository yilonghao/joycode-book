{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cdea23",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#序列特征优化方案\" data-toc-modified-id=\"序列特征优化方案-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>序列特征优化方案</a></span><ul class=\"toc-item\"><li><span><a href=\"#Base-Model\" data-toc-modified-id=\"Base-Model-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Base Model</a></span></li><li><span><a href=\"#DIN(Deep-Interest-Network)\" data-toc-modified-id=\"DIN(Deep-Interest-Network)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>DIN(Deep Interest Network)</a></span><ul class=\"toc-item\"><li><span><a href=\"#DIN核心思想\" data-toc-modified-id=\"DIN核心思想-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>DIN核心思想</a></span></li><li><span><a href=\"#mini-batch-aware-regularization(MBAR)\" data-toc-modified-id=\"mini-batch-aware-regularization(MBAR)-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>mini-batch aware regularization(MBAR)</a></span></li><li><span><a href=\"#自适应激活函数Dice\" data-toc-modified-id=\"自适应激活函数Dice-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>自适应激活函数Dice</a></span></li></ul></li><li><span><a href=\"#DIEN（target-attention-+-GRU）\" data-toc-modified-id=\"DIEN（target-attention-+-GRU）-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>DIEN（target attention + GRU）</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型结构\" data-toc-modified-id=\"模型结构-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>模型结构</a></span></li><li><span><a href=\"#Interest-Extractor-Layer\" data-toc-modified-id=\"Interest-Extractor-Layer-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Interest Extractor Layer</a></span></li><li><span><a href=\"#辅助损失函数\" data-toc-modified-id=\"辅助损失函数-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>辅助损失函数</a></span></li><li><span><a href=\"#Interest-Evolving-Layer\" data-toc-modified-id=\"Interest-Evolving-Layer-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Interest Evolving Layer</a></span></li><li><span><a href=\"#将attention机制融入GRU结构中(AUGRU)\" data-toc-modified-id=\"将attention机制融入GRU结构中(AUGRU)-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>将attention机制融入GRU结构中(AUGRU)</a></span></li></ul></li><li><span><a href=\"#DSIN\" data-toc-modified-id=\"DSIN-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>DSIN</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型结构（整体）\" data-toc-modified-id=\"模型结构（整体）-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>模型结构（整体）</a></span></li><li><span><a href=\"#embedding\" data-toc-modified-id=\"embedding-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>embedding</a></span></li><li><span><a href=\"#建模用户序列\" data-toc-modified-id=\"建模用户序列-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>建模用户序列</a></span><ul class=\"toc-item\"><li><span><a href=\"#session-division-layer\" data-toc-modified-id=\"session-division-layer-1.4.3.1\"><span class=\"toc-item-num\">1.4.3.1&nbsp;&nbsp;</span>session division layer</a></span></li><li><span><a href=\"#session-interest-extractor-layer\" data-toc-modified-id=\"session-interest-extractor-layer-1.4.3.2\"><span class=\"toc-item-num\">1.4.3.2&nbsp;&nbsp;</span>session interest extractor layer</a></span></li><li><span><a href=\"#Session-Interest-Interacting-Layer\" data-toc-modified-id=\"Session-Interest-Interacting-Layer-1.4.3.3\"><span class=\"toc-item-num\">1.4.3.3&nbsp;&nbsp;</span>Session Interest Interacting Layer</a></span></li><li><span><a href=\"#session-interest-activating-layer\" data-toc-modified-id=\"session-interest-activating-layer-1.4.3.4\"><span class=\"toc-item-num\">1.4.3.4&nbsp;&nbsp;</span>session interest activating layer</a></span></li><li><span><a href=\"#session-interest-extractor-layer-相关-activation-unit\" data-toc-modified-id=\"session-interest-extractor-layer-相关-activation-unit-1.4.3.5\"><span class=\"toc-item-num\">1.4.3.5&nbsp;&nbsp;</span>session interest extractor layer 相关 activation unit</a></span></li><li><span><a href=\"#session-interest-interacting-layer-相关-activation-unit\" data-toc-modified-id=\"session-interest-interacting-layer-相关-activation-unit-1.4.3.6\"><span class=\"toc-item-num\">1.4.3.6&nbsp;&nbsp;</span>session interest interacting layer 相关 activation unit</a></span></li></ul></li><li><span><a href=\"#MLP\" data-toc-modified-id=\"MLP-1.4.4\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span>MLP</a></span></li><li><span><a href=\"#实验数据\" data-toc-modified-id=\"实验数据-1.4.5\"><span class=\"toc-item-num\">1.4.5&nbsp;&nbsp;</span>实验数据</a></span></li></ul></li><li><span><a href=\"#京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender\" data-toc-modified-id=\"京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a></span><ul class=\"toc-item\"><li><span><a href=\"#简介\" data-toc-modified-id=\"简介-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href=\"#Deep-Multifaceted-Transformers-Layer\" data-toc-modified-id=\"Deep-Multifaceted-Transformers-Layer-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Deep Multifaceted Transformers Layer</a></span></li><li><span><a href=\"#Bias-Deep-Neural-Network\" data-toc-modified-id=\"Bias-Deep-Neural-Network-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Bias Deep Neural Network</a></span></li><li><span><a href=\"#Multi-gate-Mixture-of-Experts-Layers\" data-toc-modified-id=\"Multi-gate-Mixture-of-Experts-Layers-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Multi-gate Mixture-of-Experts Layers</a></span></li></ul></li><li><span><a href=\"#阿里|Search-based-User-Interest-Modeling-with-Lifelong-Sequential-Behavior-Data-for-Click-Through-Rate-Prediction\" data-toc-modified-id=\"阿里|Search-based-User-Interest-Modeling-with-Lifelong-Sequential-Behavior-Data-for-Click-Through-Rate-Prediction-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>阿里|Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-Search-Unit\" data-toc-modified-id=\"General-Search-Unit-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>General Search Unit</a></span></li><li><span><a href=\"#Exact-Search-Unit\" data-toc-modified-id=\"Exact-Search-Unit-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Exact Search Unit</a></span></li></ul></li><li><span><a href=\"#阿里BST（Transformer处理序列特征）\" data-toc-modified-id=\"阿里BST（Transformer处理序列特征）-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>阿里BST（Transformer处理序列特征）</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型结构\" data-toc-modified-id=\"模型结构-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>模型结构</a></span><ul class=\"toc-item\"><li><span><a href=\"#Embedding-Layer\" data-toc-modified-id=\"Embedding-Layer-1.7.1.1\"><span class=\"toc-item-num\">1.7.1.1&nbsp;&nbsp;</span>Embedding Layer</a></span></li><li><span><a href=\"#Transformer-Layer\" data-toc-modified-id=\"Transformer-Layer-1.7.1.2\"><span class=\"toc-item-num\">1.7.1.2&nbsp;&nbsp;</span>Transformer Layer</a></span></li><li><span><a href=\"#MLP-layers-and-Loss-function\" data-toc-modified-id=\"MLP-layers-and-Loss-function-1.7.1.3\"><span class=\"toc-item-num\">1.7.1.3&nbsp;&nbsp;</span>MLP layers and Loss function</a></span></li></ul></li><li><span><a href=\"#实验数据\" data-toc-modified-id=\"实验数据-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>实验数据</a></span></li></ul></li><li><span><a href=\"#阿里DMR（attention）\" data-toc-modified-id=\"阿里DMR（attention）-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>阿里DMR（attention）</a></span><ul class=\"toc-item\"><li><span><a href=\"#Item-to-Item-Network\" data-toc-modified-id=\"Item-to-Item-Network-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Item-to-Item Network</a></span></li><li><span><a href=\"#User-to-Item-Network\" data-toc-modified-id=\"User-to-Item-Network-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>User-to-Item Network</a></span></li></ul></li><li><span><a href=\"#阿里MIND\" data-toc-modified-id=\"阿里MIND-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>阿里MIND</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型介绍\" data-toc-modified-id=\"模型介绍-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>模型介绍</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型整体结构图\" data-toc-modified-id=\"模型整体结构图-1.9.1.1\"><span class=\"toc-item-num\">1.9.1.1&nbsp;&nbsp;</span>模型整体结构图</a></span></li><li><span><a href=\"#模型定义\" data-toc-modified-id=\"模型定义-1.9.1.2\"><span class=\"toc-item-num\">1.9.1.2&nbsp;&nbsp;</span>模型定义</a></span></li><li><span><a href=\"#模型结构\" data-toc-modified-id=\"模型结构-1.9.1.3\"><span class=\"toc-item-num\">1.9.1.3&nbsp;&nbsp;</span>模型结构</a></span></li><li><span><a href=\"#Training-&amp;-Serving\" data-toc-modified-id=\"Training-&amp;-Serving-1.9.1.4\"><span class=\"toc-item-num\">1.9.1.4&nbsp;&nbsp;</span>Training &amp; Serving</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5740b1",
   "metadata": {},
   "source": [
    "# 推荐系统04-序列特征优化方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad2e6b",
   "metadata": {},
   "source": [
    "## Base Model\n",
    "\n",
    "传统基于用户行为序列描述用户兴趣的方法是把序列中各个item的embedding通过pooling的方式转化成一个固定维的embedding。如下图所示，其中红色节点表示商品ID，蓝色节点表示店铺iD，粉色节点表示类目ID，白色节点表示用户特征和上下文特征，Goods 1 ~ Goods N 用来描述用户的历史行为，候选广告（Candidate Ad）本身也是商品。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/dec039c0-ee95-4d4d-9246-4038e11812f2.png\" alt=\"image\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "网络结构具有的缺点是经过pooling以后的向量与候选广告无关，对于一个用户来说是固定不变的。对于不同的候选广告，与之对应的用户兴趣分布也应该不通。因为只有用户部分的兴趣会影响当前行为（对候选广告点击或不点击）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95366c34",
   "metadata": {},
   "source": [
    "## DIN(Deep Interest Network)\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/9223919b-9c70-4403-a519-54a0b10e8b3b.png\" alt=\"image\" style=\"zoom: 67%; display: block; margin: auto;\" />\n",
    "\n",
    "### DIN核心思想\n",
    "\n",
    "DIN的核心思想是，对每一个用户，不同的广告有不同的向量表示，结合用户行为特征与给定的广告为每个用户行为计算权重，引入local-activation机制有侧重的利用用户不同的行为特征，其中$e_j$表示用户行为向量，$v_A$为候选广告的向量，a表示激活单元，$a(e_j,v_A)$为权重。整体逻辑为SUM Pooling。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/c2eb0b1e-f6a3-4cfd-99ad-3b8537a0963b.png\" alt=\"image\" style=\"zoom: 67%; display: block; margin: auto;\" />\n",
    "\n",
    "相比基础的深度推荐网络，DIN在生成用户向量的时候加了一个activation unit单元，计算每个用户行为与候选广告之间的权重。在传统的attention机制中，给定两个向量，比如u和v，通常直接做点乘。这篇论文中做了进一步的改进：首先是把u和v以及uv的外积合并起来作为输入给全连接层，最后得到权重，这样可以减少信息损失。论文中还放宽了权重加和等于1的限制，这样更有利于体现用户行为特征之间的差异化程度。\n",
    "\n",
    "### mini-batch aware regularization(MBAR)\n",
    "\n",
    "为解决大规模稀疏场景下，采用SGD对引入L2正则的loss进行更新时计算开销过大的问题，该方法只对每一个mini-batch中参数不为0的进行梯度更新。\n",
    "\n",
    "### 自适应激活函数Dice\n",
    "\n",
    "PRelu激活函数如下所示：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/5f30f50e-8c69-4212-8692-940457ec8b7a.png\" style=\"zoom: 67%; display: block; margin: auto;\" />\n",
    "\n",
    "采用PRelu激活函数时，它的rectified point固定为0，这在每一层的输入分布发生变化时是不适用的，所以文章对该激活函数做了改进，平滑了rectified point附近曲线的同时，激活函数会根据每层输入数据的分布来自适应调整rectified point的位置，具体形式如下：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ccac1f77-49db-4cc2-ab87-186e5637c1f2.png\" style=\"zoom: 67%; display: block; margin: auto;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/81556967-3897-407e-8496-2d98aa7bc705.png\" style=\"zoom: 50%; display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f365daf",
   "metadata": {},
   "source": [
    "## DIEN（target attention + GRU）\n",
    "\n",
    "阿里妈妈的精准定向检索及基础算法团队以 Deep Interest Network(DIN) 算法为基础，进一步优化升级，产出了 Deep Interest Evolution Network(DIEN)算法，主要解决了以下两个问题：\n",
    "\n",
    "- 更加精确的刻画用户的长期兴趣和短期兴趣\n",
    "- 用户的兴趣是时刻变化的，需要更加准确的刻画用户兴趣的变化\n",
    "\n",
    "### 模型结构\n",
    "如图所示，DIEN 由 embedding 层、Interest Extractor Layer 层，Interest Evolving Layer 层组成。Interest Extractor Layer 根据行为序列提取兴趣序列，Interest Evolving Layer 基于 Target Ad 对兴趣序列进行建模，得到用户兴趣向量表示，最终将用户兴趣向量表示与其他特征向量 concat 到一起后送到 MLP 中以进行最终预测。\n",
    "\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1c958050-4694-4a23-8ca2-ac929d53453a.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "\n",
    "### Interest Extractor Layer\n",
    "\n",
    "兴趣提取层的目的就是从用户的行为序列中提取出一系列的兴趣状态。为了平衡效率和性能，作者选择了GRU（Gated Recurrent Unit，门循环单元）网络来对用户行为之间的依赖进行建模。GRU既能够克服RNN梯度消失的问题，同时又比LSTM网络具有更少的参数，训练时收敛速度更快。GRU单元的表达式如下：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/225b086d-f739-42b0-bcb6-b5a1d6a2cbae.png\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "其中 $\\sigma$表示sigmoid激活函数，$\\circ$表示元素积(element-wise product)，$W$和$U$表示隐藏层参数，$i_t$表示GRU单元的输入（用户第t个行为的embedding向量），$h_{t}$表示第t个GRU单元的隐层状态。关于GRU结构的介绍可以参考文章：[GRU学习笔记](https://zhuanlan.zhihu.com/p/463521382)，主结构如下：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6ee03222-480d-457c-9509-a6270d8a4223.png\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "### 辅助损失函数\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a3592fc6-81c0-4461-b6fd-e98f90109bb6.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/44c87c81-7b82-4438-9b8c-3fddbdd69173.png\" alt=\"辅助Loss计算示意图\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "其中 $\\{e_{b}^{i},\\hat e_{b}^{i}\\} \\in D_{\\beta},i=1,2,3,...,N$表示N对行为embedding序列，$e_{b}^{i} \\in \\mathbb{R}^{T  \\times n_E}$表示用户的点击行为序列，$\\hat e_{b}^{i}\\in \\mathbb{R}^{T  \\times n_E}$表示用户没有点击的行为序列。$T$表示序列中历史行为的数量，$n_E$表示embedding向量的维度。$e_{b}^{i}[t]$表示第i个用户第t次点击行为的embedding向量。其中：\n",
    "\n",
    "$$\n",
    "\\sigma(x_1, x_2) = \\frac{1}{exp(-[x_1, x_2])}\n",
    "$$\n",
    "\n",
    "DIEN使用的整体损失函数是：\n",
    "\n",
    "$$\n",
    "L = L_{target} + \\alpha * L_{aux}\n",
    "$$\n",
    "\n",
    "### Interest Evolving Layer\n",
    "\n",
    "兴趣进化层Interest Evolution Layer的主要目标是刻画用户兴趣的进化过程。兴趣进化层的数据就是兴趣提取层的输出，兴趣进化层结合了注意力机制中的局部激活能力和GRU的序列学习能力来建模。attention部分系数计算方式如下：\n",
    "\n",
    "$$\n",
    "a_t = \\frac{exp(h_{t}W_{e_{a}})}{\\sum_{j=1}^Texp(h_{j}W_{e_{a}})}\n",
    "$$\n",
    "\n",
    "### 将attention机制融入GRU结构中(AUGRU)\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/71b8130c-bdf8-4cdf-86f4-e5eccfcdd689.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "通过将attention部分的计算结果作用到GRU结构的更新门上将attention机制融入到GRU结构中。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/664e6fa8-e51d-4fc0-8077-3c26c81319f6.png\" style=\"zoom: 33%; display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5f143",
   "metadata": {},
   "source": [
    "## DSIN\n",
    "\n",
    "这篇文章主要介绍阿里在2019年发表的排序阶段模型：Deep Session Interest Network for Click-Through Rate Prediction。主要思路为将用户的历史点击行为划分为不同session，而后通过Transformer结构学习每个session的向量表示，最后通过BiLSTM结构对session序列进行建模，整体来说具有一定的参考意义。\n",
    "\n",
    "### 模型结构（整体）\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/11a806f7-3775-44b0-83db-77a199ef130d.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "### embedding\n",
    "\n",
    "从图中可以看出模型主要由三大部分组成，其中第一部分（part1）主要处理用户画像特征（年龄、性别、所在城市等）和item侧特征（seller id, brand id等），处理方式也很简单，所有特征通过embedding的方式得到对应的向量表示，最终用户侧特征向量表示为$X^U \\in \\mathbb{R}^{N_u \\times d_{model}}$，其中$N_u$表示用户侧特征个数，item侧特征的向量表示为：$X^I \\in \\mathbb{R}^{N_i \\times d_{model}}$，其中$N_i$表示item侧特征个数。\n",
    "\n",
    "### 建模用户序列\n",
    "\n",
    "这一部分（part2）主要用来建模用户行为序列，主要包含4部分：\n",
    "\n",
    "- session division layer: 对用户行为序列进行划分\n",
    "- session interest extractor layer: 学习每个session的向量表示\n",
    "- session interest interacting layer: 对session序列进行建模\n",
    "- session interest activating layer: 引入attention机制\n",
    "\n",
    "#### session division layer\n",
    "\n",
    "主要作用就是将用户的历史点击行为序列划分为多个sessions。整个操作的符号表示为将用户的行为序列$S$划分为sessions $Q$，第$k$个session的表示形式为：\n",
    "\n",
    "$$\n",
    "Q_k = [b_1; ...; bi; ...; b_T] \\in \\mathbb{R}^{T \\times d_{model}}\n",
    "$$\n",
    "\n",
    "其中$T$为当前session对应的序列长度，$b_i$为用户在当前session中的第$i$次点击行为，文中session的划分是按照时间间隔来的，两次点击之间的间隔超过30min则将下一次点击行为计入下一个session。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/23182b90-adff-4fb1-8841-b0b18e78a747.jpg\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "#### session interest extractor layer\n",
    "\n",
    "这部分的主要作用就是学习每个session的向量表示，文中使用了multi-head self-attention的结构对每个session建模。为了刻画不同session间的顺序，DSIN使用了Bias Encoding，其中$BE \\in \\mathbb{R}^{K \\times T \\times d_{model}}$。Bias Encoding 计算方式如下：\n",
    "\n",
    "$$\n",
    "BE_{(k,t,c)} = w_k^K + w_t^T + w_c^C\n",
    "$$\n",
    "\n",
    "其中$BE_{(k,t,c)}$表示第k个session中的第t个物品的embedding向量中的第c个位置对应的bias，加入bias encoding后，用户的session表示为：\n",
    "\n",
    "$$\n",
    "Q=Q+BE\n",
    "$$\n",
    "\n",
    "Multi-head Self-attention的计算逻辑为：\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0a020f15-557f-48a9-b49f-cd177c1af371.png\" style=\"zoom: 50%; display: block; margin: auto;\" />\n",
    "\n",
    "$$\n",
    "I_k^Q = FFN(Concat(head_1, ..., head_H)W^O)\n",
    "$$\n",
    "\n",
    "$$\n",
    "I_k = Avg(I_k^Q)\n",
    "$$\n",
    "\n",
    "#### Session Interest Interacting Layer\n",
    "\n",
    "这部分主要通过Bi-LSTM结构对session序列进行建模。\n",
    "\n",
    "$$\n",
    "H_t = \\mathop{h_{ft}}\\limits ^{\\rightarrow} \\oplus \\mathop{h_{bt}}\\limits ^{\\leftarrow}\n",
    "$$\n",
    "\n",
    "其中$\\mathop{h_{ft}}\\limits ^{\\rightarrow}$表示forward隐藏层状态，$\\mathop{h_{bt}}\\limits ^{\\leftarrow}$表示backward隐藏层状态。\n",
    "\n",
    "#### session interest activating layer\n",
    "\n",
    "这部分主要是通过attention机制刻画目标item和session之间的相关性。主要思想是若session与目标item之间的相关性越高，则应该赋予越大的权重。模型结构中共有两个Activation Unit，结构是一致的。\n",
    "\n",
    "#### session interest extractor layer 相关 activation unit\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0f1a1c33-3507-4375-8fb3-8a7640b1d777.png\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "#### session interest interacting layer 相关 activation unit\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a5717872-e0b2-4240-8d6a-80b5d51b4183.png\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "### MLP\n",
    "\n",
    "这一部分（part3）为简单的MLP结构，首先将得到的所有向量表示concat到一起，而后通过2层前馈神经网络，最后通过一个softmax层得到输出。损失函数为：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/4e742bb1-d068-47c4-93c3-46351e119807.png\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "其中 $\\mathbb{D}$表示训练集，$y \\in \\{0, 1\\}$，$p(\\cdot)$表示网络最终的输出结果，表示用户点击目标item的概率。\n",
    "\n",
    "### 实验数据\n",
    "\n",
    "从实验结果来看，AUC相比其他模型均有一定幅度的提升。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/f67a91a0-f19b-47b6-95ba-a1ae66ae1a0c.png\" style=\"zoom:50%; display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5beb2",
   "metadata": {},
   "source": [
    "## 京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender\n",
    "\n",
    "\n",
    "> 推荐阅读⭐️⭐️⭐️⭐️⭐️\n",
    "> - paper: Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender\n",
    "> - [论文《Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender》](https://blog.csdn.net/whgyxy/article/details/126023948)\n",
    "> - [ctr预估：DMT模型](https://zhuanlan.zhihu.com/p/534414021)\n",
    "> - [详解谷歌之多任务学习模型MMoE(KDD 2018)](https://zhuanlan.zhihu.com/p/55752344)\n",
    "\n",
    "### 简介\n",
    "\n",
    "模型应用的业务场景商品搜索排序阶段，论文提出用多个Transfomer对用户多种类型的行为序列进行建模，在此基础上叠加MMOE建模多目标，最后使用一个消偏塔对数据进行消偏。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221231142523288-2501956.png\" style=\"zoom: 67%; display: block; margin: auto;\" />\n",
    "\n",
    "\n",
    "### Deep Multifaceted Transformers Layer\n",
    "\n",
    "Embedding Layer对每个物料使用物料id、类目id、品牌id、商铺id分别映射成低维向量，然后concat起来，形成向量；Dense特征使用了Z-score归一化。模型使用了点击，加购，成交3个Item Sequence，分别表征短期，中期和长期兴趣；分别用3个Transformer来对点击、加入购物车、购买行为序列进行建模：\n",
    "\n",
    "- encoder：用序列的item-embedding作为self-attention的输入\n",
    "- decoder：使用target item的embedding作为query，encoder输出的结果作为key和value。\n",
    "\n",
    "### Bias Deep Neural Network\n",
    "\n",
    "Bais塔的输入都是偏差相关的特征，对于位置偏差输入就是展示位置索引编号或者网页索引编号；对于近邻偏差，输入就是目标物料的类目和邻近K个物料的类目。Bias建模部分使用Bias特征+MLP，输出的Logits与主网络Logits相加。\n",
    "\n",
    "### Multi-gate Mixture-of-Experts Layers\n",
    "\n",
    "多任务建模部分使用MMoE结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c50c5",
   "metadata": {},
   "source": [
    "## 阿里|Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60857f1a",
   "metadata": {},
   "source": [
    "> 推荐阅读⭐️⭐️⭐️⭐️⭐️\n",
    "> - paper: Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction\n",
    "> - [SIM: 基于搜索的超长行为序列上的用户兴趣建模](https://zhuanlan.zhihu.com/p/370951728)\n",
    "> - [学习笔记：MIMN与SIM-长用户行为序列上的处理方法](https://zhuanlan.zhihu.com/p/216123531)\n",
    "> - [阿里妈妈基于检索的用户行为兴趣CTR模型——SIM](https://zhuanlan.zhihu.com/p/188755987)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e549410",
   "metadata": {},
   "source": [
    "像DIN、DIEN这些模型，都使用了Target Attention，这样可以对不相关的行为信息做一下过滤。但是这种方式在处理超长行为序列时的计算延时(latency)是不能接受的。\n",
    "\n",
    "针对超长行为序列两步走：\n",
    "\n",
    "- **General Search Unit(GSU)**：在用户行为序列中进行初筛，得到相关的item，压缩用户行为序列长度，初筛的方式又分两种：\n",
    "    - soft search: 用候选item的embedding去和用户行为序列中的每一项的embedding去做点积，然后取TopK，这里可以用ALSH等比较高效的方法。\n",
    "    - hard search: 标签匹配的方式，比如选出用户行为序列中与target item同类别的行为。\n",
    "    \n",
    "- **Exact Search Unit(ESU)**：对筛选后的用户行为序列建模。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20230101161905273.png\" style=\"zoom:50%; display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5dff97",
   "metadata": {},
   "source": [
    "### General Search Unit\n",
    "\n",
    "在做soft search操作时，采用的是另外一套embedding，和MLP的embedding不一样，主要是考虑到分布不一样。训练时，添加辅助loss：\n",
    "\n",
    "$$\n",
    "Loss = \\alpha Loss_{GSU} + \\beta Loss_{ESU}\n",
    "$$\n",
    "\n",
    "使用hard serach这种方式时可以认为就是标签匹配，无参，定义好筛选规则即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e46d5a",
   "metadata": {},
   "source": [
    "### Exact Search Unit\n",
    "\n",
    "和传统的DIN思路一致，这里使用的是multi head attention。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8965089",
   "metadata": {},
   "source": [
    "## 阿里BST（Transformer处理序列特征）\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. 《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》\n",
    "> 2. [Transformer模型详解（图解最完整版）](https://zhuanlan.zhihu.com/p/338817680。)\n",
    "\n",
    "阿里搜索团队在2019年发布的文章《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》中提出了BST模型，主要思路是利用Transformer结构来捕获用户行为序列中的序列信息。与WDL(wide and deep learning networks)和DIN(Deep Interest networks)模型相比：\n",
    "\n",
    "- 离线评估阶段AUC有明显的提升\n",
    "- 线上A/B实验，CTR指标也有较大幅度提升\n",
    "- 系统性能开销有所增加，平均响应时长指标上涨\n",
    "\n",
    "最终模型被实际部署到线上精排阶段，文章为序列特征的处理提供了新的思路，值得学习。\n",
    "\n",
    "### 模型结构\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/e651be9e-b64f-4d92-a9f5-caa02871727b.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "模型主要分为三大部分：Embedding Layer、Transformer Layer、MLP Layers and Loss function。\n",
    "\n",
    "#### Embedding Layer\n",
    "embedding层主要作用就是把输入的每个特征都转换为低维向量，文中的特征主要包括**用户行为序列特征**和**其他特征**。其中其他特征主要包括：用户画像特征、item相关特征、上下文特征、交叉特征，如下表所示：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/718a7f24-7e33-4b65-8c5c-ea15fbb7f7a2.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "所有其他特征通过embedding层后，得到对应的低维向量表示，将所有的低维向量concat到一起。\n",
    "\n",
    "用户行为序列特征主要包含两部分，一个是item相关的特征（Sequence Item Features），一个是位置特征（Positional Features）。\n",
    "- item相关的特征主要包含item_id和category_id（当然也可以考虑使用其他item相关的特征，文中说明在其业务场景下使用item_id和category_id即可取得足够好的效果）。\n",
    "- 位置特征主要用来刻画用户历史行为序列中的顺序信息，文中通过自定义的计算方式$pos(v_i)=t(v_t)-t(v_i)$计算每个item的位置，其中$t(v_t)$表示推荐的时间戳，$t(v_i)$表示用户点击商品$v_i$的时间戳，然后将item的位置通过embedding层投射为低维向量。\n",
    "\n",
    "#### Transformer Layer\n",
    "Transformer Layer 主要包含三个部分：Self-attention layer，Point-wise Feed-Forward Networks和Stacking the self-attention blocks。\n",
    "\n",
    "##### Self-attention layer\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/9cea67bc-dc34-40fb-abd5-bd751bf956ca.png\" style=\"zoom:30%; display: block; margin: auto;\" />\n",
    "\n",
    "self-attention主要结构如上图所示，计算时需要用到Q(查询)、K(键值)、V(值)矩阵。self-attention接收的输入是embedding层的输出矩阵$E$，$W^Q, W^K, W^V \\in \\mathbb{R} ^ {d \\times d}$，$Q、K、V$的计算方式如下：\n",
    "\n",
    "$$\n",
    "Q=EW^Q\n",
    "$$\n",
    "\n",
    "$$\n",
    "K=EW^K\n",
    "$$\n",
    "\n",
    "$$\n",
    "V=EW^V\n",
    "$$\n",
    "\n",
    "attention计算方式为：\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V\n",
    "$$\n",
    "\n",
    "multi-head attention计算方式为：\n",
    "\n",
    "$$\n",
    "S=MH(E)=Concate(head_1, head_2, ..., head_h)W^H\n",
    "$$\n",
    "\n",
    "$$\n",
    "head_i = Attention(EW^Q, EW^K, EW^V)\n",
    "$$\n",
    "\n",
    "因此，self-attention的最终输出为矩阵S（实际还会做进一步处理，后文会进一步说明）。\n",
    "\n",
    "##### Point-wise Feed-Forward Networks\n",
    "\n",
    "BST模型也通过添加FFN网络来增加模型非线性，简单形式为：\n",
    "\n",
    "$$\n",
    "F=FFN(S)\n",
    "$$\n",
    "\n",
    "本文中的FFN其实就是一个简单的二层网络，其中第一层的激活函数为为LeakyRelu，对于输入序列中每个位置上的向量x：\n",
    "\n",
    "$$\n",
    "FFN(x) = LeakyRelu(xW^{(1)} + b^{(1)})W^{(2)}+b^{(2)}\n",
    "$$\n",
    "\n",
    "##### 残差连接、Dropout、LayerNorm\n",
    "\n",
    "在Self-attention layer和Point-wise Feed-Forward Networks中均能看到残差连接、Dropout和LayerNorm的影子，其中：\n",
    "\n",
    "- 残差连接主要是为了解决多层网络训练的问题\n",
    "- Dropout主要是为了防止模型过拟合\n",
    "- LayerNorm(Layer Normalization)通常用于RNN结构，会将每一层神经元的输入都转成均值方差都一样的，可以加快模型收敛\n",
    "\n",
    "**实际上self-attention的最终输出为：**\n",
    "$$\n",
    "S' = LayerNorm(S+Dropout(S))\n",
    "$$\n",
    "\n",
    "**Feed-Forward Networks(FFN)的最终形式为：**\n",
    "\n",
    "$$\n",
    "F=LayerNorm(S' + Dropout(LeakyRelu(S'W^{(1)} + b^{(1)})W^{(2)}+b^{(2)}))\n",
    "$$\n",
    "\n",
    "##### Stacking the self-attention blocks\n",
    "\n",
    "这一部分即上文介绍的self-attention + FFN的堆叠结构（本文中尝试过1~3层的堆叠结构，单层结构效果是最好的）。\n",
    "\n",
    "$$\n",
    "S^b = SA(F^{(b-1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "F^b = FFN(S^b), \\forall_i \\in 1,2, ..., n.\n",
    "$$\n",
    "\n",
    "#### MLP layers and Loss function\n",
    "\n",
    "将所有的embedding进行拼接，输入到三层的神经网络中，并最终通过sigmoid函数转换为0-1之间的值，代表用户点击目标商品的概率。loss函数为：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/82fed813-2ed5-4c65-bad0-4ed01ae9b3b6.png\" style=\"zoom: 36%; display: block; margin: auto;\" />\n",
    "\n",
    "### 实验数据\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ba1c19d6-5cb5-47e8-a9d9-624458d28f15.png\" style=\"zoom:36%; display: block; margin: auto;\" />\n",
    "\n",
    "可以看到BST模型与其他模型相比离线AUC评估指标有大幅度的提升，线上A/B实验，点击率也有大幅度的提升，但模型性能开销也有相应的提升（平均响应时长有一定程度的上涨）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3b82d",
   "metadata": {},
   "source": [
    "## 阿里DMR（attention）\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. [阿里DMR：融合了匹配思想的深度排序模型-Deep Match to Rank](https://zhuanlan.zhihu.com/p/163096847)\n",
    "> 2. [阿里巴巴DMR模型(Deep Match to Rank)](https://zhuanlan.zhihu.com/p/418447193)\n",
    "> 3. [推荐系统序列化建模总结（二）](https://developer.aliyun.com/article/898525)\n",
    "\n",
    "DIN等模型将学习到的Sequence Embedding（用户兴趣向量）、User Profile、待排序物品特征等Concat后送入最上层的MLP进行特征交叉最终输出一个CTR预估分数，作者认为在Concat特征送入MLP进行交叉前就计算一个User和Item相关性可以降低模型的学习难度。\n",
    "\n",
    "DMR可以看做是另一种对DIN的改进方式。文章提出了两种网络结构，Item-to-Item网络和User-to-Item网络，来描述用户和候选目标item是否匹配。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20230101003506412-2545203.png\" alt=\"image-20230101003506412\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "### Item-to-Item Network\n",
    "\n",
    "$e_i$是用户行为序列中每个item的embedding，$p_i$是用户行为在序列中的位置embedding，文章通过融合行为embedding、位置embedding和target item embedding计算attention权重。\n",
    "\n",
    "$$\n",
    "\\hat a_t = \\hat z^T tanh(\\hat W_c e_c + \\hat W_p p_t + \\hat W_e e_t + \\hat b)\n",
    "$$\n",
    "\n",
    "通过softmax得到最终权重，将权重作用到每个行为embedding上，通过sum pooling的方式得到用户行为序列的向量表示。\n",
    "\n",
    "$$\n",
    "\\hat \\alpha_t = \\frac {exp(\\hat a_t)}{\\sum_{i=1}^T exp(\\hat a_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat u = \\sum_{t=1}^T(\\hat \\alpha_t \\hat e_t)\n",
    "$$\n",
    "\n",
    "此外item-to-item network子网络的输出还包括target item embedding，以及sofatmax之前各个行为embedding的attention权重和。\n",
    "\n",
    "### User-to-Item Network\n",
    "\n",
    "在计算attention的时候并不考虑target item。\n",
    "\n",
    "一般来说，用户的兴趣会随着时间发生变化，距离现在更近的行为更能反映用户的真实兴趣。根据用户行为发生的时间为用户行为指定权重可以解决这个问题，采用attention机制，把用户行为出现的位置按时间排序后，用数字编码，当做query（**这里也可以根据具体业务替换为其他重要的特征**），自主地为每个行为计算attention权重。\n",
    "\n",
    "$$\n",
    "a_t = z^T tanh(W_p p_t + W_e e_t + b)\n",
    "$$\n",
    "\n",
    "通过softmax得到最终的attention权重：\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac {exp(a_t)}{\\sum_{i=1}^T exp(a_i)}\n",
    "$$\n",
    "\n",
    "将权重作用到每个行为embedding上，最后再通过sum pooling + 非线性变换的方式得到用户行为序列的向量表示。\n",
    "\n",
    "$$\n",
    "u = g(\\sum_{t=1}^T(\\alpha_t e_t)) = g(\\sum_{t=1}^T(h_t))\n",
    "$$\n",
    "\n",
    "最后跟target item embedding 做内积运算来表示用户和目标商品的匹配程度，最终输入到MLP网络(同样的问题，这种单值特征的作用大么？)。\n",
    "\n",
    "$$\n",
    "r = u^T v'\n",
    "$$\n",
    "\n",
    "引入一个辅助训练网络来帮助训练，确保点积结果越大代表用户和target item的相关性越高。在辅助训练的时候，使用行为序列中的前T-1个行为学习用户表示，而用最后一个行为作为正样本，使用负采样的方法随机获得负样本。$p_j$表示匹配分值，$L_{NS}$表示辅助网络loss，$L_{final}$表示最终\n",
    "loss。\n",
    "\n",
    "$$\n",
    "p_j = \\frac {exp(u_{T-1}^T v'_j)}{\\sum_{i=1}^K exp(u_{T-1}^T v'_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{aux} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^K y_j^i log(p_j^i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{NS} = - \\frac {1}{N} \\sum_{i=1}^N log(\\sigma(u_{T-1}^T v'_o)) + \\sum_{j=1}^k log(\\sigma(-u_{T-1}^T v'_j))\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{final} = L_{target} + \\beta L_{NS}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a53bde4",
   "metadata": {},
   "source": [
    "## 阿里MIND\n",
    "\n",
    "推荐系统一般包含两个重要阶段：召回和排序，召回阶段负责从海量候选中选出用户感兴趣的候选集，排序阶段再对用户感兴趣的候选集进行排序取出TOPN。在这两个阶段一个很重要的因素就是如何建模表征用户兴趣，目前大部分模型都以单向量表征用户，MIND模型提出了一种用多个向量表征用户的思路。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/291d21a2-21fb-4d98-ba9e-966a11e9e2e0.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "模型的核心思想：\n",
    "\n",
    "- 利用multi-interest提取层（以胶囊网络和动态路由算法为基础）结合用户历史行为，提取用户的多元兴趣。\n",
    "- 通过label-aware attention机制帮助学习用户的多元向量表示。\n",
    "\n",
    "### 模型介绍\n",
    "\n",
    "#### 模型整体结构图\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d726d977-840a-47b1-b002-75d4a55fe964.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "#### 模型定义\n",
    "\n",
    "首先介绍一下三元组$(I_u, P_u, F_i)$中各个组成部分的含义，$I_u$表示与用户交互过的商品（用户历史行为），$P_u$表示用户画像特征（例如年龄、性别等），$F_i$表示商品的特征（例如商品id、一级分类等）。\n",
    "\n",
    "MIND的核心是学习以下函数表达：\n",
    "\n",
    "$$\n",
    "V_u = f_{user}(I_u, P_u)\n",
    "$$\n",
    "\n",
    "其中 $V_u$ 是 $d \\times K$ 维的矩阵，d表示每个向量的维度，K表示向量个数。\n",
    "\n",
    "目标商品的向量表示可以定义为：\n",
    "\n",
    "$$\n",
    "\\vec{e_i} = f_{item}(F_i)\n",
    "$$\n",
    "\n",
    "其中${e_i}$为d维向量。\n",
    "\n",
    "模型候选打分计算公式：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/8db57b6c-31bc-4849-bb54-1d94980ec949.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "即用户的每个向量与候选物料计算打分取最大值作为模型最终打分。\n",
    "\n",
    "\n",
    "#### 模型结构\n",
    "\n",
    "模型主要分为以下四部分：\n",
    "\n",
    "- Embedding & Pooling Layer\n",
    "- Multi-Interest Extractor Layer\n",
    "- Label-aware Attention Layer\n",
    "- Training & Serving\n",
    "\n",
    "##### Embedding & Pooling Layer\n",
    "\n",
    "这一层逻辑比较简单，$I_u$、$P_u$、$F_i$均通过embedding层转化为embedding向量，而后通过average pooling得到最终的向量表示。\n",
    "\n",
    "##### Multi-Interest Extractor Layer\n",
    "\n",
    "这一层通过胶囊网络学习用户的多个兴趣向量表示，来表示用户的多种兴趣。这一层涉及到胶囊网络和动态路由算法，会在其他文章中详细展开。\n",
    "\n",
    "##### Label-aware Attention Layer\n",
    "\n",
    "这一部分的原理其实也比较简单，这一层的输入即为通过Multi-Interest Extractor Layer提取到的多个用户兴趣向量（兴趣胶囊），每个用户兴趣向量表示不同的用户兴趣，通过Label-aware注意力机制确定通过这些用户兴趣向量计算最终的用户向量$\\vec{v_{u}}$时各自的权重。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/dc31472b-babe-411e-8aba-6de244da3a81.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "其中p为调节参数：\n",
    "\n",
    "- 当p为0时，那么每个用户兴趣向量有相同的权重\n",
    "- 当p>1时，p越大，与目标商品向量点积更大的用户兴趣向量会有更大的权重\n",
    "- 当p为无穷大时，实际上就相当于只使用与目标商品向量点积最大的用户兴趣向量，忽略其他用户向量，可以理解是每次只激活一个兴趣向量，文章指出使用这种方法收敛最快。\n",
    "\n",
    "#### Training & Serving\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a4f337df-f794-4099-87bc-23bfcd7acd40.png\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "得到用户向量$\\vec{v_{u}}$和商品向量$\\vec{e_{i}}$之后，通过下式可以计算得到用户对商品的交互概率：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d0ed74d8-2a9c-4b0e-9432-2149568e9951.png\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "模型的目标函数：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/8b0c7794-2ebd-44cc-a85d-de920b7587b2.png\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "模型采用了sampled softmax的方式降低开销。\n",
    "\n",
    "在Serving阶段，算出每个用户的多个向量后，每个向量都可用于为用户召回商品，最终从所有召回结果中挑选出TOPN。\n",
    "\n",
    "整个过程可以参考Youtube DNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcba5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
