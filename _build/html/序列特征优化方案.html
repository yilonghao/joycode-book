
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>序列特征优化方案 &#8212; JoyCode</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '序列特征优化方案';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="特征交叉" href="%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89.html" />
    <link rel="prev" title="双塔模型优化思路" href="%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">JoyCode</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    欢迎访问JoyCode
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html">推荐&amp;广告领域常见的业务指标</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80.html">技术基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html">双塔模型优化思路</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">序列特征优化方案</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89.html">特征交叉</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%A4%A7%E5%8E%82%E5%AE%9E%E8%B7%B5.html">大厂实践</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book/issues/new?title=Issue%20on%20page%20%2F序列特征优化方案.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/序列特征优化方案.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>序列特征优化方案</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#base-model">Base Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#din-deep-interest-network">DIN(Deep Interest Network)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#din">DIN核心思想</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-aware-regularization-mbar">mini-batch aware regularization(MBAR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dice">自适应激活函数Dice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dien">DIEN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">模型结构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-extractor-layer">Interest Extractor Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">辅助损失函数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-evolving-layer">Interest Evolving Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attentiongru-augru">将attention机制融入GRU结构中(AUGRU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dsin">DSIN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">模型结构（整体）</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding">embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">建模用户序列</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-division-layer">session division layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer">session interest extractor layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer">Session Interest Interacting Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-activating-layer">session interest activating layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer-activation-unit">session interest extractor layer 相关 activation unit</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer-activation-unit">session interest interacting layer 相关 activation unit</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp">MLP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">实验数据</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender">京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-layer">Deep Multifaceted Transformers Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-deep-neural-network">Bias Deep Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gate-mixture-of-experts-layers">Multi-gate Mixture-of-Experts Layers</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#序列特征优化方案" data-toc-modified-id="序列特征优化方案-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>序列特征优化方案</a></span><ul class="toc-item"><li><span><a href="#Base-Model" data-toc-modified-id="Base-Model-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Base Model</a></span></li><li><span><a href="#DIN(Deep-Interest-Network)" data-toc-modified-id="DIN(Deep-Interest-Network)-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>DIN(Deep Interest Network)</a></span><ul class="toc-item"><li><span><a href="#DIN核心思想" data-toc-modified-id="DIN核心思想-1.2.1"><span class="toc-item-num">1.2.1&nbsp;&nbsp;</span>DIN核心思想</a></span></li><li><span><a href="#mini-batch-aware-regularization(MBAR)" data-toc-modified-id="mini-batch-aware-regularization(MBAR)-1.2.2"><span class="toc-item-num">1.2.2&nbsp;&nbsp;</span>mini-batch aware regularization(MBAR)</a></span></li><li><span><a href="#自适应激活函数Dice" data-toc-modified-id="自适应激活函数Dice-1.2.3"><span class="toc-item-num">1.2.3&nbsp;&nbsp;</span>自适应激活函数Dice</a></span></li></ul></li><li><span><a href="#DIEN" data-toc-modified-id="DIEN-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>DIEN</a></span><ul class="toc-item"><li><span><a href="#模型结构" data-toc-modified-id="模型结构-1.3.1"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>模型结构</a></span></li><li><span><a href="#Interest-Extractor-Layer" data-toc-modified-id="Interest-Extractor-Layer-1.3.2"><span class="toc-item-num">1.3.2&nbsp;&nbsp;</span>Interest Extractor Layer</a></span></li><li><span><a href="#辅助损失函数" data-toc-modified-id="辅助损失函数-1.3.3"><span class="toc-item-num">1.3.3&nbsp;&nbsp;</span>辅助损失函数</a></span></li><li><span><a href="#Interest-Evolving-Layer" data-toc-modified-id="Interest-Evolving-Layer-1.3.4"><span class="toc-item-num">1.3.4&nbsp;&nbsp;</span>Interest Evolving Layer</a></span></li><li><span><a href="#将attention机制融入GRU结构中(AUGRU)" data-toc-modified-id="将attention机制融入GRU结构中(AUGRU)-1.3.5"><span class="toc-item-num">1.3.5&nbsp;&nbsp;</span>将attention机制融入GRU结构中(AUGRU)</a></span></li></ul></li><li><span><a href="#DSIN" data-toc-modified-id="DSIN-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>DSIN</a></span><ul class="toc-item"><li><span><a href="#模型结构（整体）" data-toc-modified-id="模型结构（整体）-1.4.1"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>模型结构（整体）</a></span></li><li><span><a href="#embedding" data-toc-modified-id="embedding-1.4.2"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>embedding</a></span></li><li><span><a href="#建模用户序列" data-toc-modified-id="建模用户序列-1.4.3"><span class="toc-item-num">1.4.3&nbsp;&nbsp;</span>建模用户序列</a></span><ul class="toc-item"><li><span><a href="#session-division-layer" data-toc-modified-id="session-division-layer-1.4.3.1"><span class="toc-item-num">1.4.3.1&nbsp;&nbsp;</span>session division layer</a></span></li><li><span><a href="#session-interest-extractor-layer" data-toc-modified-id="session-interest-extractor-layer-1.4.3.2"><span class="toc-item-num">1.4.3.2&nbsp;&nbsp;</span>session interest extractor layer</a></span></li><li><span><a href="#Session-Interest-Interacting-Layer" data-toc-modified-id="Session-Interest-Interacting-Layer-1.4.3.3"><span class="toc-item-num">1.4.3.3&nbsp;&nbsp;</span>Session Interest Interacting Layer</a></span></li><li><span><a href="#session-interest-activating-layer" data-toc-modified-id="session-interest-activating-layer-1.4.3.4"><span class="toc-item-num">1.4.3.4&nbsp;&nbsp;</span>session interest activating layer</a></span></li><li><span><a href="#session-interest-extractor-layer-相关-activation-unit" data-toc-modified-id="session-interest-extractor-layer-相关-activation-unit-1.4.3.5"><span class="toc-item-num">1.4.3.5&nbsp;&nbsp;</span>session interest extractor layer 相关 activation unit</a></span></li><li><span><a href="#session-interest-interacting-layer-相关-activation-unit" data-toc-modified-id="session-interest-interacting-layer-相关-activation-unit-1.4.3.6"><span class="toc-item-num">1.4.3.6&nbsp;&nbsp;</span>session interest interacting layer 相关 activation unit</a></span></li></ul></li><li><span><a href="#MLP" data-toc-modified-id="MLP-1.4.4"><span class="toc-item-num">1.4.4&nbsp;&nbsp;</span>MLP</a></span></li><li><span><a href="#实验数据" data-toc-modified-id="实验数据-1.4.5"><span class="toc-item-num">1.4.5&nbsp;&nbsp;</span>实验数据</a></span></li></ul></li><li><span><a href="#京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender" data-toc-modified-id="京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a></span><ul class="toc-item"><li><span><a href="#简介" data-toc-modified-id="简介-1.5.1"><span class="toc-item-num">1.5.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href="#Deep-Multifaceted-Transformers-Layer" data-toc-modified-id="Deep-Multifaceted-Transformers-Layer-1.5.2"><span class="toc-item-num">1.5.2&nbsp;&nbsp;</span>Deep Multifaceted Transformers Layer</a></span></li><li><span><a href="#Bias-Deep-Neural-Network" data-toc-modified-id="Bias-Deep-Neural-Network-1.5.3"><span class="toc-item-num">1.5.3&nbsp;&nbsp;</span>Bias Deep Neural Network</a></span></li><li><span><a href="#Multi-gate-Mixture-of-Experts-Layers" data-toc-modified-id="Multi-gate-Mixture-of-Experts-Layers-1.5.4"><span class="toc-item-num">1.5.4&nbsp;&nbsp;</span>Multi-gate Mixture-of-Experts Layers</a></span></li></ul></li></ul></li></ul></div><section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>序列特征优化方案<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="base-model">
<h2>Base Model<a class="headerlink" href="#base-model" title="Link to this heading">#</a></h2>
<p>传统基于用户行为序列描述用户兴趣的方法是把序列中各个item的embedding通过pooling的方式转化成一个固定维的embedding。如下图所示，其中红色节点表示商品ID，蓝色节点表示店铺iD，粉色节点表示类目ID，白色节点表示用户特征和上下文特征，Goods 1 ~ Goods N 用来描述用户的历史行为，候选广告（Candidate Ad）本身也是商品。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/dec039c0-ee95-4d4d-9246-4038e11812f2.png" alt="image" style="zoom: 33%;" />
<p>网络结构具有的缺点是经过pooling以后的向量与候选广告无关，对于一个用户来说是固定不变的。对于不同的候选广告，与之对应的用户兴趣分布也应该不通。因为只有用户部分的兴趣会影响当前行为（对候选广告点击或不点击）。</p>
</section>
<section id="din-deep-interest-network">
<h2>DIN(Deep Interest Network)<a class="headerlink" href="#din-deep-interest-network" title="Link to this heading">#</a></h2>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/9223919b-9c70-4403-a519-54a0b10e8b3b.png" alt="image" style="zoom: 67%;" />
<section id="din">
<h3>DIN核心思想<a class="headerlink" href="#din" title="Link to this heading">#</a></h3>
<p>DIN的核心思想是，对每一个用户，不同的广告有不同的向量表示，结合用户行为特征与给定的广告为每个用户行为计算权重，引入local-activation机制有侧重的利用用户不同的行为特征，其中<span class="math notranslate nohighlight">\(e_j\)</span>表示用户行为向量，<span class="math notranslate nohighlight">\(v_A\)</span>为候选广告的向量，a表示激活单元，<span class="math notranslate nohighlight">\(a(e_j,v_A)\)</span>为权重。整体逻辑为SUM Pooling。</p>
<p><img alt="image" src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/c2eb0b1e-f6a3-4cfd-99ad-3b8537a0963b.png" /></p>
<p>相比基础的深度推荐网络，DIN在生成用户向量的时候加了一个activation unit单元，计算每个用户行为与候选广告之间的权重。在传统的attention机制中，给定两个向量，比如u和v，通常直接做点乘。这篇论文中做了进一步的改进：首先是把u和v以及uv的外积合并起来作为输入给全连接层，最后得到权重，这样可以减少信息损失。论文中还放宽了权重加和等于1的限制，这样更有利于体现用户行为特征之间的差异化程度。</p>
</section>
<section id="mini-batch-aware-regularization-mbar">
<h3>mini-batch aware regularization(MBAR)<a class="headerlink" href="#mini-batch-aware-regularization-mbar" title="Link to this heading">#</a></h3>
<p>为解决大规模稀疏场景下，采用SGD对引入L2正则的loss进行更新时计算开销过大的问题，该方法只对每一个mini-batch中参数不为0的进行梯度更新。</p>
</section>
<section id="dice">
<h3>自适应激活函数Dice<a class="headerlink" href="#dice" title="Link to this heading">#</a></h3>
<p>PRelu激活函数如下所示：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/5f30f50e-8c69-4212-8692-940457ec8b7a.png" style="zoom: 67%;" />
<p>采用PRelu激活函数时，它的rectified point固定为0，这在每一层的输入分布发生变化时是不适用的，所以文章对该激活函数做了改进，平滑了rectified point附近曲线的同时，激活函数会根据每层输入数据的分布来自适应调整rectified point的位置，具体形式如下：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ccac1f77-49db-4cc2-ab87-186e5637c1f2.png" style="zoom: 67%;" />
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/81556967-3897-407e-8496-2d98aa7bc705.png" style="zoom: 50%;" /></section>
</section>
<section id="dien">
<h2>DIEN<a class="headerlink" href="#dien" title="Link to this heading">#</a></h2>
<p>阿里妈妈的精准定向检索及基础算法团队以 Deep Interest Network(DIN) 算法为基础，进一步优化升级，产出了 Deep Interest Evolution Network(DIEN)算法，主要解决了以下两个问题：</p>
<ul class="simple">
<li><p>更加精确的刻画用户的长期兴趣和短期兴趣</p></li>
<li><p>用户的兴趣是时刻变化的，需要更加准确的刻画用户兴趣的变化</p></li>
</ul>
<section id="id2">
<h3>模型结构<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>如图所示，DIEN 由 embedding 层、Interest Extractor Layer 层，Interest Evolving Layer 层组成。Interest Extractor Layer 根据行为序列提取兴趣序列，Interest Evolving Layer 基于 Target Ad 对兴趣序列进行建模，得到用户兴趣向量表示，最终将用户兴趣向量表示与其他特征向量 concat 到一起后送到 MLP 中以进行最终预测。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1c958050-4694-4a23-8ca2-ac929d53453a.png" style="zoom:50%;" />
</section>
<section id="interest-extractor-layer">
<h3>Interest Extractor Layer<a class="headerlink" href="#interest-extractor-layer" title="Link to this heading">#</a></h3>
<p>兴趣提取层的目的就是从用户的行为序列中提取出一系列的兴趣状态。为了平衡效率和性能，作者选择了GRU（Gated Recurrent Unit，门循环单元）网络来对用户行为之间的依赖进行建模。GRU既能够克服RNN梯度消失的问题，同时又比LSTM网络具有更少的参数，训练时收敛速度更快。GRU单元的表达式如下：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/225b086d-f739-42b0-bcb6-b5a1d6a2cbae.png" style="zoom: 33%;" />
<p>其中 <span class="math notranslate nohighlight">\(\sigma\)</span>表示sigmoid激活函数，<span class="math notranslate nohighlight">\(\circ\)</span>表示元素积(element-wise product)，<span class="math notranslate nohighlight">\(W\)</span>和<span class="math notranslate nohighlight">\(U\)</span>表示隐藏层参数，<span class="math notranslate nohighlight">\(i_t\)</span>表示GRU单元的输入（用户第t个行为的embedding向量），<span class="math notranslate nohighlight">\(h_{t}\)</span>表示第t个GRU单元的隐层状态。关于GRU结构的介绍可以参考文章：<a class="reference external" href="https://zhuanlan.zhihu.com/p/463521382">GRU学习笔记</a>，主结构如下：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6ee03222-480d-457c-9509-a6270d8a4223.png" style="zoom: 33%;" />
</section>
<section id="id3">
<h3>辅助损失函数<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a3592fc6-81c0-4461-b6fd-e98f90109bb6.png" style="zoom:50%;" />
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/44c87c81-7b82-4438-9b8c-3fddbdd69173.png" alt="辅助Loss计算示意图" style="zoom:50%;" />
<p>其中 <span class="math notranslate nohighlight">\(\{e_{b}^{i},\hat e_{b}^{i}\} \in D_{\beta},i=1,2,3,...,N\)</span>表示N对行为embedding序列，<span class="math notranslate nohighlight">\(e_{b}^{i} \in \mathbb{R}^{T  \times n_E}\)</span>表示用户的点击行为序列，<span class="math notranslate nohighlight">\(\hat e_{b}^{i}\in \mathbb{R}^{T  \times n_E}\)</span>表示用户没有点击的行为序列。<span class="math notranslate nohighlight">\(T\)</span>表示序列中历史行为的数量，<span class="math notranslate nohighlight">\(n_E\)</span>表示embedding向量的维度。<span class="math notranslate nohighlight">\(e_{b}^{i}[t]\)</span>表示第i个用户第t次点击行为的embedding向量。其中：
$<span class="math notranslate nohighlight">\(
\sigma(x_1, x_2) = \frac{1}{exp(-[x_1, x_2])}
\)</span>$</p>
<p>DIEN使用的整体损失函数是：</p>
<div class="math notranslate nohighlight">
\[
L = L_{target} + \alpha * L_{aux}
\]</div>
</section>
<section id="interest-evolving-layer">
<h3>Interest Evolving Layer<a class="headerlink" href="#interest-evolving-layer" title="Link to this heading">#</a></h3>
<p>兴趣进化层Interest Evolution Layer的主要目标是刻画用户兴趣的进化过程。兴趣进化层的数据就是兴趣提取层的输出，兴趣进化层结合了注意力机制中的局部激活能力和GRU的序列学习能力来建模。attention部分系数计算方式如下：
$<span class="math notranslate nohighlight">\(
a_t = \frac{exp(h_{t}W_{e_{a}})}{\sum_{j=1}^Texp(h_{j}W_{e_{a}})}
\)</span>$</p>
</section>
<section id="attentiongru-augru">
<h3>将attention机制融入GRU结构中(AUGRU)<a class="headerlink" href="#attentiongru-augru" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/71b8130c-bdf8-4cdf-86f4-e5eccfcdd689.png" style="zoom:50%;" />
<p>通过将attention部分的计算结果作用到GRU结构的更新门上将attention机制融入到GRU结构中。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/664e6fa8-e51d-4fc0-8077-3c26c81319f6.png" style="zoom: 33%;" /></section>
</section>
<section id="dsin">
<h2>DSIN<a class="headerlink" href="#dsin" title="Link to this heading">#</a></h2>
<p>这篇文章主要介绍阿里在2019年发表的排序阶段模型：Deep Session Interest Network for Click-Through Rate Prediction。主要思路为将用户的历史点击行为划分为不同session，而后通过Transformer结构学习每个session的向量表示，最后通过BiLSTM结构对session序列进行建模，整体来说具有一定的参考意义。</p>
<section id="id4">
<h3>模型结构（整体）<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/11a806f7-3775-44b0-83db-77a199ef130d.png" style="zoom:50%;" />
</section>
<section id="embedding">
<h3>embedding<a class="headerlink" href="#embedding" title="Link to this heading">#</a></h3>
<p>从图中可以看出模型主要由三大部分组成，其中第一部分（part1）主要处理用户画像特征（年龄、性别、所在城市等）和item侧特征（seller id, brand id等），处理方式也很简单，所有特征通过embedding的方式得到对应的向量表示，最终用户侧特征向量表示为<span class="math notranslate nohighlight">\(X^U \in \mathbb{R}^{N_u \times d_{model}}\)</span>，其中<span class="math notranslate nohighlight">\(N_u\)</span>表示用户侧特征个数，item侧特征的向量表示为：<span class="math notranslate nohighlight">\(X^I \in \mathbb{R}^{N_i \times d_{model}}\)</span>，其中<span class="math notranslate nohighlight">\(N_i\)</span>表示item侧特征个数。</p>
</section>
<section id="id5">
<h3>建模用户序列<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>这一部分（part2）主要用来建模用户行为序列，主要包含4部分：</p>
<ul class="simple">
<li><p>session division layer: 对用户行为序列进行划分</p></li>
<li><p>session interest extractor layer: 学习每个session的向量表示</p></li>
<li><p>session interest interacting layer: 对session序列进行建模</p></li>
<li><p>session interest activating layer: 引入attention机制</p></li>
</ul>
<section id="session-division-layer">
<h4>session division layer<a class="headerlink" href="#session-division-layer" title="Link to this heading">#</a></h4>
<p>主要作用就是将用户的历史点击行为序列划分为多个sessions。整个操作的符号表示为将用户的行为序列<span class="math notranslate nohighlight">\(S\)</span>划分为sessions <span class="math notranslate nohighlight">\(Q\)</span>，第<span class="math notranslate nohighlight">\(k\)</span>个session的表示形式为：</p>
<div class="math notranslate nohighlight">
\[
Q_k = [b_1; ...; bi; ...; b_T] \in \mathbb{R}^{T \times d_{model}}
\]</div>
<p>其中<span class="math notranslate nohighlight">\(T\)</span>为当前session对应的序列长度，<span class="math notranslate nohighlight">\(b_i\)</span>为用户在当前session中的第<span class="math notranslate nohighlight">\(i\)</span>次点击行为，文中session的划分是按照时间间隔来的，两次点击之间的间隔超过30min则将下一次点击行为计入下一个session。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/23182b90-adff-4fb1-8841-b0b18e78a747.jpg" style="zoom:50%;" />
</section>
<section id="session-interest-extractor-layer">
<h4>session interest extractor layer<a class="headerlink" href="#session-interest-extractor-layer" title="Link to this heading">#</a></h4>
<p>这部分的主要作用就是学习每个session的向量表示，文中使用了multi-head self-attention的结构对每个session建模。为了刻画不同session间的顺序，DSIN使用了Bias Encoding，其中<span class="math notranslate nohighlight">\(BE \in \mathbb{R}^{K \times T \times d_{model}}\)</span>。Bias Encoding 计算方式如下：</p>
<div class="math notranslate nohighlight">
\[
BE_{(k,t,c)} = w_k^K + w_t^T + w_c^C
\]</div>
<p>其中<span class="math notranslate nohighlight">\(BE_{(k,t,c)}\)</span>表示第k个session中的第t个物品的embedding向量中的第c个位置对应的bias，加入bias encoding后，用户的session表示为：</p>
<div class="math notranslate nohighlight">
\[
Q=Q+BE
\]</div>
<p>Multi-head Self-attention的计算逻辑为：
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0a020f15-557f-48a9-b49f-cd177c1af371.png" style="zoom: 50%;" />
$<span class="math notranslate nohighlight">\(
I_k^Q = FFN(Concat(head_1, ..., head_H)W^O)
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
I_k = Avg(I_k^Q)
\]</div>
</section>
<section id="session-interest-interacting-layer">
<h4>Session Interest Interacting Layer<a class="headerlink" href="#session-interest-interacting-layer" title="Link to this heading">#</a></h4>
<p>这部分主要通过Bi-LSTM结构对session序列进行建模。</p>
<div class="math notranslate nohighlight">
\[
H_t = \mathop{h_{ft}}\limits ^{\rightarrow} \oplus \mathop{h_{bt}}\limits ^{\leftarrow}
\]</div>
<p>其中<span class="math notranslate nohighlight">\(\mathop{h_{ft}}\limits ^{\rightarrow}\)</span>表示forward隐藏层状态，<span class="math notranslate nohighlight">\(\mathop{h_{bt}}\limits ^{\leftarrow}\)</span>表示backward隐藏层状态。</p>
</section>
<section id="session-interest-activating-layer">
<h4>session interest activating layer<a class="headerlink" href="#session-interest-activating-layer" title="Link to this heading">#</a></h4>
<p>这部分主要是通过attention机制刻画目标item和session之间的相关性。主要思想是若session与目标item之间的相关性越高，则应该赋予越大的权重。模型结构中共有两个Activation Unit，结构是一致的。</p>
</section>
<section id="session-interest-extractor-layer-activation-unit">
<h4>session interest extractor layer 相关 activation unit<a class="headerlink" href="#session-interest-extractor-layer-activation-unit" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0f1a1c33-3507-4375-8fb3-8a7640b1d777.png" style="zoom: 33%;" />
</section>
<section id="session-interest-interacting-layer-activation-unit">
<h4>session interest interacting layer 相关 activation unit<a class="headerlink" href="#session-interest-interacting-layer-activation-unit" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a5717872-e0b2-4240-8d6a-80b5d51b4183.png" style="zoom: 33%;" />
</section>
</section>
<section id="mlp">
<h3>MLP<a class="headerlink" href="#mlp" title="Link to this heading">#</a></h3>
<p>这一部分（part3）为简单的MLP结构，首先将得到的所有向量表示concat到一起，而后通过2层前馈神经网络，最后通过一个softmax层得到输出。损失函数为：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/4e742bb1-d068-47c4-93c3-46351e119807.png" style="zoom:33%;" />
<p>其中 <span class="math notranslate nohighlight">\(\mathbb{D}\)</span>表示训练集，<span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>，<span class="math notranslate nohighlight">\(p(\cdot)\)</span>表示网络最终的输出结果，表示用户点击目标item的概率。</p>
</section>
<section id="id6">
<h3>实验数据<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>从实验结果来看，AUC相比其他模型均有一定幅度的提升。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/f67a91a0-f19b-47b6-95ba-a1ae66ae1a0c.png" style="zoom:50%;" /></section>
</section>
<section id="deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender">
<h2>京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender<a class="headerlink" href="#deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>推荐阅读⭐️⭐️⭐️⭐️⭐️</p>
<ul class="simple">
<li><p>paper: Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/whgyxy/article/details/126023948">论文《Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender》</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/534414021">ctr预估：DMT模型</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/55752344">详解谷歌之多任务学习模型MMoE(KDD 2018)</a></p></li>
</ul>
</div></blockquote>
<section id="id7">
<h3>简介<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>模型应用的业务场景商品搜索排序阶段，论文提出用多个Transfomer对用户多种类型的行为序列进行建模，在此基础上叠加MMOE建模多目标，最后使用一个消偏塔对数据进行消偏。</p>
<p><img alt="image-20221231142523288" src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221231142523288-2501956.png" /></p>
</section>
<section id="deep-multifaceted-transformers-layer">
<h3>Deep Multifaceted Transformers Layer<a class="headerlink" href="#deep-multifaceted-transformers-layer" title="Link to this heading">#</a></h3>
<p>Embedding Layer对每个物料使用物料id、类目id、品牌id、商铺id分别映射成低维向量，然后concat起来，形成向量；Dense特征使用了Z-score归一化。模型使用了点击，加购，成交3个Item Sequence，分别表征短期，中期和长期兴趣；分别用3个Transformer来对点击、加入购物车、购买行为序列进行建模：</p>
<ul class="simple">
<li><p>encoder：用序列的item-embedding作为self-attention的输入</p></li>
<li><p>decoder：使用target item的embedding作为query，encoder输出的结果作为key和value。</p></li>
</ul>
</section>
<section id="bias-deep-neural-network">
<h3>Bias Deep Neural Network<a class="headerlink" href="#bias-deep-neural-network" title="Link to this heading">#</a></h3>
<p>Bais塔的输入都是偏差相关的特征，对于位置偏差输入就是展示位置索引编号或者网页索引编号；对于近邻偏差，输入就是目标物料的类目和邻近K个物料的类目。Bias建模部分使用Bias特征+MLP，输出的Logits与主网络Logits相加。</p>
</section>
<section id="multi-gate-mixture-of-experts-layers">
<h3>Multi-gate Mixture-of-Experts Layers<a class="headerlink" href="#multi-gate-mixture-of-experts-layers" title="Link to this heading">#</a></h3>
<p>多任务建模部分使用MMoE结构。</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">双塔模型优化思路</p>
      </div>
    </a>
    <a class="right-next"
       href="%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">特征交叉</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#base-model">Base Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#din-deep-interest-network">DIN(Deep Interest Network)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#din">DIN核心思想</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-aware-regularization-mbar">mini-batch aware regularization(MBAR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dice">自适应激活函数Dice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dien">DIEN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">模型结构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-extractor-layer">Interest Extractor Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">辅助损失函数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-evolving-layer">Interest Evolving Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attentiongru-augru">将attention机制融入GRU结构中(AUGRU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dsin">DSIN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">模型结构（整体）</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding">embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">建模用户序列</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-division-layer">session division layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer">session interest extractor layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer">Session Interest Interacting Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-activating-layer">session interest activating layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer-activation-unit">session interest extractor layer 相关 activation unit</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer-activation-unit">session interest interacting layer 相关 activation unit</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp">MLP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">实验数据</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender">京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-layer">Deep Multifaceted Transformers Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-deep-neural-network">Bias Deep Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gate-mixture-of-experts-layers">Multi-gate Mixture-of-Experts Layers</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yi Longhao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>