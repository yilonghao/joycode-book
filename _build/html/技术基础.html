
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>技术基础 &#8212; JoyCode</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '技术基础';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="双塔模型优化思路" href="%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html" />
    <link rel="prev" title="推荐&amp;广告领域常见的业务指标" href="%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">JoyCode</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    欢迎访问JoyCode
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html">推荐&amp;广告领域常见的业务指标</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">技术基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html">双塔模型优化思路</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88.html">序列特征优化方案</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89.html">特征交叉</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E5%A4%A7%E5%8E%82%E5%AE%9E%E8%B7%B5.html">大厂实践</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book/issues/new?title=Issue%20on%20page%20%2F技术基础.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/技术基础.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>技术基础</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search">beam search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-search">贪心搜索(greedy search)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">集束搜索(beam search)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simhash">SimHash</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">TF-IDF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf">词频 TF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#idf">逆文档频率 IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">TF-IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">代码实现</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gru">GRU</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">梯度消失和长距离依赖问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">GRU结构简介</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reset-gate-r-t-update-gate-z-t">reset gate <span class="math notranslate nohighlight">\(r_t\)</span> 和 update gate <span class="math notranslate nohighlight">\(z_t\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">当前时刻的新信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-norm">L-2 Norm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">交叉熵损失函数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">定义</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">通俗解释</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#focal-loss">Focal Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">推导过程（可跳过该章节）</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#oneepoch">OneEpoch现象</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">温度系数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hard">Hard样本</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">温度系数与Hard样本的具体关系</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">温度系数对预估值的影响</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">温度系数对loss的影响</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#技术基础" data-toc-modified-id="技术基础-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>技术基础</a></span><ul class="toc-item"><li><span><a href="#beam-search" data-toc-modified-id="beam-search-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>beam search</a></span><ul class="toc-item"><li><span><a href="#简介" data-toc-modified-id="简介-1.1.1"><span class="toc-item-num">1.1.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href="#贪心搜索(greedy-search)" data-toc-modified-id="贪心搜索(greedy-search)-1.1.2"><span class="toc-item-num">1.1.2&nbsp;&nbsp;</span>贪心搜索(greedy search)</a></span></li><li><span><a href="#集束搜索(beam-search)" data-toc-modified-id="集束搜索(beam-search)-1.1.3"><span class="toc-item-num">1.1.3&nbsp;&nbsp;</span>集束搜索(beam search)</a></span></li></ul></li><li><span><a href="#SimHash" data-toc-modified-id="SimHash-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>SimHash</a></span></li><li><span><a href="#TF-IDF" data-toc-modified-id="TF-IDF-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>TF-IDF</a></span><ul class="toc-item"><li><span><a href="#词频-TF-计算方式" data-toc-modified-id="词频-TF-计算方式-1.3.1"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>词频 TF 计算方式</a></span></li><li><span><a href="#逆文档频率-IDF-计算方式" data-toc-modified-id="逆文档频率-IDF-计算方式-1.3.2"><span class="toc-item-num">1.3.2&nbsp;&nbsp;</span>逆文档频率 IDF 计算方式</a></span></li><li><span><a href="#TF-IDF-计算方式" data-toc-modified-id="TF-IDF-计算方式-1.3.3"><span class="toc-item-num">1.3.3&nbsp;&nbsp;</span>TF-IDF 计算方式</a></span></li><li><span><a href="#代码实现" data-toc-modified-id="代码实现-1.3.4"><span class="toc-item-num">1.3.4&nbsp;&nbsp;</span>代码实现</a></span></li></ul></li><li><span><a href="#GRU" data-toc-modified-id="GRU-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>GRU</a></span><ul class="toc-item"><li><span><a href="#梯度消失和长距离依赖问题" data-toc-modified-id="梯度消失和长距离依赖问题-1.4.1"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>梯度消失和长距离依赖问题</a></span></li><li><span><a href="#GRU结构简介" data-toc-modified-id="GRU结构简介-1.4.2"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>GRU结构简介</a></span><ul class="toc-item"><li><span><a href="#reset-gate-$r_t$-和-update-gate-$z_t$" data-toc-modified-id="reset-gate-$r_t$-和-update-gate-$z_t$-1.4.2.1"><span class="toc-item-num">1.4.2.1&nbsp;&nbsp;</span>reset gate $r_t$ 和 update gate $z_t$</a></span></li><li><span><a href="#当前时刻的新信息" data-toc-modified-id="当前时刻的新信息-1.4.2.2"><span class="toc-item-num">1.4.2.2&nbsp;&nbsp;</span>当前时刻的新信息</a></span></li></ul></li></ul></li><li><span><a href="#L-2-Norm" data-toc-modified-id="L-2-Norm-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>L-2 Norm</a></span></li><li><span><a href="#Batch-Normalization" data-toc-modified-id="Batch-Normalization-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>Batch Normalization</a></span></li><li><span><a href="#交叉熵损失函数" data-toc-modified-id="交叉熵损失函数-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>交叉熵损失函数</a></span><ul class="toc-item"><li><span><a href="#定义" data-toc-modified-id="定义-1.7.1"><span class="toc-item-num">1.7.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href="#通俗解释" data-toc-modified-id="通俗解释-1.7.2"><span class="toc-item-num">1.7.2&nbsp;&nbsp;</span>通俗解释</a></span></li></ul></li><li><span><a href="#Focal-Loss" data-toc-modified-id="Focal-Loss-1.8"><span class="toc-item-num">1.8&nbsp;&nbsp;</span>Focal Loss</a></span><ul class="toc-item"><li><span><a href="#简介" data-toc-modified-id="简介-1.8.1"><span class="toc-item-num">1.8.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href="#推导过程（可跳过该章节）" data-toc-modified-id="推导过程（可跳过该章节）-1.8.2"><span class="toc-item-num">1.8.2&nbsp;&nbsp;</span>推导过程（可跳过该章节）</a></span></li></ul></li><li><span><a href="#OneEpoch现象" data-toc-modified-id="OneEpoch现象-1.9"><span class="toc-item-num">1.9&nbsp;&nbsp;</span>OneEpoch现象</a></span></li><li><span><a href="#温度系数" data-toc-modified-id="温度系数-1.10"><span class="toc-item-num">1.10&nbsp;&nbsp;</span>温度系数</a></span><ul class="toc-item"><li><span><a href="#Hard样本" data-toc-modified-id="Hard样本-1.10.1"><span class="toc-item-num">1.10.1&nbsp;&nbsp;</span>Hard样本</a></span></li><li><span><a href="#温度系数与Hard样本的具体关系" data-toc-modified-id="温度系数与Hard样本的具体关系-1.10.2"><span class="toc-item-num">1.10.2&nbsp;&nbsp;</span>温度系数与Hard样本的具体关系</a></span></li><li><span><a href="#温度系数对预估值的影响" data-toc-modified-id="温度系数对预估值的影响-1.10.3"><span class="toc-item-num">1.10.3&nbsp;&nbsp;</span>温度系数对预估值的影响</a></span></li><li><span><a href="#温度系数对loss的影响" data-toc-modified-id="温度系数对loss的影响-1.10.4"><span class="toc-item-num">1.10.4&nbsp;&nbsp;</span>温度系数对loss的影响</a></span></li></ul></li></ul></li></ul></div><section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>技术基础<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="beam-search">
<h2>beam search<a class="headerlink" href="#beam-search" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.cnblogs.com/sddai/p/10552592.html">Beam Search（集束搜索/束搜索）</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/hecongqing/article/details/105040105">十分钟读懂Beam Search(1/2)</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html">9.8. Beam Search</a></p></li>
</ol>
</div></blockquote>
<section id="id2">
<h3>简介<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。这样减少了空间消耗，并提高了时间效率，但缺点就是有可能存在潜在的最佳方案被丢弃，因此Beam Search算法是不完全的，一般用于解空间较大的系统中。</p>
</section>
<section id="greedy-search">
<h3>贪心搜索(greedy search)<a class="headerlink" href="#greedy-search" title="Link to this heading">#</a></h3>
<p>贪心搜索最为简单，每一个时间步都取出了条件概率最大一个结果。</p>
<p><img alt="" src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/84824679-64f7-4731-9eda-3df61e02dc87.png" /></p>
</section>
<section id="id3">
<h3>集束搜索(beam search)<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>而beam search是对贪心策略一个改进。思路也很简单，就是稍微放宽一些考察的范围。在每一个时间步，不再只保留当前分数最高的1个输出，而是保留num_beams个。当num_beams=1时集束搜索就退化成了贪心搜索。</p>
<p>下图是一个实际的例子，每个时间步有A、B、C、D、E共5种可能的输出，图中的num_beams=2，也就是说每个时间步都会保留到当前步为止条件概率最优的2个序列。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/fa985ef1-40bc-4745-9e8b-fd6b4976f796.png" style="zoom: 67%;" />
<ul class="simple">
<li><p>第 1 个时间步，A和C是最优的两个序列，因此得到了两个结果<code class="docutils literal notranslate"><span class="pre">[A],[C]</span></code>，其他三个就被丢弃了；</p></li>
<li><p>第 2 个时间步，得到10个候选<code class="docutils literal notranslate"><span class="pre">[AA],[AB],[AC],[AD],[AE],[CA],[CB],[CC],[CD],[CE]</span></code>，对这10个序列进行统一排名，再保留最优的两个序列，即<code class="docutils literal notranslate"><span class="pre">[AB]</span></code>和<code class="docutils literal notranslate"><span class="pre">[CE]</span></code>；</p></li>
<li><p>第 3 个时间步，得到10个候选<code class="docutils literal notranslate"><span class="pre">[ABA],[ABB],[ABC],[ABD],[ABE],[CEA],[CEB],[CEC],[CED],[CEE]</span></code>，对这10个序列进行统一排名，再保留最优的两个序列，即<code class="docutils literal notranslate"><span class="pre">[ABD],[CED]</span></code>。</p></li>
</ul>
<p>可以发现，beam search在每一步需要考察的候选人数量是贪心搜索的num_beams倍，因此是一种牺牲时间换性能的方法。</p>
</section>
</section>
<section id="simhash">
<h2>SimHash<a class="headerlink" href="#simhash" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/444198579">5分钟搞懂LSH之SimHash算法原理</a></p></li>
</ol>
</div></blockquote>
<p>在许多场景中，都会遇到海量数据相似度计算的问题，如：电商场景中根据商品embedding计算相似度，取出相似的topk个商品。然而，这种计算相似度需要笛卡尔积的时间复杂度，在数据量较小时，时间还可以接受，但是当数据量达到几十万甚至几百几千万时，是没有办法接受的，这个时候就需要想其他办法。本文主要介绍海量item之间相似度计算问题——局部敏感哈希(Locality-Sensitive Hashing, LSH)之SimHash算法原理。</p>
<p>假设有3个商品，即：item1、item2和item3，每个商品用二维的embedding来表示，同时随机初始化6个超平面，即：s1、s2、s3、s4、s5和s6，每个超平面也是一个二维的embedding，这时可以在二维平面直角坐标系下表示，如下图：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-ace449e4d4c8a00b8d12cabbc9232ff3_1440w.webp" alt="img" style="zoom:33%;" />
<p>接下来，我们让每个item分别与6个超平面进行向量点积（相似度计算的一种方式），如果结果大于0，则结果为1，否则结果为0。因此会有如下结果表格：</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>s1</p></th>
<th class="head"><p>s2</p></th>
<th class="head"><p>s3</p></th>
<th class="head"><p>s4</p></th>
<th class="head"><p>s5</p></th>
<th class="head"><p>s6</p></th>
<th class="head"><p>SimHash</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>item1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>111000</p></td>
</tr>
<tr class="row-odd"><td><p>item2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>111000</p></td>
</tr>
<tr class="row-even"><td><p>item3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>000111</p></td>
</tr>
</tbody>
</table>
<p>通过上面的表格，item1、item2和超平面s1、s2、s3的相似度（向量点积）大于0，对应表格中的值1；与s4、s5、s6的相似度小于0，对应表格中的值0。同理，item3和超平面s1、s2、s3的相似度小于0，对应表格中的值0；与s4、s5、s6的相似度大于0，对应表格中的值1。这时，把每个超平面叫做哈希函数，SimHash值是每个item与各个超平面向量点积后的二进制结果。我们发现item1与item2的SimHash值是一样的，而与item3的SimHash值不同。这时把SimHash值相同的放在一个桶里面，如下图：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-9de151911806b2032c081b2ce88dbb9d_1440w.jpeg" alt="img" style="zoom: 67%;" />
<p>如果有几十万这样的item，SimHash算法计算后，每个桶都会有一定数量的item。这时计算item的topk个相似item时，只需要将此item与对应桶中其他item进行相似度计算，然后找到其topk个相似的item。下面是SimHash算法伪代码：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221027082521020.png" alt="image-20221027082521020" style="zoom:50%;" /></section>
<section id="tf-idf">
<h2>TF-IDF<a class="headerlink" href="#tf-idf" title="Link to this heading">#</a></h2>
<p>TF-IDF算法通常用于提取一篇文章的关键词，算法的核心思想比较简单，我们也可以加以拓展应用到推荐系统中。</p>
<section id="tf">
<h3>词频 TF 计算方式<a class="headerlink" href="#tf" title="Link to this heading">#</a></h3>
<p>词频的计算方式有多种，比较常见的为方式一，在提取文章关键词的场景下，方式一和方式二是等价的（计算每个词的TF-IDF值，方式二相对于方式一相当于都除了同一个常数，而后按照TF-IDF值倒排取TOPN个词作为文章的关键词）。</p>
<p>方式一：
$<span class="math notranslate nohighlight">\(
词频(TF) = 某个词在文章中出现的次数
\)</span>$</p>
<p>方式二：
$<span class="math notranslate nohighlight">\(
词频(TF) = \frac{某个词在文章中出现的次数}{文章的总词数}
\)</span>$</p>
</section>
<section id="idf">
<h3>逆文档频率 IDF 计算方式<a class="headerlink" href="#idf" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
逆文档频率(IDF) = log_2(\frac{语料库的文档总数}{包含该词的文档总数 + 1})
\]</div>
</section>
<section id="id4">
<h3>TF-IDF 计算方式<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[TF\text{-}IDF = 词频TF \times 逆文档频率IDF\]</div>
</section>
<section id="id5">
<h3>代码实现<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>


<span class="c1"># 示例文档</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The quick brown fox jumps over the lazy dog.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Never jump over the lazy dog quickly.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;A quick brown dog outpaces a quick fox.&quot;</span>
<span class="p">]</span>


<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;预处理文档&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">compute_tf</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;计算词频TF&quot;&quot;&quot;</span>
    <span class="n">tf_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="n">tf_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">doc_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tf_dict</span><span class="p">:</span>
        <span class="n">tf_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">doc_len</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf_dict</span>


<span class="k">def</span> <span class="nf">compute_idf</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;计算逆文档频率IDF&quot;&quot;&quot;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="n">idf_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">all_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">:</span>
        <span class="n">containing_docs</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">)</span>
        <span class="c1"># math.log 计算出来的值可能会是负数，尤其是当词频很高时（例如，词出现在所有文档中）</span>
        <span class="c1"># 添加1到最终的IDF值：避免负值并确保所有IDF值都是正的</span>
        <span class="n">idf_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">containing_docs</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">idf_dict</span>


<span class="k">def</span> <span class="nf">compute_tfidf</span><span class="p">(</span><span class="n">tf</span><span class="p">,</span> <span class="n">idf</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;计算TF-IDF值&quot;&quot;&quot;</span>
    <span class="n">tfidf</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tf_val</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tfidf</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_val</span> <span class="o">*</span> <span class="n">idf</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tfidf</span>


<span class="n">preprocessed_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
<span class="n">tf_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_tf</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">preprocessed_docs</span><span class="p">]</span>
<span class="n">idf</span> <span class="o">=</span> <span class="n">compute_idf</span><span class="p">(</span><span class="n">preprocessed_docs</span><span class="p">)</span>
<span class="n">tfidf_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_tfidf</span><span class="p">(</span><span class="n">tf</span><span class="p">,</span> <span class="n">idf</span><span class="p">)</span> <span class="k">for</span> <span class="n">tf</span> <span class="ow">in</span> <span class="n">tf_list</span><span class="p">]</span>


<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tfidf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tfidf_list</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">文档 </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">documents</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">tfidf</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;词: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">, TF-IDF值: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>文档 1: The quick brown fox jumps over the lazy dog.
词: the, TF-IDF值: 0.2222
词: jumps, TF-IDF值: 0.1562
词: quick, TF-IDF值: 0.1111
词: brown, TF-IDF值: 0.1111
词: fox, TF-IDF值: 0.1111
词: over, TF-IDF值: 0.1111
词: lazy, TF-IDF值: 0.1111
词: dog, TF-IDF值: 0.0791

文档 2: Never jump over the lazy dog quickly.
词: never, TF-IDF值: 0.2008
词: jump, TF-IDF值: 0.2008
词: quickly, TF-IDF值: 0.2008
词: over, TF-IDF值: 0.1429
词: the, TF-IDF值: 0.1429
词: lazy, TF-IDF值: 0.1429
词: dog, TF-IDF值: 0.1018

文档 3: A quick brown dog outpaces a quick fox.
词: a, TF-IDF值: 0.3514
词: quick, TF-IDF值: 0.2500
词: outpaces, TF-IDF值: 0.1757
词: brown, TF-IDF值: 0.1250
词: fox, TF-IDF值: 0.1250
词: dog, TF-IDF值: 0.0890
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="gru">
<h2>GRU<a class="headerlink" href="#gru" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>梯度消失和长距离依赖问题<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>RNN 中存在的梯度消失问题会导致难以学习到长距离依赖的问题。由于梯度消失问题的存在，越早的时刻对参数的修正起到的作用就越小，也就是说模型很难捕捉到长距离依赖关系。</p>
</section>
<section id="id7">
<h3>GRU结构简介<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>GRU 引入了 <strong>reset gate</strong> 和 <strong>update gate</strong>。其结构图如下，其中 <span class="math notranslate nohighlight">\(*\)</span> 表示按位乘。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6ee03222-480d-457c-9509-a6270d8a4223-20221007224201070.png" style="zoom: 33%;" />
<section id="reset-gate-r-t-update-gate-z-t">
<h4>reset gate <span class="math notranslate nohighlight">\(r_t\)</span> 和 update gate <span class="math notranslate nohighlight">\(z_t\)</span><a class="headerlink" href="#reset-gate-r-t-update-gate-z-t" title="Link to this heading">#</a></h4>
<p>reset gate 用来控制计算当前时刻的新信息时，保留多少之前的记忆。举个例子来说明一下，假设每个时刻输入的是一个词的话，那么如果 <span class="math notranslate nohighlight">\(r_t\)</span> 为 0，那么 <span class="math notranslate nohighlight">\(\widetilde{h}_t\)</span> 中就会只包含当前词的信息。</p>
<div class="math notranslate nohighlight">
\[
{r}_t = \sigma (W_r \cdot [h_{t-1}, x_t])
\]</div>
<p>update gate 控制需要从前一时刻的隐藏层状态 <span class="math notranslate nohighlight">\(h_{t-1}\)</span> 中忘记多少信息，同时控制需要将多少当前时刻的新信息加入到隐藏层状态中。</p>
<div class="math notranslate nohighlight">
\[
{z}_t = \sigma (W_z \cdot [h_{t-1}, x_t])
\]</div>
<p>reset gate 允许模型丢弃一些和未来无关的信息，如果reset gate接近0，那么之前的隐藏层信息就会丢弃。update gate 控制当前时刻的隐藏层输出 <span class="math notranslate nohighlight">\(h_t\)</span> 需要保留多少之前的隐藏层信息，若 <span class="math notranslate nohighlight">\(z_t\)</span> 接近于 1，相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。 一般来说那些具有短距离依赖的单元 reset gate 比较活跃，具有长距离依赖的单元 update gate 比较活跃。</p>
</section>
<section id="id8">
<h4>当前时刻的新信息<a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<p>接下来计算当前时刻的新信息（candidate values, <span class="math notranslate nohighlight">\(\widetilde{h}_t\)</span>）。这跟 LSTM 中的 candidates values(<span class="math notranslate nohighlight">\(\widetilde{C}_t\)</span>) 是类似的。计算方式如下：</p>
<div class="math notranslate nohighlight">
\[
\widetilde{h}_t = tanh(W_h \cdot [r_t \circ h_{t-1}, x_t])
\]</div>
</section>
</section>
</section>
<section id="l-2-norm">
<h2>L-2 Norm<a class="headerlink" href="#l-2-norm" title="Link to this heading">#</a></h2>
<p><strong>L-2 Norm的作用是把embedding的长度都归一化为1，也就是说把它们都映射到一个长度为1的单位超球面上去。如果把它投影到单位超平面上，会增加训练稳定性和投影空间的线性可分性</strong>。增加线性可分性，意思也就是说你用简单算法也能得到比较好的效果。试想一下，一个单位超球面和一个不规则球面的向量空间，是不是前者更容易做到线性可分呢？这是目前图像领域里面得出的结论。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240313085555643.png" alt="image-20240313085555643" style="zoom: 67%;" /><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">vector</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">vector</span>
    <span class="k">return</span> <span class="n">vector</span> <span class="o">/</span> <span class="n">norm</span>


<span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">normalized_vector</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;向量 </span><span class="si">{</span><span class="n">vector</span><span class="si">}</span><span class="s2"> 的归一化结果是: </span><span class="si">{</span><span class="n">normalized_vector</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>向量 [3 4] 的归一化结果是: [0.6 0.8]
</pre></div>
</div>
</div>
</div>
</section>
<section id="batch-normalization">
<h2>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Link to this heading">#</a></h2>
<p>BN假设所有样本独立同分布，并且使用所有样本的共享统计量进行normalization，<span class="math notranslate nohighlight">\(E\)</span>和<span class="math notranslate nohighlight">\(Var\)</span>分别是滑动均值和方差，<span class="math notranslate nohighlight">\(\gamma\)</span>和<span class="math notranslate nohighlight">\(\beta\)</span>是可以学习的scale和bias。多场景建模时，一般为每个场景添加独立的BN层，确保每个场景的数据都能被正确归一化。
$<span class="math notranslate nohighlight">\(
z' = \gamma \frac{z-E}{\sqrt{Var + \epsilon}} + \beta
\)</span>$</p>
<p>Batch Normalization（批量归一化）的主要作用可以概括为以下几点：</p>
<ol class="arabic simple">
<li><p>稳定训练过程：训练神经网络时，数据在每一层都会经过一系列的变化，导致分布可能会变得不稳定。Batch Normalization通过将数据重新调整为标准正态分布，使训练过程更加平稳，减少训练的不确定性。</p></li>
<li><p>加速收敛：通过归一化处理，模型在训练过程中可以更快地达到最优状态。因为数据分布被稳定下来，模型可以用更高的学习率，从而减少训练的时间。</p></li>
<li><p>防止过拟合：Batch Normalization有助于防止模型过拟合。这种归一化方法有类似正则化的效果，减少对其他正则化方法（如Dropout）的依赖。</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">BatchNormalization</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>  <span class="c1"># 缩放参数γ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>  <span class="c1"># 平移参数β</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;前向传播&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1"># 计算当前批次的均值和方差</span>
            <span class="n">batch_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">batch_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># 归一化</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">batch_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">batch_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="c1"># 平滑的更新运行中的均值和方差</span>
            <span class="c1"># 平衡了当前批次和历史批次的统计量，防止单个批次对整体统计量的剧烈影响</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_mean</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_var</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 使用运行中的均值和方差进行归一化</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="c1"># 应用缩放和偏移</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_norm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">set_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">training</span>

<span class="c1"># 示例使用</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">bn</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># 训练模式下的前向传播</span>
<span class="n">bn</span><span class="o">.</span><span class="n">set_training</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output_train</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;训练模式下的归一化结果:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">output_train</span><span class="p">)</span>

<span class="c1"># 测试模式下的前向传播</span>
<span class="n">bn</span><span class="o">.</span><span class="n">set_training</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">output_test</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;测试模式下的归一化结果:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">output_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1.76405235  0.40015721  0.97873798]
 [ 2.2408932   1.86755799 -0.97727788]
 [ 0.95008842 -0.15135721 -0.10321885]
 [ 0.4105985   0.14404357  1.45427351]
 [ 0.76103773  0.12167502  0.44386323]]
训练模式下的归一化结果:
 [[ 0.79834163 -0.10633427  0.73103552]
 [ 1.50498534  1.93980876 -1.57728392]
 [-0.40789413 -0.87536574 -0.54579564]
 [-1.20737923 -0.46345902  1.29222109]
 [-0.68805361 -0.49464972  0.09982295]]
测试模式下的归一化结果:
 [[ 1.6881266   0.36139964  0.95638524]
 [ 2.1785064   1.8657813  -1.02779382]
 [ 0.85105179 -0.20401383 -0.14114988]
 [ 0.29624413  0.09883153  1.43876764]
 [ 0.65663338  0.07589925  0.41380923]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="id9">
<h2>交叉熵损失函数<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<section id="id10">
<h3>定义<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>交叉熵损失函数（Cross-Entropy Loss），在机器学习中特别是分类任务中非常常用。它衡量的是模型预测的概率分布与真实概率分布之间的差异。换句话说，交叉熵损失越小，表示模型的预测结果越接近真实标签。</p>
<p>对于一个分类任务，交叉熵损失函数的定义如下：</p>
<div class="math notranslate nohighlight">
\[
\text{CE}(y, \hat{y}) = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
\]</div>
<p>其中：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> 是真实标签的独热编码（one-hot encoding）。</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y} \)</span> 是模型预测的概率分布。</p></li>
<li><p><span class="math notranslate nohighlight">\( C \)</span> 是类别的总数。</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> 是真实标签中第 <span class="math notranslate nohighlight">\(i\)</span> 类的值（对于独热编码，只有一个位置是1，其余为0）。</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> 是模型预测的第 <span class="math notranslate nohighlight">\(i\)</span> 类的概率。</p></li>
</ul>
</section>
<section id="id11">
<h3>通俗解释<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>可以用一个简单的例子来解释交叉熵损失函数。假设我们在做一个猫和狗的二分类任务：</p>
<ul class="simple">
<li><p>如果图片是猫，真实标签 <span class="math notranslate nohighlight">\( y \)</span> 可以表示为 [1, 0]。</p></li>
<li><p>如果图片是狗，真实标签 <span class="math notranslate nohighlight">\( y \)</span> 可以表示为 [0, 1]。</p></li>
</ul>
<p>模型输出的是对这两个类别的预测概率，例如：</p>
<ul class="simple">
<li><p>模型预测图片是猫的概率为 0.9，是狗的概率为 0.1，预测结果为 [0.9, 0.1]。</p></li>
</ul>
<p>交叉熵损失函数计算真实标签和预测概率之间的差异，对于标签 [1, 0] 和预测 [0.9, 0.1]，交叉熵损失为：
$<span class="math notranslate nohighlight">\(
\text{CE} = - (1 \cdot \log(0.9) + 0 \cdot \log(0.1)) = - \log(0.9)
\)</span>$</p>
<p>损失值越小，表示预测结果越接近真实标签。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    计算交叉熵损失</span>

<span class="sd">    参数:</span>
<span class="sd">    y_true -- 真实标签，形状为 (batch_size, num_classes)</span>
<span class="sd">    y_pred -- 预测概率，形状为 (batch_size, num_classes)</span>

<span class="sd">    返回:</span>
<span class="sd">    loss -- 交叉熵损失值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-9</span>  <span class="c1"># 避免log(0)的情况</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>


<span class="c1"># 示例用法</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># 真实标签 (one-hot encoded)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>  <span class="c1"># 预测概率</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;交叉熵损失:&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>交叉熵损失: 0.7418746839526391
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="focal-loss">
<h2>Focal Loss<a class="headerlink" href="#focal-loss" title="Link to this heading">#</a></h2>
<section id="id12">
<h3>简介<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>Focal Loss 是一种改进的交叉熵损失函数，旨在解决样本类别不均衡的问题。它通过降低对易分类样本的损失权重，增强对难分类样本的关注，从而改善模型在不平衡数据集上的表现。Focal Loss 的公式如下：</p>
<div class="math notranslate nohighlight">
\[
Loss_{fl} =-\alpha_{t}(1- p_{t})^\gamma log(p_{t})
\]</div>
<p>其中公式中各个部分的含义如下：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p_t\)</span>是模型对正确类别的预测概率。</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_t\)</span>是平衡因子，用于平衡正负样本的影响（可选）。</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span>是调整因子，用于控制易分类样本的权重降低程度。</p></li>
</ul>
<p>Focal Loss是如何做到降低对易分类样本的损失权重，增强对难分类样本的关注的？</p>
<ul class="simple">
<li><p>对于易分类样本，<span class="math notranslate nohighlight">\(p_{t}\)</span>接近 1（模型对其分类的信心高），那么<span class="math notranslate nohighlight">\(1-p_{t}\)</span>接近 0，因此 <span class="math notranslate nohighlight">\((1- p_{t})^\gamma\)</span> 也接近 0，这个调节项会显著降低易分类样本的损失值。</p></li>
<li><p>对于难分类样本，<span class="math notranslate nohighlight">\(p_{t}\)</span>接近 0（模型对其分类的信心低），那么<span class="math notranslate nohighlight">\(1-p_{t}\)</span>接近 1，因此 <span class="math notranslate nohighlight">\((1- p_{t})^\gamma\)</span> 仍接近 1，这个调节项对难分类样本的影响很小。</p></li>
</ul>
</section>
<section id="id13">
<h3>推导过程（可跳过该章节）<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>交叉熵损失函数（其中 <span class="math notranslate nohighlight">\(\hat p\)</span>为预测概率大小）：
$<span class="math notranslate nohighlight">\(
Loss = L(y, \hat p) =-ylog(\hat p) - (1-y)log(1-\hat p)
\)</span>$</p>
<p>对于二分类问题，先简化公式：
$<span class="math notranslate nohighlight">\(
Loss = L(y, \hat p) =-log(\hat p) - log(1-\hat p)
\)</span>$</p>
<p>对于所有样本来说，假设N为总样本量，m为正样本量，n为负样本量，当m &lt;&lt; n时，负样本就会在损失函数里占据主导地位，由于损失函数的倾斜，模型训练过程中会倾向于样本多的类别，造成模型对少样本类别的性能较差。
$<span class="math notranslate nohighlight">\(
Loss =\frac{1}{N}(\sum_{y_i=1}^{m}-log(\hat p) + \sum_{y_i=o}^{n}-log(1-\hat p))
\)</span>$</p>
<p>focal loss具体形式：
$<span class="math notranslate nohighlight">\(
Loss =\frac{1}{N}(\sum_{y_i=1}^{m}-(1-\hat p)^\gamma log(\hat p) + \sum_{y_i=o}^{n}-\hat p^{\gamma} log(1-\hat p))
\)</span>$</p>
<p>如果我做以下定义：
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240312223812014.png" alt="image-20240312223812014" style="zoom:50%;" /></p>
<p><strong>focal loss表达式：</strong>
$<span class="math notranslate nohighlight">\(
Loss_{fl} =-(1- p_t)^\gamma log(p_t)
\)</span>$</p>
<p><strong>交叉熵表达式：</strong>
$<span class="math notranslate nohighlight">\(
Loss_{ce} =-log(p_t)
\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">focal_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    计算Focal Loss</span>

<span class="sd">    参数:</span>
<span class="sd">    y_true -- 真实标签，形状为 (batch_size, num_classes)</span>
<span class="sd">    y_pred -- 预测概率，形状为 (batch_size, num_classes)</span>
<span class="sd">    alpha -- 平衡因子，默认值为 0.25</span>
<span class="sd">    gamma -- 调整因子，默认值为 2.0</span>

<span class="sd">    返回:</span>
<span class="sd">    loss -- Focal Loss 值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-9</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># 避免log(0)情况</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">cross_entropy</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>


<span class="c1"># 示例用法</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># 真实标签</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>  <span class="c1"># 预测概率</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">focal_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Focal Loss:&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Focal Loss: 0.09273549740974675
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="oneepoch">
<h2>OneEpoch现象<a class="headerlink" href="#oneepoch" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p>paper: <strong>Multi-Epoch Learning</strong> for Deep Click-Through Rate Prediction Models</p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/669063912">阿里OneEpoch VS 快手MultiEpoch</a></p></li>
</ol>
</div></blockquote>
<p><strong>模型AUC在第一个epoch内逐步提升，但是从第二个epoch开始，AUC效果突然剧烈下降</strong>。产生OneEpoch现象的原因：</p>
<ul class="simple">
<li><p>embedding  + mlp的结构</p></li>
<li><p>能使模型快速收敛的优化器算法（eg: 学习率较大的adam优化器）</p></li>
<li><p>高维稀疏特征（eg: item_id等细粒度特征）</p></li>
<li><p>其他不相关因素：模型参数量、激活函数、batch size、weight decay、dropout</p></li>
</ul>
<p>多Epoch探究：<strong>每一轮训练都重置embedding，更新embedding和mlp</strong>，避免embedding层过拟合，并让mlp层学的更充分。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-4a751d00853416d18182ef4d97f2f5b4_1440w-0067336.webp" alt="img" style="zoom:50%;" /></section>
<section id="id14">
<h2>温度系数<a class="headerlink" href="#id14" title="Link to this heading">#</a></h2>
<p>温度系数常应用在召回/粗排等双塔模型结构中，点乘之后除以一个固定的系数（温度系数），τ是温度系数，一般来说加一个温度系数是有效的。原因是温度系数可以让模型更聚焦于hard负例，且τ越小越聚焦，也就不用花费大力气挖掘hard负例。</p>
<section id="hard">
<h3>Hard样本<a class="headerlink" href="#hard" title="Link to this heading">#</a></h3>
<p>Hard样本是指那些模型难以正确分类的样本。这些样本可能由于以下原因难以分类：</p>
<ul class="simple">
<li><p>数据不平衡</p></li>
<li><p>特征不明显</p></li>
<li><p>噪声或异常数据</p></li>
</ul>
</section>
<section id="id15">
<h3>温度系数与Hard样本的具体关系<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>低温度系数（&lt;1）</strong>:</p>
<ul class="simple">
<li><p><strong>效果</strong>: 使得模型的输出概率分布更尖锐（更接近于0或1）。</p></li>
<li><p><strong>对hard样本的影响</strong>:</p>
<ul>
<li><p>模型对hard样本的预测可能会更不确定，因为这些样本本身难以分类，输出概率会更加极端（高概率的类别与低概率的类别差距更大）。</p></li>
<li><p>模型可能会更加确信自己的错误预测，从而难以在后续训练中纠正。</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>高温度系数（&gt;1）</strong>:</p>
<ul class="simple">
<li><p><strong>效果</strong>: 使得模型的输出概率分布更平滑（更接近于均匀分布）。</p></li>
<li><p><strong>对hard样本的影响</strong>:</p>
<ul>
<li><p>模型对hard样本的预测会变得更不确定，输出的概率更接近于均匀分布（各类别的概率差距缩小）。</p></li>
<li><p>这种不确定性可以提示模型在训练过程中对这些hard样本进行更多关注，从而帮助模型更好地学习和纠正错误。</p></li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section id="id16">
<h3>温度系数对预估值的影响<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<p>假设一个分类模型在一个三类问题中输出如下分数（未归一化）：</p>
<ul class="simple">
<li><p>易分类样本：[10, 2, 1]</p></li>
<li><p>难分类样本：[3, 3, 2.5]</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>无温度调整（温度=1）</strong>:</p>
<ul class="simple">
<li><p>易分类样本的Softmax概率：[0.999, 0.001, 0.000]</p></li>
<li><p>难分类样本的Softmax概率：[0.4, 0.4, 0.2]</p></li>
</ul>
</li>
<li><p><strong>降低温度（温度=0.5）</strong>:</p>
<ul class="simple">
<li><p>易分类样本的Softmax概率：[1.0, 0.0, 0.0]</p></li>
<li><p>难分类样本的Softmax概率：[0.42, 0.42, 0.16]（更尖锐）</p></li>
</ul>
</li>
<li><p><strong>升高温度（温度=2）</strong>:</p>
<ul class="simple">
<li><p>易分类样本的Softmax概率：[0.88, 0.06, 0.06]</p></li>
<li><p>难分类样本的Softmax概率：[0.34, 0.34, 0.32]（更平滑）</p></li>
</ul>
</li>
</ol>
</section>
<section id="loss">
<h3>温度系数对loss的影响<a class="headerlink" href="#loss" title="Link to this heading">#</a></h3>
<p>在机器学习和深度学习中，损失函数（loss）是用于衡量模型预测与真实标签之间差异的指标。对于分类任务，常用的损失函数是交叉熵损失。温度系数的调整会影响模型的预测概率分布，从而对损失值产生影响。</p>
<p>假设模型的输出分数为：[3.0, 1.0, 0.1]，且真实标签对应的类别为第一个类别（索引0）。</p>
<ol class="arabic simple">
<li><p><strong>无温度调整（温度=1）</strong>:</p>
<ul class="simple">
<li><p>使用Softmax函数计算概率：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( P_0 = \frac{e^{3.0}}{e^{3.0} + e^{1.0} + e^{0.1}} \approx 0.84 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_1 = \frac{e^{1.0}}{e^{3.0} + e^{1.0} + e^{0.1}} \approx 0.11 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_2 = \frac{e^{0.1}}{e^{3.0} + e^{1.0} + e^{0.1}} \approx 0.05 \)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>降低温度（温度=0.5）</strong>:</p>
<ul class="simple">
<li><p>调整后的Softmax计算：</p>
<ul>
<li><p>调整后的分数：[6.0, 2.0, 0.2]（分数被拉开）</p></li>
<li><p><span class="math notranslate nohighlight">\( P_0 = \frac{e^{6.0}}{e^{6.0} + e^{2.0} + e^{0.2}} \approx 0.97 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_1 = \frac{e^{2.0}}{e^{6.0} + e^{2.0} + e^{0.2}} \approx 0.03 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_2 = \frac{e^{0.2}}{e^{6.0} + e^{2.0} + e^{0.2}} \approx 0.0007 \)</span></p></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>交叉熵损失的公式为：
$<span class="math notranslate nohighlight">\( L = - \sum_{i} y_i \log(p_i) \)</span><span class="math notranslate nohighlight">\(
其中 \)</span>y_i<span class="math notranslate nohighlight">\( 是真实标签的one-hot编码，\)</span>p_i$ 是模型预测的概率。</p>
<ol class="arabic simple">
<li><p><strong>无温度调整时的损失</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y = [1, 0, 0] \)</span></p></li>
<li><p>损失：<span class="math notranslate nohighlight">\( L = - \log(0.84) \approx 0.17 \)</span></p></li>
</ul>
</li>
<li><p><strong>降低温度后的损失</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y = [1, 0, 0] \)</span></p></li>
<li><p>损失：<span class="math notranslate nohighlight">\( L = - \log(0.97) \approx 0.03 \)</span></p></li>
</ul>
</li>
</ol>
<p>当温度系数降低时，模型的输出概率分布变得更尖锐，预估值被拉开。如果模型预测正确，类别的概率接近1，交叉熵损失将会变小。相反，如果模型预测错误，高温度系数导致的平滑概率分布可能会导致较高的损失。因此，预估值被拉开不一定会导致损失变大。具体影响取决于模型预测的正确性。对于正确的预测，预估值被拉开会降低损失；对于错误的预测，预估值被拉开会增加损失。</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">推荐&amp;广告领域常见的业务指标</p>
      </div>
    </a>
    <a class="right-next"
       href="%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">双塔模型优化思路</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search">beam search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-search">贪心搜索(greedy search)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">集束搜索(beam search)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simhash">SimHash</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">TF-IDF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf">词频 TF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#idf">逆文档频率 IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">TF-IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">代码实现</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gru">GRU</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">梯度消失和长距离依赖问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">GRU结构简介</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reset-gate-r-t-update-gate-z-t">reset gate <span class="math notranslate nohighlight">\(r_t\)</span> 和 update gate <span class="math notranslate nohighlight">\(z_t\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">当前时刻的新信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-norm">L-2 Norm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">交叉熵损失函数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">定义</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">通俗解释</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#focal-loss">Focal Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">推导过程（可跳过该章节）</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#oneepoch">OneEpoch现象</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">温度系数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hard">Hard样本</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">温度系数与Hard样本的具体关系</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">温度系数对预估值的影响</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">温度系数对loss的影响</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yi Longhao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>