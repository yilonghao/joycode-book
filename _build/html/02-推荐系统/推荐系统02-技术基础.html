
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>推荐系统02-技术基础 &#8212; JoyCode</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '02-推荐系统/推荐系统02-技术基础';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="推荐系统03-双塔模型优化思路" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F03-%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html" />
    <link rel="prev" title="推荐系统01-推荐&amp;广告领域常见的业务指标" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F01-%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">JoyCode</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    欢迎访问JoyCode
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A001-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习01-逻辑回归算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A002-K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习02-K近邻算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A003-K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习03-K-Means聚类算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A004-K-Means%2B%2B%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习04-K-Means++聚类算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A005-%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95.html">机器学习&amp;深度学习05-分类模型评估方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F01-%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html">推荐系统01-推荐&amp;广告领域常见的业务指标</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">推荐系统02-技术基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F03-%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html">推荐系统03-双塔模型优化思路</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F04-%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%A1%88.html">推荐系统04-序列特征优化方案</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F05-%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%A4%9A%E5%9C%BA%E6%99%AF%E5%BB%BA%E6%A8%A1.html">推荐系统05-多目标与多场景建模</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F06-%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89.html">推荐系统06-特征交叉</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F08-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">推荐系统08-图神经网络</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book/issues/new?title=Issue%20on%20page%20%2F02-推荐系统/推荐系统02-技术基础.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/02-推荐系统/推荐系统02-技术基础.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>推荐系统02-技术基础</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search">beam search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-search">贪心搜索(greedy search)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">集束搜索(beam search)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simhash">SimHash</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">TF-IDF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf">词频 TF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#idf">逆文档频率 IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">TF-IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">代码实现</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-long-short-term-memories">LSTM(Long Short Term Memories)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">对 RNN 简单结构的解释</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm">LSTM结构拆分</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cell-state">cell state</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">LSTM 中的门</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#forget-gate-f-t">forget gate <span class="math notranslate nohighlight">\(f_t\)</span></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#input-gate-i-t">input gate <span class="math notranslate nohighlight">\(i_t\)</span></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">对以上两个门的总结</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#output-gate-o-t">output gate <span class="math notranslate nohighlight">\(o_t\)</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gru">GRU</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">梯度消失和长距离依赖问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">GRU结构简介</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reset-gate-r-t-update-gate-z-t">reset gate <span class="math notranslate nohighlight">\(r_t\)</span> 和 update gate <span class="math notranslate nohighlight">\(z_t\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">当前时刻的新信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">深度残差网络（ResNet）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">深度网络出现的退化问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">只做恒等映射也不该出现退化问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">ResNet</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#plaint-net">Plaint net</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-net">Residual net</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-block">Residual Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">34层的ResNet结构图</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">两种残差学习单元</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">不同深度的ResNet</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnnpooling">CNN模型中常见的Pooling操作</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#max-pooling-over-time">Max Pooling Over Time</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-max-pooling">K-Max Pooling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chunk-max-pooling">Chunk-Max Pooling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-norm">L-2 Norm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">交叉熵损失函数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">定义</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">通俗解释</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#focal-loss">Focal Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">推导过程（可跳过该章节）</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#oneepoch">OneEpoch现象</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">温度系数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hard">Hard样本</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">温度系数与Hard样本的具体关系</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">温度系数对预估值的影响</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">温度系数对loss的影响</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">softmax函数</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">蒸馏学习</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#技术基础" data-toc-modified-id="技术基础-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>技术基础</a></span><ul class="toc-item"><li><span><a href="#beam-search" data-toc-modified-id="beam-search-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>beam search</a></span><ul class="toc-item"><li><span><a href="#简介" data-toc-modified-id="简介-1.1.1"><span class="toc-item-num">1.1.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href="#贪心搜索(greedy-search)" data-toc-modified-id="贪心搜索(greedy-search)-1.1.2"><span class="toc-item-num">1.1.2&nbsp;&nbsp;</span>贪心搜索(greedy search)</a></span></li><li><span><a href="#集束搜索(beam-search)" data-toc-modified-id="集束搜索(beam-search)-1.1.3"><span class="toc-item-num">1.1.3&nbsp;&nbsp;</span>集束搜索(beam search)</a></span></li></ul></li><li><span><a href="#SimHash" data-toc-modified-id="SimHash-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>SimHash</a></span></li><li><span><a href="#TF-IDF" data-toc-modified-id="TF-IDF-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>TF-IDF</a></span><ul class="toc-item"><li><span><a href="#词频-TF-计算方式" data-toc-modified-id="词频-TF-计算方式-1.3.1"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>词频 TF 计算方式</a></span></li><li><span><a href="#逆文档频率-IDF-计算方式" data-toc-modified-id="逆文档频率-IDF-计算方式-1.3.2"><span class="toc-item-num">1.3.2&nbsp;&nbsp;</span>逆文档频率 IDF 计算方式</a></span></li><li><span><a href="#TF-IDF-计算方式" data-toc-modified-id="TF-IDF-计算方式-1.3.3"><span class="toc-item-num">1.3.3&nbsp;&nbsp;</span>TF-IDF 计算方式</a></span></li><li><span><a href="#代码实现" data-toc-modified-id="代码实现-1.3.4"><span class="toc-item-num">1.3.4&nbsp;&nbsp;</span>代码实现</a></span></li></ul></li><li><span><a href="#LSTM(Long-Short-Term-Memories)" data-toc-modified-id="LSTM(Long-Short-Term-Memories)-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>LSTM(Long Short Term Memories)</a></span><ul class="toc-item"><li><span><a href="#对-RNN-简单结构的解释" data-toc-modified-id="对-RNN-简单结构的解释-1.4.1"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>对 RNN 简单结构的解释</a></span></li><li><span><a href="#LSTM结构拆分" data-toc-modified-id="LSTM结构拆分-1.4.2"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>LSTM结构拆分</a></span><ul class="toc-item"><li><span><a href="#cell-state" data-toc-modified-id="cell-state-1.4.2.1"><span class="toc-item-num">1.4.2.1&nbsp;&nbsp;</span>cell state</a></span></li><li><span><a href="#LSTM-中的门" data-toc-modified-id="LSTM-中的门-1.4.2.2"><span class="toc-item-num">1.4.2.2&nbsp;&nbsp;</span>LSTM 中的门</a></span></li></ul></li></ul></li><li><span><a href="#GRU" data-toc-modified-id="GRU-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>GRU</a></span><ul class="toc-item"><li><span><a href="#梯度消失和长距离依赖问题" data-toc-modified-id="梯度消失和长距离依赖问题-1.5.1"><span class="toc-item-num">1.5.1&nbsp;&nbsp;</span>梯度消失和长距离依赖问题</a></span></li><li><span><a href="#GRU结构简介" data-toc-modified-id="GRU结构简介-1.5.2"><span class="toc-item-num">1.5.2&nbsp;&nbsp;</span>GRU结构简介</a></span><ul class="toc-item"><li><span><a href="#reset-gate-$r_t$-和-update-gate-$z_t$" data-toc-modified-id="reset-gate-$r_t$-和-update-gate-$z_t$-1.5.2.1"><span class="toc-item-num">1.5.2.1&nbsp;&nbsp;</span>reset gate $r_t$ 和 update gate $z_t$</a></span></li><li><span><a href="#当前时刻的新信息" data-toc-modified-id="当前时刻的新信息-1.5.2.2"><span class="toc-item-num">1.5.2.2&nbsp;&nbsp;</span>当前时刻的新信息</a></span></li></ul></li></ul></li><li><span><a href="#深度残差网络（ResNet）" data-toc-modified-id="深度残差网络（ResNet）-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>深度残差网络（ResNet）</a></span><ul class="toc-item"><li><span><a href="#深度网络出现的退化问题" data-toc-modified-id="深度网络出现的退化问题-1.6.1"><span class="toc-item-num">1.6.1&nbsp;&nbsp;</span>深度网络出现的退化问题</a></span></li><li><span><a href="#只做恒等映射也不该出现退化问题" data-toc-modified-id="只做恒等映射也不该出现退化问题-1.6.2"><span class="toc-item-num">1.6.2&nbsp;&nbsp;</span>只做恒等映射也不该出现退化问题</a></span></li><li><span><a href="#ResNet" data-toc-modified-id="ResNet-1.6.3"><span class="toc-item-num">1.6.3&nbsp;&nbsp;</span>ResNet</a></span><ul class="toc-item"><li><span><a href="#Plaint-net" data-toc-modified-id="Plaint-net-1.6.3.1"><span class="toc-item-num">1.6.3.1&nbsp;&nbsp;</span>Plaint net</a></span></li><li><span><a href="#Residual-net" data-toc-modified-id="Residual-net-1.6.3.2"><span class="toc-item-num">1.6.3.2&nbsp;&nbsp;</span>Residual net</a></span></li><li><span><a href="#Residual-Block" data-toc-modified-id="Residual-Block-1.6.3.3"><span class="toc-item-num">1.6.3.3&nbsp;&nbsp;</span>Residual Block</a></span></li><li><span><a href="#34层的ResNet结构图" data-toc-modified-id="34层的ResNet结构图-1.6.3.4"><span class="toc-item-num">1.6.3.4&nbsp;&nbsp;</span>34层的ResNet结构图</a></span></li><li><span><a href="#两种残差学习单元" data-toc-modified-id="两种残差学习单元-1.6.3.5"><span class="toc-item-num">1.6.3.5&nbsp;&nbsp;</span>两种残差学习单元</a></span></li><li><span><a href="#不同深度的ResNet" data-toc-modified-id="不同深度的ResNet-1.6.3.6"><span class="toc-item-num">1.6.3.6&nbsp;&nbsp;</span>不同深度的ResNet</a></span></li></ul></li></ul></li><li><span><a href="#CNN模型中常见的Pooling操作" data-toc-modified-id="CNN模型中常见的Pooling操作-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>CNN模型中常见的Pooling操作</a></span><ul class="toc-item"><li><span><a href="#Max-Pooling-Over-Time" data-toc-modified-id="Max-Pooling-Over-Time-1.7.1"><span class="toc-item-num">1.7.1&nbsp;&nbsp;</span>Max Pooling Over Time</a></span></li><li><span><a href="#K-Max-Pooling" data-toc-modified-id="K-Max-Pooling-1.7.2"><span class="toc-item-num">1.7.2&nbsp;&nbsp;</span>K-Max Pooling</a></span></li><li><span><a href="#Chunk-Max-Pooling" data-toc-modified-id="Chunk-Max-Pooling-1.7.3"><span class="toc-item-num">1.7.3&nbsp;&nbsp;</span>Chunk-Max Pooling</a></span></li></ul></li><li><span><a href="#L-2-Norm" data-toc-modified-id="L-2-Norm-1.8"><span class="toc-item-num">1.8&nbsp;&nbsp;</span>L-2 Norm</a></span></li><li><span><a href="#Batch-Normalization" data-toc-modified-id="Batch-Normalization-1.9"><span class="toc-item-num">1.9&nbsp;&nbsp;</span>Batch Normalization</a></span></li><li><span><a href="#交叉熵损失函数" data-toc-modified-id="交叉熵损失函数-1.10"><span class="toc-item-num">1.10&nbsp;&nbsp;</span>交叉熵损失函数</a></span><ul class="toc-item"><li><span><a href="#定义" data-toc-modified-id="定义-1.10.1"><span class="toc-item-num">1.10.1&nbsp;&nbsp;</span>定义</a></span></li><li><span><a href="#通俗解释" data-toc-modified-id="通俗解释-1.10.2"><span class="toc-item-num">1.10.2&nbsp;&nbsp;</span>通俗解释</a></span></li></ul></li><li><span><a href="#Focal-Loss" data-toc-modified-id="Focal-Loss-1.11"><span class="toc-item-num">1.11&nbsp;&nbsp;</span>Focal Loss</a></span><ul class="toc-item"><li><span><a href="#简介" data-toc-modified-id="简介-1.11.1"><span class="toc-item-num">1.11.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href="#推导过程（可跳过该章节）" data-toc-modified-id="推导过程（可跳过该章节）-1.11.2"><span class="toc-item-num">1.11.2&nbsp;&nbsp;</span>推导过程（可跳过该章节）</a></span></li></ul></li><li><span><a href="#OneEpoch现象" data-toc-modified-id="OneEpoch现象-1.12"><span class="toc-item-num">1.12&nbsp;&nbsp;</span>OneEpoch现象</a></span></li><li><span><a href="#温度系数" data-toc-modified-id="温度系数-1.13"><span class="toc-item-num">1.13&nbsp;&nbsp;</span>温度系数</a></span><ul class="toc-item"><li><span><a href="#Hard样本" data-toc-modified-id="Hard样本-1.13.1"><span class="toc-item-num">1.13.1&nbsp;&nbsp;</span>Hard样本</a></span></li><li><span><a href="#温度系数与Hard样本的具体关系" data-toc-modified-id="温度系数与Hard样本的具体关系-1.13.2"><span class="toc-item-num">1.13.2&nbsp;&nbsp;</span>温度系数与Hard样本的具体关系</a></span></li><li><span><a href="#温度系数对预估值的影响" data-toc-modified-id="温度系数对预估值的影响-1.13.3"><span class="toc-item-num">1.13.3&nbsp;&nbsp;</span>温度系数对预估值的影响</a></span></li><li><span><a href="#温度系数对loss的影响" data-toc-modified-id="温度系数对loss的影响-1.13.4"><span class="toc-item-num">1.13.4&nbsp;&nbsp;</span>温度系数对loss的影响</a></span></li></ul></li><li><span><a href="#softmax函数" data-toc-modified-id="softmax函数-1.14"><span class="toc-item-num">1.14&nbsp;&nbsp;</span>softmax函数</a></span></li><li><span><a href="#蒸馏学习" data-toc-modified-id="蒸馏学习-1.15"><span class="toc-item-num">1.15&nbsp;&nbsp;</span>蒸馏学习</a></span></li></ul></li></ul></div><section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>推荐系统02-技术基础<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="beam-search">
<h2>beam search<a class="headerlink" href="#beam-search" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://www.cnblogs.com/sddai/p/10552592.html">Beam Search（集束搜索/束搜索）</a></p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/hecongqing/article/details/105040105">十分钟读懂Beam Search(1/2)</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html">9.8. Beam Search</a></p></li>
</ol>
</div></blockquote>
<section id="id2">
<h3>简介<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Beam Search（集束搜索）是一种启发式图搜索算法，通常用在图的解空间比较大的情况下，为了减少搜索所占用的空间和时间，在每一步深度扩展的时候，剪掉一些质量比较差的结点，保留下一些质量较高的结点。这样减少了空间消耗，并提高了时间效率，但缺点就是有可能存在潜在的最佳方案被丢弃，因此Beam Search算法是不完全的，一般用于解空间较大的系统中。</p>
</section>
<section id="greedy-search">
<h3>贪心搜索(greedy search)<a class="headerlink" href="#greedy-search" title="Link to this heading">#</a></h3>
<p>贪心搜索最为简单，每一个时间步都取出了条件概率最大一个结果。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/84824679-64f7-4731-9eda-3df61e02dc87.png" style="zoom: 67%; display: block; margin: auto;" />
</section>
<section id="id3">
<h3>集束搜索(beam search)<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>而beam search是对贪心策略一个改进。思路也很简单，就是稍微放宽一些考察的范围。在每一个时间步，不再只保留当前分数最高的1个输出，而是保留num_beams个。当num_beams=1时集束搜索就退化成了贪心搜索。</p>
<p>下图是一个实际的例子，每个时间步有A、B、C、D、E共5种可能的输出，图中的num_beams=2，也就是说每个时间步都会保留到当前步为止条件概率最优的2个序列。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/fa985ef1-40bc-4745-9e8b-fd6b4976f796.png" style="zoom: 67%; display: block; margin: auto;" />
<ul class="simple">
<li><p>第 1 个时间步，A和C是最优的两个序列，因此得到了两个结果<code class="docutils literal notranslate"><span class="pre">[A],[C]</span></code>，其他三个就被丢弃了；</p></li>
<li><p>第 2 个时间步，得到10个候选<code class="docutils literal notranslate"><span class="pre">[AA],[AB],[AC],[AD],[AE],[CA],[CB],[CC],[CD],[CE]</span></code>，对这10个序列进行统一排名，再保留最优的两个序列，即<code class="docutils literal notranslate"><span class="pre">[AB]</span></code>和<code class="docutils literal notranslate"><span class="pre">[CE]</span></code>；</p></li>
<li><p>第 3 个时间步，得到10个候选<code class="docutils literal notranslate"><span class="pre">[ABA],[ABB],[ABC],[ABD],[ABE],[CEA],[CEB],[CEC],[CED],[CEE]</span></code>，对这10个序列进行统一排名，再保留最优的两个序列，即<code class="docutils literal notranslate"><span class="pre">[ABD],[CED]</span></code>。</p></li>
</ul>
<p>可以发现，beam search在每一步需要考察的候选人数量是贪心搜索的num_beams倍，因此是一种牺牲时间换性能的方法。</p>
</section>
</section>
<section id="simhash">
<h2>SimHash<a class="headerlink" href="#simhash" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/444198579">5分钟搞懂LSH之SimHash算法原理</a></p></li>
</ol>
</div></blockquote>
<p>在许多场景中，都会遇到海量数据相似度计算的问题，如：电商场景中根据商品embedding计算相似度，取出相似的topk个商品。然而，这种计算相似度需要笛卡尔积的时间复杂度，在数据量较小时，时间还可以接受，但是当数据量达到几十万甚至几百几千万时，是没有办法接受的，这个时候就需要想其他办法。本文主要介绍海量item之间相似度计算问题——局部敏感哈希(Locality-Sensitive Hashing, LSH)之SimHash算法原理。</p>
<p>假设有3个商品，即：item1、item2和item3，每个商品用二维的embedding来表示，同时随机初始化6个超平面，即：s1、s2、s3、s4、s5和s6，每个超平面也是一个二维的embedding，这时可以在二维平面直角坐标系下表示，如下图：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-ace449e4d4c8a00b8d12cabbc9232ff3_1440w.webp" alt="img" style="zoom:33%; display: block; margin: auto;" />
<p>接下来，我们让每个item分别与6个超平面进行向量点积（相似度计算的一种方式），如果结果大于0，则结果为1，否则结果为0。因此会有如下结果表格：</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>s1</p></th>
<th class="head"><p>s2</p></th>
<th class="head"><p>s3</p></th>
<th class="head"><p>s4</p></th>
<th class="head"><p>s5</p></th>
<th class="head"><p>s6</p></th>
<th class="head"><p>SimHash</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>item1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>111000</p></td>
</tr>
<tr class="row-odd"><td><p>item2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>111000</p></td>
</tr>
<tr class="row-even"><td><p>item3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>000111</p></td>
</tr>
</tbody>
</table>
<p>通过上面的表格，item1、item2和超平面s1、s2、s3的相似度（向量点积）大于0，对应表格中的值1；与s4、s5、s6的相似度小于0，对应表格中的值0。同理，item3和超平面s1、s2、s3的相似度小于0，对应表格中的值0；与s4、s5、s6的相似度大于0，对应表格中的值1。这时，把每个超平面叫做哈希函数，SimHash值是每个item与各个超平面向量点积后的二进制结果。我们发现item1与item2的SimHash值是一样的，而与item3的SimHash值不同。这时把SimHash值相同的放在一个桶里面，如下图：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-9de151911806b2032c081b2ce88dbb9d_1440w.jpeg" alt="img" style="zoom: 67%; display: block; margin: auto;" />
<p>如果有几十万这样的item，SimHash算法计算后，每个桶都会有一定数量的item。这时计算item的topk个相似item时，只需要将此item与对应桶中其他item进行相似度计算，然后找到其topk个相似的item。下面是SimHash算法伪代码：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221027082521020.png" alt="image-20221027082521020" style="zoom:50%; display: block; margin: auto;" /></section>
<section id="tf-idf">
<h2>TF-IDF<a class="headerlink" href="#tf-idf" title="Link to this heading">#</a></h2>
<p>TF-IDF算法通常用于提取一篇文章的关键词，算法的核心思想比较简单，我们也可以加以拓展应用到推荐系统中。</p>
<section id="tf">
<h3>词频 TF 计算方式<a class="headerlink" href="#tf" title="Link to this heading">#</a></h3>
<p>词频的计算方式有多种，比较常见的为方式一，在提取文章关键词的场景下，方式一和方式二是等价的（计算每个词的TF-IDF值，方式二相对于方式一相当于都除了同一个常数，而后按照TF-IDF值倒排取TOPN个词作为文章的关键词）。</p>
<p>方式一：</p>
<div class="math notranslate nohighlight">
\[
词频(TF) = 某个词在文章中出现的次数
\]</div>
<p>方式二：</p>
<div class="math notranslate nohighlight">
\[
词频(TF) = \frac{某个词在文章中出现的次数}{文章的总词数}
\]</div>
</section>
<section id="idf">
<h3>逆文档频率 IDF 计算方式<a class="headerlink" href="#idf" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
逆文档频率(IDF) = log_2(\frac{语料库的文档总数}{包含该词的文档总数 + 1})
\]</div>
</section>
<section id="id4">
<h3>TF-IDF 计算方式<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
TF\text{-}IDF = 词频TF \times 逆文档频率IDF
\]</div>
</section>
<section id="id5">
<h3>代码实现<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>


<span class="c1"># 示例文档</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The quick brown fox jumps over the lazy dog.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Never jump over the lazy dog quickly.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;A quick brown dog outpaces a quick fox.&quot;</span>
<span class="p">]</span>


<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;预处理文档&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">doc</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">compute_tf</span><span class="p">(</span><span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;计算词频TF&quot;&quot;&quot;</span>
    <span class="n">tf_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="n">tf_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">doc_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tf_dict</span><span class="p">:</span>
        <span class="n">tf_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">/=</span> <span class="nb">float</span><span class="p">(</span><span class="n">doc_len</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf_dict</span>


<span class="k">def</span> <span class="nf">compute_idf</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;计算逆文档频率IDF&quot;&quot;&quot;</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
    <span class="n">idf_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">all_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">:</span>
        <span class="n">containing_docs</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">)</span>
        <span class="c1"># math.log 计算出来的值可能会是负数，尤其是当词频很高时（例如，词出现在所有文档中）</span>
        <span class="c1"># 添加1到最终的IDF值：避免负值并确保所有IDF值都是正的</span>
        <span class="n">idf_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">N</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">containing_docs</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">idf_dict</span>


<span class="k">def</span> <span class="nf">compute_tfidf</span><span class="p">(</span><span class="n">tf</span><span class="p">,</span> <span class="n">idf</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;计算TF-IDF值&quot;&quot;&quot;</span>
    <span class="n">tfidf</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tf_val</span> <span class="ow">in</span> <span class="n">tf</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tfidf</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_val</span> <span class="o">*</span> <span class="n">idf</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tfidf</span>


<span class="n">preprocessed_docs</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
<span class="n">tf_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_tf</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">preprocessed_docs</span><span class="p">]</span>
<span class="n">idf</span> <span class="o">=</span> <span class="n">compute_idf</span><span class="p">(</span><span class="n">preprocessed_docs</span><span class="p">)</span>
<span class="n">tfidf_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_tfidf</span><span class="p">(</span><span class="n">tf</span><span class="p">,</span> <span class="n">idf</span><span class="p">)</span> <span class="k">for</span> <span class="n">tf</span> <span class="ow">in</span> <span class="n">tf_list</span><span class="p">]</span>


<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tfidf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tfidf_list</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">文档 </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">documents</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">tfidf</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;词: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">, TF-IDF值: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>文档 1: The quick brown fox jumps over the lazy dog.
词: the, TF-IDF值: 0.2222
词: jumps, TF-IDF值: 0.1562
词: quick, TF-IDF值: 0.1111
词: brown, TF-IDF值: 0.1111
词: fox, TF-IDF值: 0.1111
词: over, TF-IDF值: 0.1111
词: lazy, TF-IDF值: 0.1111
词: dog, TF-IDF值: 0.0791

文档 2: Never jump over the lazy dog quickly.
词: never, TF-IDF值: 0.2008
词: jump, TF-IDF值: 0.2008
词: quickly, TF-IDF值: 0.2008
词: over, TF-IDF值: 0.1429
词: the, TF-IDF值: 0.1429
词: lazy, TF-IDF值: 0.1429
词: dog, TF-IDF值: 0.1018

文档 3: A quick brown dog outpaces a quick fox.
词: a, TF-IDF值: 0.3514
词: quick, TF-IDF值: 0.2500
词: outpaces, TF-IDF值: 0.1757
词: brown, TF-IDF值: 0.1250
词: fox, TF-IDF值: 0.1250
词: dog, TF-IDF值: 0.0890
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="lstm-long-short-term-memories">
<h2>LSTM(Long Short Term Memories)<a class="headerlink" href="#lstm-long-short-term-memories" title="Link to this heading">#</a></h2>
<p>LSTM(Long Short Term Memories) 是一类特殊的循环神经网络结构，其隐藏层有着特殊的结构，给出了一种计算隐藏层状态的新方法。<strong>通过引入门控机制来解决 RNN 的梯度消失问题，从而能够学习到长距离依赖。</strong></p>
<p>我们之前分析了 RNN 的简单结构，其结构大致如下图所示：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d16edd7a-1baf-489f-ac7b-f9feae1aa536.png" style="zoom: 25%; display: block; margin: auto;" />
<p>LSTM 的基本结构如下图所示：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/f0470b7a-fc46-43a9-b340-13bc2da1de6a.png" style="zoom:25%; display: block; margin: auto;" />
<p>图中各个符号的定义：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/e491ec00-1435-4d7b-9743-fd6da1f6b317.png" style="zoom:75%; display: block; margin: auto;" />
<p>各个符号的语义如下：</p>
<ul class="simple">
<li><p>Neural NetWork Layer：该图表示一个神经网络层</p></li>
<li><p>Pointwise Operation：该图表示一种操作，如加号表示矩阵或向量的求和，乘号表示向量的乘法操作</p></li>
<li><p>Vector Tansfer：每一条线表示一个向量，从一个节点输出到另一个节点</p></li>
<li><p>Concatenate：该图表示两个向量的合并，即由两个向量合并为一个向量，如有 <span class="math notranslate nohighlight">\(x_1\)</span> 和 <span class="math notranslate nohighlight">\(x_2\)</span> 两向量合并后为 <span class="math notranslate nohighlight">\([x_1,x_2]\)</span> 向量</p></li>
<li><p>Copy：该图表示一个向量复制了两个向量，其中两个向量值相同</p></li>
</ul>
<section id="rnn">
<h3>对 RNN 简单结构的解释<a class="headerlink" href="#rnn" title="Link to this heading">#</a></h3>
<p>为了接下来叙述的更清楚，先描述一下 RNN 的简单结构。</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_t\)</span> 为 t 时刻的输入</p></li>
<li><p><span class="math notranslate nohighlight">\(h_t\)</span> 首先为 t 时刻的隐藏层状态，同时也为 t + 1 时刻的输入，这里还是直接把 <span class="math notranslate nohighlight">\(h_t\)</span> 作为输出而不做进一步处理</p></li>
</ul>
<p>那么我们可以理解成在时刻 t， <span class="math notranslate nohighlight">\(h_{t-1}\)</span> 和 <span class="math notranslate nohighlight">\(x_t\)</span> 作为一个激活函数为 tanh 的神经网络层的输入，通过计算得到了当前时刻的隐藏层状态 <span class="math notranslate nohighlight">\(h_t\)</span>，这个 <span class="math notranslate nohighlight">\(h_t\)</span> 既是下一时刻的输入，也是当前时刻的输出。</p>
</section>
<section id="lstm">
<h3>LSTM结构拆分<a class="headerlink" href="#lstm" title="Link to this heading">#</a></h3>
<section id="cell-state">
<h4>cell state<a class="headerlink" href="#cell-state" title="Link to this heading">#</a></h4>
<p>LSTM最核心的部分是<strong>cell state</strong>。时刻 t 对应的 cell state 是 <span class="math notranslate nohighlight">\(C_t\)</span>。如下图中的直线所示 cell state 贯穿所有时刻。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d3f9cf75-a7d1-4696-867c-a696dff8dea7.png" style="zoom: 33%; display: block; margin: auto;" />
</section>
<section id="id6">
<h4>LSTM 中的门<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<p>在前向传播的过程中，通过门来控制 <span class="math notranslate nohighlight">\(c_t\)</span> 中信息的增减。LSTM中的门是通过一个激活函数为sigmoid的神经网络层来实现的，门的输出值在 <span class="math notranslate nohighlight">\(0 \sim 1\)</span> 之间。然后把门的取值向量和目标数据按位相乘就可以达到控制数据流通的效果。LSTM中共有三个门，分别为forget gate，input gate，output gate。这三个门的计算方法公式一样，都是根据 <span class="math notranslate nohighlight">\(x_t\)</span> 和 <span class="math notranslate nohighlight">\(h_t−1\)</span> 来计算， 区别在于这三个门对应的神经网络层的权重矩阵和偏置不同。</p>
<section id="forget-gate-f-t">
<h5>forget gate <span class="math notranslate nohighlight">\(f_t\)</span><a class="headerlink" href="#forget-gate-f-t" title="Link to this heading">#</a></h5>
<p>首先考虑从上一时刻的 cell state 中丢弃什么信息，这由 forget gate 来控制。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/3457079d-cc45-4630-b8a5-8d4a7f7c592e.png" style="zoom:100%; display: block; margin: auto;" />
</section>
<section id="input-gate-i-t">
<h5>input gate <span class="math notranslate nohighlight">\(i_t\)</span><a class="headerlink" href="#input-gate-i-t" title="Link to this heading">#</a></h5>
<p>接着考虑当前时刻的新信息（candidate values, <span class="math notranslate nohighlight">\(\widetilde{C}_t\)</span>）有哪些需要添加到 cell state。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/e4fff2fd-e2a3-4076-a135-8c8a3fa8f5c6.png" style="zoom:33%; display: block; margin: auto;" />
<p>从公式中可以看出 candidates values(<span class="math notranslate nohighlight">\(\widetilde{C}_t\)</span>) 的计算方式就是简单的 RNN 结构中的隐藏层状态的计算方式。</p>
<p>计算出 <span class="math notranslate nohighlight">\(i_t\)</span> 和 <span class="math notranslate nohighlight">\(\widetilde{C}_t\)</span> 后，<span class="math notranslate nohighlight">\(i_t \circ \widetilde{C}\)</span>，就是要添加到 cell state 中的新信息，其中 <span class="math notranslate nohighlight">\(\circ\)</span> 代表两个向量按位乘。</p>
</section>
<section id="id7">
<h5>对以上两个门的总结<a class="headerlink" href="#id7" title="Link to this heading">#</a></h5>
<p>通过 forget gate 和 input gate 这两个门的控制作用，我们已经丢弃了 cell state 中该丢弃的那部分信息，并且向 cell state 中添加了该添加的新信息。图中的 <span class="math notranslate nohighlight">\(*\)</span> 表示按位乘。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/9072855b-09f5-43f2-8fbd-96f27738e316.png" style="zoom:33%; display: block; margin: auto;" />
</section>
<section id="output-gate-o-t">
<h5>output gate <span class="math notranslate nohighlight">\(o_t\)</span><a class="headerlink" href="#output-gate-o-t" title="Link to this heading">#</a></h5>
<p>通过之前的计算，我们已经得到了当前时刻的 cell state（<span class="math notranslate nohighlight">\(C_t\)</span>），接下来我们考虑通过当前的 cell state 产生输出。通过 output gate 来控制当前时刻的 cell state 有哪些信息应该被输出。图中的 <span class="math notranslate nohighlight">\(*\)</span> 表示按位乘。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d258b3f3-7bf0-40b5-840f-cbf57e1248e9.png" style="zoom:33%; display: block; margin: auto;" /></section>
</section>
</section>
</section>
<section id="gru">
<h2>GRU<a class="headerlink" href="#gru" title="Link to this heading">#</a></h2>
<section id="id8">
<h3>梯度消失和长距离依赖问题<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p>RNN 中存在的梯度消失问题会导致难以学习到长距离依赖的问题。由于梯度消失问题的存在，越早的时刻对参数的修正起到的作用就越小，也就是说模型很难捕捉到长距离依赖关系。</p>
</section>
<section id="id9">
<h3>GRU结构简介<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>GRU 引入了 <strong>reset gate</strong> 和 <strong>update gate</strong>。其结构图如下，其中 <span class="math notranslate nohighlight">\(*\)</span> 表示按位乘。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6ee03222-480d-457c-9509-a6270d8a4223-20221007224201070.png" style="zoom: 33%; display: block; margin: auto;" />
<section id="reset-gate-r-t-update-gate-z-t">
<h4>reset gate <span class="math notranslate nohighlight">\(r_t\)</span> 和 update gate <span class="math notranslate nohighlight">\(z_t\)</span><a class="headerlink" href="#reset-gate-r-t-update-gate-z-t" title="Link to this heading">#</a></h4>
<p>reset gate 用来控制计算当前时刻的新信息时，保留多少之前的记忆。举个例子来说明一下，假设每个时刻输入的是一个词的话，那么如果 <span class="math notranslate nohighlight">\(r_t\)</span> 为 0，那么 <span class="math notranslate nohighlight">\(\widetilde{h}_t\)</span> 中就会只包含当前词的信息。</p>
<div class="math notranslate nohighlight">
\[
{r}_t = \sigma (W_r \cdot [h_{t-1}, x_t])
\]</div>
<p>update gate 控制需要从前一时刻的隐藏层状态 <span class="math notranslate nohighlight">\(h_{t-1}\)</span> 中忘记多少信息，同时控制需要将多少当前时刻的新信息加入到隐藏层状态中。</p>
<div class="math notranslate nohighlight">
\[
{z}_t = \sigma (W_z \cdot [h_{t-1}, x_t])
\]</div>
<p>reset gate 允许模型丢弃一些和未来无关的信息，如果reset gate接近0，那么之前的隐藏层信息就会丢弃。update gate 控制当前时刻的隐藏层输出 <span class="math notranslate nohighlight">\(h_t\)</span> 需要保留多少之前的隐藏层信息，若 <span class="math notranslate nohighlight">\(z_t\)</span> 接近于 1，相当于我们之前把之前的隐藏层信息拷贝到当前时刻，可以学习长距离依赖。 一般来说那些具有短距离依赖的单元 reset gate 比较活跃，具有长距离依赖的单元 update gate 比较活跃。</p>
</section>
<section id="id10">
<h4>当前时刻的新信息<a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<p>接下来计算当前时刻的新信息（candidate values, <span class="math notranslate nohighlight">\(\widetilde{h}_t\)</span>）。这跟 LSTM 中的 candidates values(<span class="math notranslate nohighlight">\(\widetilde{C}_t\)</span>) 是类似的。计算方式如下：</p>
<div class="math notranslate nohighlight">
\[
\widetilde{h}_t = tanh(W_h \cdot [r_t \circ h_{t-1}, x_t])
\]</div>
</section>
</section>
</section>
<section id="resnet">
<h2>深度残差网络（ResNet）<a class="headerlink" href="#resnet" title="Link to this heading">#</a></h2>
<section id="id11">
<h3>深度网络出现的退化问题<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>深度残差网络是2015年提出的深度卷积网络，一经出世，便在ImageNet中斩获图像分类、检测、定位三项的冠军。从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果，但是实验发现深度网络出现了退化问题（Degradation problem）。网络深度增加时，网络准确度出现饱和，甚至出现下降。如下图所示：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0cd4eae7-e0fa-4b7d-9d31-183cf3560081.png" style="zoom:50%; display: block; margin: auto;" />
<p>可以看出56层的网络比20层网络效果还要差。但是这不是过拟合的问题，因为56层网络的训练误差同样高。</p>
</section>
<section id="id12">
<h3>只做恒等映射也不该出现退化问题<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>深度网络的退化问题至少说明深度网络不容易训练。但是考虑以下事实：现在已经有了一个浅层神经网络，通过向上堆积新层来建立深层网络。一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即向上堆积的层仅仅是在做恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。<strong>但是问题可能是，网络并不是那么容易的就能学到恒等映射。随着网络层数不断加深，求解器不能找到解决途径。</strong></p>
</section>
<section id="id13">
<h3>ResNet<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>ResNet 就是通过显式的修改网络结构，加入残差通路，让网络更容易的学习到恒等映射。通过改进，我们发现深层神经网络的性能不仅不比浅层神经网络差，还要高出不少。</p>
<section id="plaint-net">
<h4>Plaint net<a class="headerlink" href="#plaint-net" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/b971182e-16bd-490e-8074-02df67a62f67.png" style="zoom:50%; display: block; margin: auto;" />
<p><span class="math notranslate nohighlight">\(H(x)\)</span>代表的是我们最终想要得到的一个映射。在 Plaint net 中，我们就是希望这两层网络能够直接拟合出<span class="math notranslate nohighlight">\(H(x)\)</span>。</p>
</section>
<section id="residual-net">
<h4>Residual net<a class="headerlink" href="#residual-net" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d39f0035-7e3d-4675-958b-4e2407da8f88.png" style="zoom:50%; display: block; margin: auto;" />
<p><span class="math notranslate nohighlight">\(H(x)\)</span>代表的仍然是我们最终想要得到的一个映射。与 Plain net 不同的是，这里通过一个捷径连接（shortcut connections）直接将<span class="math notranslate nohighlight">\(x\)</span>传到了后面与这两层网络拟合出的结果相加，<span class="math notranslate nohighlight">\(H(x)\)</span>是我们最终想要得到的一个映射，假设这两层网络拟合出来的映射为<span class="math notranslate nohighlight">\(F(x)\)</span>，那么<span class="math notranslate nohighlight">\(F(x)\)</span>应该等于<span class="math notranslate nohighlight">\(H(x)-x\)</span>。</p>
<p><strong>做这种变换的作用</strong>：如果 x 已经是最优的了，也就是说我们希望得到的映射 <span class="math notranslate nohighlight">\(H(x)\)</span> 恰好就是此时的输入 <span class="math notranslate nohighlight">\(x\)</span>，也就是说要做恒等映射，这个时候只需要将权重值设定为 0。也就是让 <span class="math notranslate nohighlight">\(F(x) = 0\)</span> 就好了。我们发现这比直接学习 <span class="math notranslate nohighlight">\(H(x) = x\)</span> 要容易的多。</p>
<p>实际上残差网络相当于将学习目标改变了，学习的不再是一个完整的输出，而是目标值 <span class="math notranslate nohighlight">\(H(x)\)</span> 和 x 的差值，也就是这篇文章一直在讨论的残差 <span class="math notranslate nohighlight">\(F(x)\)</span>。并且有 <span class="math notranslate nohighlight">\(F(x) = H(x) - x\)</span>。</p>
</section>
<section id="residual-block">
<h4>Residual Block<a class="headerlink" href="#residual-block" title="Link to this heading">#</a></h4>
<p>残差网络（Residual Networks）由许多隔层相连的神经元子模块组成，我们称之为 Residual Block。</p>
</section>
<section id="id14">
<h4>34层的ResNet结构图<a class="headerlink" href="#id14" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/00fe0d43-20e7-4c0f-9867-0512f41f9c56.png" style="zoom:50%; display: block; margin: auto;" />
<p><strong>图中的虚线部分：</strong></p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/2fc76a01-0a38-4859-9f85-494c91a982fe.png" style="zoom:50%; display: block; margin: auto;" />
<p>经过捷径连接（shortcut connections）后，<span class="math notranslate nohighlight">\(H(x) = F(x) + x\)</span>，如果 <span class="math notranslate nohighlight">\(F(x)\)</span> 和 <span class="math notranslate nohighlight">\(x\)</span> 的通道数相同，则可直接相加。但是如果二者通道数不同，那么就不可以直接相加了。上图中的实线和虚线就是为了区分这两种情况：</p>
<ol class="arabic simple">
<li><p>实线：表示二者通道数相同，二者可以直接相加。</p></li>
<li><p>虚线：表示二者通道数不同，此时需要采用的计算方式为 <span class="math notranslate nohighlight">\(H(x)=F(x) + Wx\)</span>，其中 <span class="math notranslate nohighlight">\(W\)</span> 代表卷积操作，用来调整 <span class="math notranslate nohighlight">\(x\)</span> 的通道数。</p></li>
</ol>
</section>
<section id="id15">
<h4>两种残差学习单元<a class="headerlink" href="#id15" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ed8a3dac-d97f-4fa7-8bbb-60e30ccd0609.png" style="zoom:50%; display: block; margin: auto;" />
<p><strong>两种结构分别针对ResNet34（左图）和 ResNet50/101/152（右图）。右图的主要目的是减少参数数量。</strong></p>
<p>为了做个详细的对比，我们这里假设左图的残差单元的输入不是 64-d 的，而是 256-d 的，那么左图应该为两个 <span class="math notranslate nohighlight">\(3 \times 3, 256\)</span> 的卷积。参数总数为：</p>
<div class="math notranslate nohighlight">
\[
3 \times 3 \times 256 \times 256 \times 2 = 1179648
\]</div>
<p>说明：<span class="math notranslate nohighlight">\(3 \times 3 \times 256\)</span> 计算的是每个 filter 的参数数目，第 2 个 256 是说每层有 256 个filter，最后一个 2 是说一共有两层。</p>
<p>右图的输入同样为 256-d 的，首先通过一个 <span class="math notranslate nohighlight">\(1 \times 1, 64\)</span> 的卷积层将通道数降为 64。然后是一个 <span class="math notranslate nohighlight">\(3 \times 3, 64\)</span> 的卷积层。最后再通过一个 <span class="math notranslate nohighlight">\(1 \times 1, 256\)</span> 的卷积层通道数恢复为 256。参数总数为：</p>
<div class="math notranslate nohighlight">
\[
1 \times 1 \times 256 \times 64 + 3\times 3 \times 64 \times 64 + 1 \times 1 \times 64 \times 256 = 69632
\]</div>
<p>可见参数数量明显变少了。</p>
<p>通常来说对于常规的ResNet，可以用于34层或者更少的网络中（左图）；对于更深的网络（如101层），则使用右图，其目的是减少计算和参数量。</p>
</section>
<section id="id16">
<h4>不同深度的ResNet<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ee80109d-77f5-4833-a153-e6f5fb1111b4.png" style="zoom:50%; display: block; margin: auto;" /></section>
</section>
</section>
<section id="cnnpooling">
<h2>CNN模型中常见的Pooling操作<a class="headerlink" href="#cnnpooling" title="Link to this heading">#</a></h2>
<p>CNN是目前自然语言处理中和RNN并驾齐驱的两种最常见的深度学习模型。一般而言，输入的字或者词用Word Embedding的方式表达，这样本来一维的文本信息输入就转换成了二维的输入结构，假设输入X包含m个字符，而每个字符的Word Embedding的长度为d，那么输入就是m*d的二维向量。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1240.jpeg" style="zoom: 67%; display: block; margin: auto;" />
<p>这里可以看出，因为NLP中的句子长度是不同的，所以CNN的输入矩阵大小是不确定的，这取决于m的大小是多少。卷积层本质上是个特征抽取层，可以设定超参数F来指定设立多少个特征抽取器（Filter），对于某个Filter来说，可以想象有一个k*d大小的移动窗口从输入矩阵的第一个字开始不断往后移动，其中k是Filter指定的窗口大小，d是Word Embedding长度。对于某个时刻的窗口，通过神经网络的非线性变换，将这个窗口内的输入值转换为某个特征值，随着窗口不断往后移动，这个Filter对应的特征值不断产生，形成这个Filter的特征向量。这就是卷积层抽取特征的过程。每个Filter都如此操作，形成了不同的特征抽取器。Pooling 层则对Filter的特征进行降维操作，形成最终的特征。一般在Pooling层之后为全连层。下面我们重点介绍NLP中CNN模型常见的Pooling操作方法。</p>
<section id="max-pooling-over-time">
<h3>Max Pooling Over Time<a class="headerlink" href="#max-pooling-over-time" title="Link to this heading">#</a></h3>
<p>Max Pooling Over Time是NLP中CNN模型中最常见的一种下采样操作。意思是对于某个Filter的卷积运算结果，只取其中得分最大的那个值作为Pooling层保留值，其它特征值全部抛弃，值最大代表只保留这些特征中最强的，而抛弃其它弱的此类特征。</p>
<p>这个操作可以保证特征的位置与旋转不变性，因为不论这个强特征在哪个位置出现，都会不考虑其出现位置而能把它提出来。对于图像处理来说这种位置与旋转不变性是很好的特性，但是对于NLP来说，这个特性其实并不一定是好事，因为在很多NLP的应用场合，特征的出现位置信息是很重要的，比如主语出现位置一般在句子头，宾语一般出现在句子尾等，这些位置信息其实有时候对于分类任务来说还是很重要的，但是Max Pooling 基本把这些信息抛掉了。</p>
<p>其次，Max Pooling能减少模型参数数量，防止模型过拟合。因为经过Pooling操作后，往往把2D或者1D的数组转换为单一数值。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1240-20221007222713043.jpeg" style="zoom:67%; display: block; margin: auto;" />
</section>
<section id="k-max-pooling">
<h3>K-Max Pooling<a class="headerlink" href="#k-max-pooling" title="Link to this heading">#</a></h3>
<p>K-Max Pooling的意思是：原先的Max Pooling Over Time从Convolution层一系列特征值中只取最强的那个值，那么我们思路可以扩展一下，K-Max Pooling可以取所有特征值中得分在TopK的值，并保留这些特征值原始的先后顺序（图3是2-max Pooling的示意图），就是说通过多保留一些特征信息供后续阶段使用。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1240-20221007222713024.jpeg" style="zoom:67%; display: block; margin: auto;" />
</section>
<section id="chunk-max-pooling">
<h3>Chunk-Max Pooling<a class="headerlink" href="#chunk-max-pooling" title="Link to this heading">#</a></h3>
<p>Chunk-MaxPooling的思想是：把某个Filter对应的Convolution层的所有特征向量进行分段，切割成若干段后，在每个分段里面各自取得一个最大特征值，比如将某个Filter的特征向量切成3个Chunk，那么就在每个Chunk里面取一个最大值，于是获得3个特征值（如图4所示，不同颜色代表不同分段）。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1240-20221007222713024-5152833.jpeg" style="zoom:67%; display: block; margin: auto;" />
<p>乍一看Chunk-Max Pooling思路类似于K-Max Pooling，因为它也是从Convolution层取出了K个特征值，但是两者的主要区别是：</p>
<ul class="simple">
<li><p>K-Max Pooling是一种全局取TopK特征的操作方式</p></li>
<li><p>Chunk-Max Pooling则是先分段，在分段内包含特征数据里面取最大值，所以其实是一种局部TopK的特征抽取方式。</p></li>
</ul>
</section>
</section>
<section id="l-2-norm">
<h2>L-2 Norm<a class="headerlink" href="#l-2-norm" title="Link to this heading">#</a></h2>
<p><strong>L-2 Norm的作用是把embedding的长度都归一化为1，也就是说把它们都映射到一个长度为1的单位超球面上去。如果把它投影到单位超平面上，会增加训练稳定性和投影空间的线性可分性</strong>。增加线性可分性，意思也就是说你用简单算法也能得到比较好的效果。试想一下，一个单位超球面和一个不规则球面的向量空间，是不是前者更容易做到线性可分呢？这是目前图像领域里面得出的结论。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240313085555643.png" alt="image-20240313085555643" style="zoom: 67%; display: block; margin: auto;" /><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">vector</span><span class="p">):</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">vector</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">vector</span>
    <span class="k">return</span> <span class="n">vector</span> <span class="o">/</span> <span class="n">norm</span>


<span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">normalized_vector</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;向量 </span><span class="si">{</span><span class="n">vector</span><span class="si">}</span><span class="s2"> 的归一化结果是: </span><span class="si">{</span><span class="n">normalized_vector</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>向量 [3 4] 的归一化结果是: [0.6 0.8]
</pre></div>
</div>
</div>
</div>
</section>
<section id="batch-normalization">
<h2>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Link to this heading">#</a></h2>
<p>BN假设所有样本独立同分布，并且使用所有样本的共享统计量进行normalization，<span class="math notranslate nohighlight">\(E\)</span>和<span class="math notranslate nohighlight">\(Var\)</span>分别是滑动均值和方差，<span class="math notranslate nohighlight">\(\gamma\)</span>和<span class="math notranslate nohighlight">\(\beta\)</span>是可以学习的scale和bias。多场景建模时，一般为每个场景添加独立的BN层，确保每个场景的数据都能被正确归一化。</p>
<div class="math notranslate nohighlight">
\[
z' = \gamma \frac{z-E}{\sqrt{Var + \epsilon}} + \beta
\]</div>
<p>Batch Normalization（批量归一化）的主要作用可以概括为以下几点：</p>
<ol class="arabic simple">
<li><p>稳定训练过程：训练神经网络时，数据在每一层都会经过一系列的变化，导致分布可能会变得不稳定。Batch Normalization通过将数据重新调整为标准正态分布，使训练过程更加平稳，减少训练的不确定性。</p></li>
<li><p>加速收敛：通过归一化处理，模型在训练过程中可以更快地达到最优状态。因为数据分布被稳定下来，模型可以用更高的学习率，从而减少训练的时间。</p></li>
<li><p>防止过拟合：Batch Normalization有助于防止模型过拟合。这种归一化方法有类似正则化的效果，减少对其他正则化方法（如Dropout）的依赖。</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">BatchNormalization</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>  <span class="c1"># 缩放参数γ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>  <span class="c1"># 平移参数β</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;前向传播&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1"># 计算当前批次的均值和方差</span>
            <span class="n">batch_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">batch_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># 归一化</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">batch_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">batch_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="c1"># 平滑的更新运行中的均值和方差</span>
            <span class="c1"># 平衡了当前批次和历史批次的统计量，防止单个批次对整体统计量的剧烈影响</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_mean</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_var</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 使用运行中的均值和方差进行归一化</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">X_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">running_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="c1"># 应用缩放和偏移</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_norm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">set_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">training</span>

<span class="c1"># 示例使用</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">bn</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># 训练模式下的前向传播</span>
<span class="n">bn</span><span class="o">.</span><span class="n">set_training</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output_train</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;训练模式下的归一化结果:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">output_train</span><span class="p">)</span>

<span class="c1"># 测试模式下的前向传播</span>
<span class="n">bn</span><span class="o">.</span><span class="n">set_training</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">output_test</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;测试模式下的归一化结果:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">output_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1.76405235  0.40015721  0.97873798]
 [ 2.2408932   1.86755799 -0.97727788]
 [ 0.95008842 -0.15135721 -0.10321885]
 [ 0.4105985   0.14404357  1.45427351]
 [ 0.76103773  0.12167502  0.44386323]]
训练模式下的归一化结果:
 [[ 0.79834163 -0.10633427  0.73103552]
 [ 1.50498534  1.93980876 -1.57728392]
 [-0.40789413 -0.87536574 -0.54579564]
 [-1.20737923 -0.46345902  1.29222109]
 [-0.68805361 -0.49464972  0.09982295]]
测试模式下的归一化结果:
 [[ 1.6881266   0.36139964  0.95638524]
 [ 2.1785064   1.8657813  -1.02779382]
 [ 0.85105179 -0.20401383 -0.14114988]
 [ 0.29624413  0.09883153  1.43876764]
 [ 0.65663338  0.07589925  0.41380923]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="id17">
<h2>交叉熵损失函数<a class="headerlink" href="#id17" title="Link to this heading">#</a></h2>
<section id="id18">
<h3>定义<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p>交叉熵损失函数（Cross-Entropy Loss），在机器学习中特别是分类任务中非常常用。它衡量的是模型预测的概率分布与真实概率分布之间的差异。换句话说，交叉熵损失越小，表示模型的预测结果越接近真实标签。</p>
<p>对于一个分类任务，交叉熵损失函数的定义如下：</p>
<div class="math notranslate nohighlight">
\[
\text{CE}(y, \hat{y}) = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)
\]</div>
<p>其中：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> 是真实标签的独热编码（one-hot encoding）。</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y} \)</span> 是模型预测的概率分布。</p></li>
<li><p><span class="math notranslate nohighlight">\( C \)</span> 是类别的总数。</p></li>
<li><p><span class="math notranslate nohighlight">\( y_i \)</span> 是真实标签中第 <span class="math notranslate nohighlight">\(i\)</span> 类的值（对于独热编码，只有一个位置是1，其余为0）。</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}_i \)</span> 是模型预测的第 <span class="math notranslate nohighlight">\(i\)</span> 类的概率。</p></li>
</ul>
</section>
<section id="id19">
<h3>通俗解释<a class="headerlink" href="#id19" title="Link to this heading">#</a></h3>
<p>可以用一个简单的例子来解释交叉熵损失函数。假设我们在做一个猫和狗的二分类任务：</p>
<ul class="simple">
<li><p>如果图片是猫，真实标签 <span class="math notranslate nohighlight">\( y \)</span> 可以表示为 [1, 0]。</p></li>
<li><p>如果图片是狗，真实标签 <span class="math notranslate nohighlight">\( y \)</span> 可以表示为 [0, 1]。</p></li>
</ul>
<p>模型输出的是对这两个类别的预测概率，例如：</p>
<ul class="simple">
<li><p>模型预测图片是猫的概率为 0.9，是狗的概率为 0.1，预测结果为 [0.9, 0.1]。</p></li>
</ul>
<p>交叉熵损失函数计算真实标签和预测概率之间的差异，对于标签 [1, 0] 和预测 [0.9, 0.1]，交叉熵损失为：</p>
<div class="math notranslate nohighlight">
\[
\text{CE} = - (1 \cdot \log(0.9) + 0 \cdot \log(0.1)) = - \log(0.9)
\]</div>
<p>损失值越小，表示预测结果越接近真实标签。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    计算交叉熵损失</span>

<span class="sd">    参数:</span>
<span class="sd">    y_true -- 真实标签，形状为 (batch_size, num_classes)</span>
<span class="sd">    y_pred -- 预测概率，形状为 (batch_size, num_classes)</span>

<span class="sd">    返回:</span>
<span class="sd">    loss -- 交叉熵损失值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-9</span>  <span class="c1"># 避免log(0)的情况</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>


<span class="c1"># 示例用法</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># 真实标签 (one-hot encoded)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>  <span class="c1"># 预测概率</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;交叉熵损失:&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>交叉熵损失: 0.7418746839526391
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="focal-loss">
<h2>Focal Loss<a class="headerlink" href="#focal-loss" title="Link to this heading">#</a></h2>
<section id="id20">
<h3>简介<a class="headerlink" href="#id20" title="Link to this heading">#</a></h3>
<p>Focal Loss 是一种改进的交叉熵损失函数，旨在解决样本类别不均衡的问题。它通过降低对易分类样本的损失权重，增强对难分类样本的关注，从而改善模型在不平衡数据集上的表现。Focal Loss 的公式如下：</p>
<div class="math notranslate nohighlight">
\[
Loss_{fl} =-\alpha_{t}(1- p_{t})^\gamma log(p_{t})
\]</div>
<p>其中公式中各个部分的含义如下：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p_t\)</span>是模型对正确类别的预测概率。</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_t\)</span>是平衡因子，用于平衡正负样本的影响（可选）。</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span>是调整因子，用于控制易分类样本的权重降低程度。</p></li>
</ul>
<p>Focal Loss是如何做到降低对易分类样本的损失权重，增强对难分类样本的关注的？</p>
<ul class="simple">
<li><p>对于易分类样本，<span class="math notranslate nohighlight">\(p_{t}\)</span>接近 1（模型对其分类的信心高），那么<span class="math notranslate nohighlight">\(1-p_{t}\)</span>接近 0，因此 <span class="math notranslate nohighlight">\((1- p_{t})^\gamma\)</span> 也接近 0，这个调节项会显著降低易分类样本的损失值。</p></li>
<li><p>对于难分类样本，<span class="math notranslate nohighlight">\(p_{t}\)</span>接近 0（模型对其分类的信心低），那么<span class="math notranslate nohighlight">\(1-p_{t}\)</span>接近 1，因此 <span class="math notranslate nohighlight">\((1- p_{t})^\gamma\)</span> 仍接近 1，这个调节项对难分类样本的影响很小。</p></li>
</ul>
</section>
<section id="id21">
<h3>推导过程（可跳过该章节）<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<p>交叉熵损失函数（其中 <span class="math notranslate nohighlight">\(\hat p\)</span>为预测概率大小）：</p>
<div class="math notranslate nohighlight">
\[
Loss = L(y, \hat p) =-ylog(\hat p) - (1-y)log(1-\hat p)
\]</div>
<p>对于二分类问题，先简化公式：</p>
<div class="math notranslate nohighlight">
\[
Loss = L(y, \hat p) =-log(\hat p) - log(1-\hat p)
\]</div>
<p>对于所有样本来说，假设N为总样本量，m为正样本量，n为负样本量，当m &lt;&lt; n时，负样本就会在损失函数里占据主导地位，由于损失函数的倾斜，模型训练过程中会倾向于样本多的类别，造成模型对少样本类别的性能较差。</p>
<div class="math notranslate nohighlight">
\[
Loss =\frac{1}{N}(\sum_{y_i=1}^{m}-log(\hat p) + \sum_{y_i=o}^{n}-log(1-\hat p))
\]</div>
<p>focal loss具体形式：</p>
<div class="math notranslate nohighlight">
\[
Loss =\frac{1}{N}(\sum_{y_i=1}^{m}-(1-\hat p)^\gamma log(\hat p) + \sum_{y_i=o}^{n}-\hat p^{\gamma} log(1-\hat p))
\]</div>
<p>如果我做以下定义：
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240312223812014.png" alt="image-20240312223812014" style="zoom:50%; display: block; margin: auto;" /></p>
<p><strong>focal loss表达式：</strong></p>
<div class="math notranslate nohighlight">
\[
Loss_{fl} =-(1- p_t)^\gamma log(p_t)
\]</div>
<p><strong>交叉熵表达式：</strong></p>
<div class="math notranslate nohighlight">
\[
Loss_{ce} =-log(p_t)
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">focal_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    计算Focal Loss</span>

<span class="sd">    参数:</span>
<span class="sd">    y_true -- 真实标签，形状为 (batch_size, num_classes)</span>
<span class="sd">    y_pred -- 预测概率，形状为 (batch_size, num_classes)</span>
<span class="sd">    alpha -- 平衡因子，默认值为 0.25</span>
<span class="sd">    gamma -- 调整因子，默认值为 2.0</span>

<span class="sd">    返回:</span>
<span class="sd">    loss -- Focal Loss 值</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-9</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># 避免log(0)情况</span>
    <span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">y_true</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">cross_entropy</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>


<span class="c1"># 示例用法</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>  <span class="c1"># 真实标签</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>  <span class="c1"># 预测概率</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">focal_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Focal Loss:&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Focal Loss: 0.09273549740974675
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="oneepoch">
<h2>OneEpoch现象<a class="headerlink" href="#oneepoch" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p>paper: <strong>Multi-Epoch Learning</strong> for Deep Click-Through Rate Prediction Models</p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/669063912">阿里OneEpoch VS 快手MultiEpoch</a></p></li>
</ol>
</div></blockquote>
<p><strong>模型AUC在第一个epoch内逐步提升，但是从第二个epoch开始，AUC效果突然剧烈下降</strong>。产生OneEpoch现象的原因：</p>
<ul class="simple">
<li><p>embedding  + mlp的结构</p></li>
<li><p>能使模型快速收敛的优化器算法（eg: 学习率较大的adam优化器）</p></li>
<li><p>高维稀疏特征（eg: item_id等细粒度特征）</p></li>
<li><p>其他不相关因素：模型参数量、激活函数、batch size、weight decay、dropout</p></li>
</ul>
<p>多Epoch探究：<strong>每一轮训练都重置embedding，更新embedding和mlp</strong>，避免embedding层过拟合，并让mlp层学的更充分。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/v2-4a751d00853416d18182ef4d97f2f5b4_1440w-0067336.webp" alt="img" style="zoom:50%; display: block; margin: auto;" /></section>
<section id="id22">
<h2>温度系数<a class="headerlink" href="#id22" title="Link to this heading">#</a></h2>
<p>温度系数常应用在召回/粗排等双塔模型结构中，点乘之后除以一个固定的系数（温度系数），τ是温度系数，一般来说加一个温度系数是有效的。原因是温度系数可以让模型更聚焦于hard负例，且τ越小越聚焦，也就不用花费大力气挖掘hard负例。</p>
<section id="hard">
<h3>Hard样本<a class="headerlink" href="#hard" title="Link to this heading">#</a></h3>
<p>Hard样本是指那些模型难以正确分类的样本。这些样本可能由于以下原因难以分类：</p>
<ul class="simple">
<li><p>数据不平衡</p></li>
<li><p>特征不明显</p></li>
<li><p>噪声或异常数据</p></li>
</ul>
</section>
<section id="id23">
<h3>温度系数与Hard样本的具体关系<a class="headerlink" href="#id23" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>低温度系数（&lt;1）</strong>:</p>
<ul class="simple">
<li><p><strong>效果</strong>: 使得模型的输出概率分布更尖锐（更接近于0或1）。</p></li>
<li><p><strong>对hard样本的影响</strong>:</p>
<ul>
<li><p>模型对hard样本的预测可能会更不确定，因为这些样本本身难以分类，输出概率会更加极端（高概率的类别与低概率的类别差距更大）。</p></li>
<li><p>模型可能会更加确信自己的错误预测，从而难以在后续训练中纠正。</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>高温度系数（&gt;1）</strong>:</p>
<ul class="simple">
<li><p><strong>效果</strong>: 使得模型的输出概率分布更平滑（更接近于均匀分布）。</p></li>
<li><p><strong>对hard样本的影响</strong>:</p>
<ul>
<li><p>模型对hard样本的预测会变得更不确定，输出的概率更接近于均匀分布（各类别的概率差距缩小）。</p></li>
<li><p>这种不确定性可以提示模型在训练过程中对这些hard样本进行更多关注，从而帮助模型更好地学习和纠正错误。</p></li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section id="id24">
<h3>温度系数对预估值的影响<a class="headerlink" href="#id24" title="Link to this heading">#</a></h3>
<p>假设一个分类模型在一个三类问题中输出如下分数（未归一化）：</p>
<ul class="simple">
<li><p>易分类样本：[10, 2, 1]</p></li>
<li><p>难分类样本：[3, 3, 2.5]</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>无温度调整（温度=1）</strong>:</p>
<ul class="simple">
<li><p>易分类样本的Softmax概率：[0.999, 0.001, 0.000]</p></li>
<li><p>难分类样本的Softmax概率：[0.4, 0.4, 0.2]</p></li>
</ul>
</li>
<li><p><strong>降低温度（温度=0.5）</strong>:</p>
<ul class="simple">
<li><p>易分类样本的Softmax概率：[1.0, 0.0, 0.0]</p></li>
<li><p>难分类样本的Softmax概率：[0.42, 0.42, 0.16]（更尖锐）</p></li>
</ul>
</li>
<li><p><strong>升高温度（温度=2）</strong>:</p>
<ul class="simple">
<li><p>易分类样本的Softmax概率：[0.88, 0.06, 0.06]</p></li>
<li><p>难分类样本的Softmax概率：[0.34, 0.34, 0.32]（更平滑）</p></li>
</ul>
</li>
</ol>
</section>
<section id="loss">
<h3>温度系数对loss的影响<a class="headerlink" href="#loss" title="Link to this heading">#</a></h3>
<p>在机器学习和深度学习中，损失函数（loss）是用于衡量模型预测与真实标签之间差异的指标。对于分类任务，常用的损失函数是交叉熵损失。温度系数的调整会影响模型的预测概率分布，从而对损失值产生影响。</p>
<p>假设模型的输出分数为：[3.0, 1.0, 0.1]，且真实标签对应的类别为第一个类别（索引0）。</p>
<ol class="arabic simple">
<li><p><strong>无温度调整（温度=1）</strong>:</p>
<ul class="simple">
<li><p>使用Softmax函数计算概率：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( P_0 = \frac{e^{3.0}}{e^{3.0} + e^{1.0} + e^{0.1}} \approx 0.84 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_1 = \frac{e^{1.0}}{e^{3.0} + e^{1.0} + e^{0.1}} \approx 0.11 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_2 = \frac{e^{0.1}}{e^{3.0} + e^{1.0} + e^{0.1}} \approx 0.05 \)</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>降低温度（温度=0.5）</strong>:</p>
<ul class="simple">
<li><p>调整后的Softmax计算：</p>
<ul>
<li><p>调整后的分数：[6.0, 2.0, 0.2]（分数被拉开）</p></li>
<li><p><span class="math notranslate nohighlight">\( P_0 = \frac{e^{6.0}}{e^{6.0} + e^{2.0} + e^{0.2}} \approx 0.97 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_1 = \frac{e^{2.0}}{e^{6.0} + e^{2.0} + e^{0.2}} \approx 0.03 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( P_2 = \frac{e^{0.2}}{e^{6.0} + e^{2.0} + e^{0.2}} \approx 0.0007 \)</span></p></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>交叉熵损失的公式为：
$<span class="math notranslate nohighlight">\( L = - \sum_{i} y_i \log(p_i) \)</span><span class="math notranslate nohighlight">\(
其中 \)</span>y_i<span class="math notranslate nohighlight">\( 是真实标签的one-hot编码，\)</span>p_i$ 是模型预测的概率。</p>
<ol class="arabic simple">
<li><p><strong>无温度调整时的损失</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y = [1, 0, 0] \)</span></p></li>
<li><p>损失：<span class="math notranslate nohighlight">\( L = - \log(0.84) \approx 0.17 \)</span></p></li>
</ul>
</li>
<li><p><strong>降低温度后的损失</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y = [1, 0, 0] \)</span></p></li>
<li><p>损失：<span class="math notranslate nohighlight">\( L = - \log(0.97) \approx 0.03 \)</span></p></li>
</ul>
</li>
</ol>
<p>当温度系数降低时，模型的输出概率分布变得更尖锐，预估值被拉开。如果模型预测正确，类别的概率接近1，交叉熵损失将会变小。相反，如果模型预测错误，高温度系数导致的平滑概率分布可能会导致较高的损失。因此，预估值被拉开不一定会导致损失变大。具体影响取决于模型预测的正确性。对于正确的预测，预估值被拉开会降低损失；对于错误的预测，预估值被拉开会增加损失。</p>
</section>
</section>
<section id="softmax">
<h2>softmax函数<a class="headerlink" href="#softmax" title="Link to this heading">#</a></h2>
<p>一个非常漂亮且实用的函数。Softmax 函数公式如下，它可以将数值处理成概率。</p>
<div class="math notranslate nohighlight">
\[
softmax(x_i) = \frac {exp(x_i)}{\sum_{j}exp(x_j)}
\]</div>
<p><strong>构建神经网络时，碰到多分类问题，我们可以将全连接层的输出通过函数转为概率</strong>。例如，我们在鸢尾花分类问题中，如果最后全连接层给出了 3 个输出，分别是 -1.3，2.6，-0.9。通过 Softmax 函数处理之后，就可以得到 0.02，0.95，0.03 的概率值。也就是说有 95% 的概率属于 Versicolor 类别的鸢尾花。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/document-uid214893labid6102timestamp1532323296407-20240509081428043.png" alt="img" style="zoom: 67%; display: block; margin: auto;" /><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Softmax 实现</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">softmax</span><span class="p">([</span><span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">]),</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.02, 0.95, 0.03])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id25">
<h2>蒸馏学习<a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ul class="simple">
<li><p>《Dislillation the Knowledge in a Neural Network》</p></li>
</ul>
</div></blockquote>
<p>MD（Model Distillation）：教师模型和学生模型处理相同的输入特征，其中教师模型会比学生模型更为复杂。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240411010129023.png" alt="image-20240411010129023" style="zoom: 50%; display: block; margin: auto;" />
<p>PFD（Privileged Features Distillation）：教师模型和学生模型使用相同网络结构，而处理不同的输入特征，学生模型处理常规特征（Regular Features），教师模型处理优势特征（Privileged Features）和常规特征（Regular Features）。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20240411010157990.png" alt="image-20240411010157990" style="zoom:50%; display: block; margin: auto;" /></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./02-推荐系统"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F01-%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">推荐系统01-推荐&amp;广告领域常见的业务指标</p>
      </div>
    </a>
    <a class="right-next"
       href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F03-%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">推荐系统03-双塔模型优化思路</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beam-search">beam search</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-search">贪心搜索(greedy search)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">集束搜索(beam search)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simhash">SimHash</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">TF-IDF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tf">词频 TF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#idf">逆文档频率 IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">TF-IDF 计算方式</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">代码实现</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-long-short-term-memories">LSTM(Long Short Term Memories)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rnn">对 RNN 简单结构的解释</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm">LSTM结构拆分</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cell-state">cell state</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">LSTM 中的门</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#forget-gate-f-t">forget gate <span class="math notranslate nohighlight">\(f_t\)</span></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#input-gate-i-t">input gate <span class="math notranslate nohighlight">\(i_t\)</span></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">对以上两个门的总结</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#output-gate-o-t">output gate <span class="math notranslate nohighlight">\(o_t\)</span></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gru">GRU</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">梯度消失和长距离依赖问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">GRU结构简介</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#reset-gate-r-t-update-gate-z-t">reset gate <span class="math notranslate nohighlight">\(r_t\)</span> 和 update gate <span class="math notranslate nohighlight">\(z_t\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">当前时刻的新信息</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet">深度残差网络（ResNet）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">深度网络出现的退化问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">只做恒等映射也不该出现退化问题</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">ResNet</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#plaint-net">Plaint net</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-net">Residual net</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-block">Residual Block</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">34层的ResNet结构图</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">两种残差学习单元</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">不同深度的ResNet</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnnpooling">CNN模型中常见的Pooling操作</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#max-pooling-over-time">Max Pooling Over Time</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-max-pooling">K-Max Pooling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chunk-max-pooling">Chunk-Max Pooling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-norm">L-2 Norm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">交叉熵损失函数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">定义</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">通俗解释</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#focal-loss">Focal Loss</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">推导过程（可跳过该章节）</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#oneepoch">OneEpoch现象</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">温度系数</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hard">Hard样本</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">温度系数与Hard样本的具体关系</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">温度系数对预估值的影响</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss">温度系数对loss的影响</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">softmax函数</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">蒸馏学习</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yi Longhao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>