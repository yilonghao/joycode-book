
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>推荐系统04-序列特征优化方案 &#8212; JoyCode</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '02-推荐系统/推荐系统04-序列特征优化方案';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="推荐系统05-多目标与多场景建模" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F05-%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%A4%9A%E5%9C%BA%E6%99%AF%E5%BB%BA%E6%A8%A1.html" />
    <link rel="prev" title="推荐系统03-双塔模型优化思路" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F03-%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">JoyCode</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    欢迎访问JoyCode
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A001-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习01-逻辑回归算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A002-K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习02-K近邻算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A003-K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习03-K-Means聚类算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A004-K-Means%2B%2B%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.html">机器学习&amp;深度学习04-K-Means++聚类算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%26%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A005-%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95.html">机器学习&amp;深度学习05-分类模型评估方法</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F01-%E6%8E%A8%E8%8D%90%26%E5%B9%BF%E5%91%8A%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E6%8C%87%E6%A0%87.html">推荐系统01-推荐&amp;广告领域常见的业务指标</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F02-%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80.html">推荐系统02-技术基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F03-%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html">推荐系统03-双塔模型优化思路</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">推荐系统04-序列特征优化方案</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F05-%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%A4%9A%E5%9C%BA%E6%99%AF%E5%BB%BA%E6%A8%A1.html">推荐系统05-多目标与多场景建模</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F06-%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89.html">推荐系统06-特征交叉</a></li>
<li class="toctree-l1"><a class="reference internal" href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F08-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">推荐系统08-图神经网络</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/yilonghao/joycode-book/issues/new?title=Issue%20on%20page%20%2F02-推荐系统/推荐系统04-序列特征优化方案.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/02-推荐系统/推荐系统04-序列特征优化方案.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>推荐系统04-序列特征优化方案</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#base-model">Base Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#din-deep-interest-network">DIN(Deep Interest Network)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#din">DIN核心思想</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-aware-regularization-mbar">mini-batch aware regularization(MBAR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dice">自适应激活函数Dice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dien-target-attention-gru">DIEN（target attention + GRU）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">模型结构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-extractor-layer">Interest Extractor Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">辅助损失函数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-evolving-layer">Interest Evolving Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attentiongru-augru">将attention机制融入GRU结构中(AUGRU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dsin">DSIN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">模型结构（整体）</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding">embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">建模用户序列</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-division-layer">session division layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer">session interest extractor layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer">Session Interest Interacting Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-activating-layer">session interest activating layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer-activation-unit">session interest extractor layer 相关 activation unit</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer-activation-unit">session interest interacting layer 相关 activation unit</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp">MLP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">实验数据</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender">京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-layer">Deep Multifaceted Transformers Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-deep-neural-network">Bias Deep Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gate-mixture-of-experts-layers">Multi-gate Mixture-of-Experts Layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#search-based-user-interest-modeling-with-lifelong-sequential-behavior-data-for-click-through-rate-prediction">阿里|Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-search-unit">General Search Unit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-search-unit">Exact Search Unit</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bst-transformer">阿里BST（Transformer处理序列特征）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">模型结构</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-layer">Transformer Layer</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-layer">Self-attention layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#point-wise-feed-forward-networks">Point-wise Feed-Forward Networks</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#dropoutlayernorm">残差连接、Dropout、LayerNorm</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-the-self-attention-blocks">Stacking the self-attention blocks</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-layers-and-loss-function">MLP layers and Loss function</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">实验数据</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dmr-attention">阿里DMR（attention）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#item-to-item-network">Item-to-Item Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#user-to-item-network">User-to-Item Network</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mind">阿里MIND</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">模型介绍</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">模型整体结构图</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">模型定义</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">模型结构</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-pooling-layer">Embedding &amp; Pooling Layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-interest-extractor-layer">Multi-Interest Extractor Layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#label-aware-attention-layer">Label-aware Attention Layer</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-serving">Training &amp; Serving</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#序列特征优化方案" data-toc-modified-id="序列特征优化方案-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>序列特征优化方案</a></span><ul class="toc-item"><li><span><a href="#Base-Model" data-toc-modified-id="Base-Model-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Base Model</a></span></li><li><span><a href="#DIN(Deep-Interest-Network)" data-toc-modified-id="DIN(Deep-Interest-Network)-1.2"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>DIN(Deep Interest Network)</a></span><ul class="toc-item"><li><span><a href="#DIN核心思想" data-toc-modified-id="DIN核心思想-1.2.1"><span class="toc-item-num">1.2.1&nbsp;&nbsp;</span>DIN核心思想</a></span></li><li><span><a href="#mini-batch-aware-regularization(MBAR)" data-toc-modified-id="mini-batch-aware-regularization(MBAR)-1.2.2"><span class="toc-item-num">1.2.2&nbsp;&nbsp;</span>mini-batch aware regularization(MBAR)</a></span></li><li><span><a href="#自适应激活函数Dice" data-toc-modified-id="自适应激活函数Dice-1.2.3"><span class="toc-item-num">1.2.3&nbsp;&nbsp;</span>自适应激活函数Dice</a></span></li></ul></li><li><span><a href="#DIEN（target-attention-+-GRU）" data-toc-modified-id="DIEN（target-attention-+-GRU）-1.3"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>DIEN（target attention + GRU）</a></span><ul class="toc-item"><li><span><a href="#模型结构" data-toc-modified-id="模型结构-1.3.1"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>模型结构</a></span></li><li><span><a href="#Interest-Extractor-Layer" data-toc-modified-id="Interest-Extractor-Layer-1.3.2"><span class="toc-item-num">1.3.2&nbsp;&nbsp;</span>Interest Extractor Layer</a></span></li><li><span><a href="#辅助损失函数" data-toc-modified-id="辅助损失函数-1.3.3"><span class="toc-item-num">1.3.3&nbsp;&nbsp;</span>辅助损失函数</a></span></li><li><span><a href="#Interest-Evolving-Layer" data-toc-modified-id="Interest-Evolving-Layer-1.3.4"><span class="toc-item-num">1.3.4&nbsp;&nbsp;</span>Interest Evolving Layer</a></span></li><li><span><a href="#将attention机制融入GRU结构中(AUGRU)" data-toc-modified-id="将attention机制融入GRU结构中(AUGRU)-1.3.5"><span class="toc-item-num">1.3.5&nbsp;&nbsp;</span>将attention机制融入GRU结构中(AUGRU)</a></span></li></ul></li><li><span><a href="#DSIN" data-toc-modified-id="DSIN-1.4"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>DSIN</a></span><ul class="toc-item"><li><span><a href="#模型结构（整体）" data-toc-modified-id="模型结构（整体）-1.4.1"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>模型结构（整体）</a></span></li><li><span><a href="#embedding" data-toc-modified-id="embedding-1.4.2"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>embedding</a></span></li><li><span><a href="#建模用户序列" data-toc-modified-id="建模用户序列-1.4.3"><span class="toc-item-num">1.4.3&nbsp;&nbsp;</span>建模用户序列</a></span><ul class="toc-item"><li><span><a href="#session-division-layer" data-toc-modified-id="session-division-layer-1.4.3.1"><span class="toc-item-num">1.4.3.1&nbsp;&nbsp;</span>session division layer</a></span></li><li><span><a href="#session-interest-extractor-layer" data-toc-modified-id="session-interest-extractor-layer-1.4.3.2"><span class="toc-item-num">1.4.3.2&nbsp;&nbsp;</span>session interest extractor layer</a></span></li><li><span><a href="#Session-Interest-Interacting-Layer" data-toc-modified-id="Session-Interest-Interacting-Layer-1.4.3.3"><span class="toc-item-num">1.4.3.3&nbsp;&nbsp;</span>Session Interest Interacting Layer</a></span></li><li><span><a href="#session-interest-activating-layer" data-toc-modified-id="session-interest-activating-layer-1.4.3.4"><span class="toc-item-num">1.4.3.4&nbsp;&nbsp;</span>session interest activating layer</a></span></li><li><span><a href="#session-interest-extractor-layer-相关-activation-unit" data-toc-modified-id="session-interest-extractor-layer-相关-activation-unit-1.4.3.5"><span class="toc-item-num">1.4.3.5&nbsp;&nbsp;</span>session interest extractor layer 相关 activation unit</a></span></li><li><span><a href="#session-interest-interacting-layer-相关-activation-unit" data-toc-modified-id="session-interest-interacting-layer-相关-activation-unit-1.4.3.6"><span class="toc-item-num">1.4.3.6&nbsp;&nbsp;</span>session interest interacting layer 相关 activation unit</a></span></li></ul></li><li><span><a href="#MLP" data-toc-modified-id="MLP-1.4.4"><span class="toc-item-num">1.4.4&nbsp;&nbsp;</span>MLP</a></span></li><li><span><a href="#实验数据" data-toc-modified-id="实验数据-1.4.5"><span class="toc-item-num">1.4.5&nbsp;&nbsp;</span>实验数据</a></span></li></ul></li><li><span><a href="#京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender" data-toc-modified-id="京东-|-Deep-Multifaceted-Transformers-for-Multi-objective-Ranking-in-Large-Scale-E-commerce-Recommender-1.5"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a></span><ul class="toc-item"><li><span><a href="#简介" data-toc-modified-id="简介-1.5.1"><span class="toc-item-num">1.5.1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href="#Deep-Multifaceted-Transformers-Layer" data-toc-modified-id="Deep-Multifaceted-Transformers-Layer-1.5.2"><span class="toc-item-num">1.5.2&nbsp;&nbsp;</span>Deep Multifaceted Transformers Layer</a></span></li><li><span><a href="#Bias-Deep-Neural-Network" data-toc-modified-id="Bias-Deep-Neural-Network-1.5.3"><span class="toc-item-num">1.5.3&nbsp;&nbsp;</span>Bias Deep Neural Network</a></span></li><li><span><a href="#Multi-gate-Mixture-of-Experts-Layers" data-toc-modified-id="Multi-gate-Mixture-of-Experts-Layers-1.5.4"><span class="toc-item-num">1.5.4&nbsp;&nbsp;</span>Multi-gate Mixture-of-Experts Layers</a></span></li></ul></li><li><span><a href="#阿里|Search-based-User-Interest-Modeling-with-Lifelong-Sequential-Behavior-Data-for-Click-Through-Rate-Prediction" data-toc-modified-id="阿里|Search-based-User-Interest-Modeling-with-Lifelong-Sequential-Behavior-Data-for-Click-Through-Rate-Prediction-1.6"><span class="toc-item-num">1.6&nbsp;&nbsp;</span>阿里|Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction</a></span><ul class="toc-item"><li><span><a href="#General-Search-Unit" data-toc-modified-id="General-Search-Unit-1.6.1"><span class="toc-item-num">1.6.1&nbsp;&nbsp;</span>General Search Unit</a></span></li><li><span><a href="#Exact-Search-Unit" data-toc-modified-id="Exact-Search-Unit-1.6.2"><span class="toc-item-num">1.6.2&nbsp;&nbsp;</span>Exact Search Unit</a></span></li></ul></li><li><span><a href="#阿里BST（Transformer处理序列特征）" data-toc-modified-id="阿里BST（Transformer处理序列特征）-1.7"><span class="toc-item-num">1.7&nbsp;&nbsp;</span>阿里BST（Transformer处理序列特征）</a></span><ul class="toc-item"><li><span><a href="#模型结构" data-toc-modified-id="模型结构-1.7.1"><span class="toc-item-num">1.7.1&nbsp;&nbsp;</span>模型结构</a></span><ul class="toc-item"><li><span><a href="#Embedding-Layer" data-toc-modified-id="Embedding-Layer-1.7.1.1"><span class="toc-item-num">1.7.1.1&nbsp;&nbsp;</span>Embedding Layer</a></span></li><li><span><a href="#Transformer-Layer" data-toc-modified-id="Transformer-Layer-1.7.1.2"><span class="toc-item-num">1.7.1.2&nbsp;&nbsp;</span>Transformer Layer</a></span></li><li><span><a href="#MLP-layers-and-Loss-function" data-toc-modified-id="MLP-layers-and-Loss-function-1.7.1.3"><span class="toc-item-num">1.7.1.3&nbsp;&nbsp;</span>MLP layers and Loss function</a></span></li></ul></li><li><span><a href="#实验数据" data-toc-modified-id="实验数据-1.7.2"><span class="toc-item-num">1.7.2&nbsp;&nbsp;</span>实验数据</a></span></li></ul></li><li><span><a href="#阿里DMR（attention）" data-toc-modified-id="阿里DMR（attention）-1.8"><span class="toc-item-num">1.8&nbsp;&nbsp;</span>阿里DMR（attention）</a></span><ul class="toc-item"><li><span><a href="#Item-to-Item-Network" data-toc-modified-id="Item-to-Item-Network-1.8.1"><span class="toc-item-num">1.8.1&nbsp;&nbsp;</span>Item-to-Item Network</a></span></li><li><span><a href="#User-to-Item-Network" data-toc-modified-id="User-to-Item-Network-1.8.2"><span class="toc-item-num">1.8.2&nbsp;&nbsp;</span>User-to-Item Network</a></span></li></ul></li><li><span><a href="#阿里MIND" data-toc-modified-id="阿里MIND-1.9"><span class="toc-item-num">1.9&nbsp;&nbsp;</span>阿里MIND</a></span><ul class="toc-item"><li><span><a href="#模型介绍" data-toc-modified-id="模型介绍-1.9.1"><span class="toc-item-num">1.9.1&nbsp;&nbsp;</span>模型介绍</a></span><ul class="toc-item"><li><span><a href="#模型整体结构图" data-toc-modified-id="模型整体结构图-1.9.1.1"><span class="toc-item-num">1.9.1.1&nbsp;&nbsp;</span>模型整体结构图</a></span></li><li><span><a href="#模型定义" data-toc-modified-id="模型定义-1.9.1.2"><span class="toc-item-num">1.9.1.2&nbsp;&nbsp;</span>模型定义</a></span></li><li><span><a href="#模型结构" data-toc-modified-id="模型结构-1.9.1.3"><span class="toc-item-num">1.9.1.3&nbsp;&nbsp;</span>模型结构</a></span></li><li><span><a href="#Training-&amp;-Serving" data-toc-modified-id="Training-&amp;-Serving-1.9.1.4"><span class="toc-item-num">1.9.1.4&nbsp;&nbsp;</span>Training &amp; Serving</a></span></li></ul></li></ul></li></ul></li></ul></div><section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>推荐系统04-序列特征优化方案<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="base-model">
<h2>Base Model<a class="headerlink" href="#base-model" title="Link to this heading">#</a></h2>
<p>传统基于用户行为序列描述用户兴趣的方法是把序列中各个item的embedding通过pooling的方式转化成一个固定维的embedding。如下图所示，其中红色节点表示商品ID，蓝色节点表示店铺iD，粉色节点表示类目ID，白色节点表示用户特征和上下文特征，Goods 1 ~ Goods N 用来描述用户的历史行为，候选广告（Candidate Ad）本身也是商品。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/dec039c0-ee95-4d4d-9246-4038e11812f2.png" alt="image" style="zoom: 33%; display: block; margin: auto;" />
<p>网络结构具有的缺点是经过pooling以后的向量与候选广告无关，对于一个用户来说是固定不变的。对于不同的候选广告，与之对应的用户兴趣分布也应该不通。因为只有用户部分的兴趣会影响当前行为（对候选广告点击或不点击）。</p>
</section>
<section id="din-deep-interest-network">
<h2>DIN(Deep Interest Network)<a class="headerlink" href="#din-deep-interest-network" title="Link to this heading">#</a></h2>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/9223919b-9c70-4403-a519-54a0b10e8b3b.png" alt="image" style="zoom: 67%; display: block; margin: auto;" />
<section id="din">
<h3>DIN核心思想<a class="headerlink" href="#din" title="Link to this heading">#</a></h3>
<p>DIN的核心思想是，对每一个用户，不同的广告有不同的向量表示，结合用户行为特征与给定的广告为每个用户行为计算权重，引入local-activation机制有侧重的利用用户不同的行为特征，其中<span class="math notranslate nohighlight">\(e_j\)</span>表示用户行为向量，<span class="math notranslate nohighlight">\(v_A\)</span>为候选广告的向量，a表示激活单元，<span class="math notranslate nohighlight">\(a(e_j,v_A)\)</span>为权重。整体逻辑为SUM Pooling。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/c2eb0b1e-f6a3-4cfd-99ad-3b8537a0963b.png" alt="image" style="zoom: 67%; display: block; margin: auto;" />
<p>相比基础的深度推荐网络，DIN在生成用户向量的时候加了一个activation unit单元，计算每个用户行为与候选广告之间的权重。在传统的attention机制中，给定两个向量，比如u和v，通常直接做点乘。这篇论文中做了进一步的改进：首先是把u和v以及uv的外积合并起来作为输入给全连接层，最后得到权重，这样可以减少信息损失。论文中还放宽了权重加和等于1的限制，这样更有利于体现用户行为特征之间的差异化程度。</p>
</section>
<section id="mini-batch-aware-regularization-mbar">
<h3>mini-batch aware regularization(MBAR)<a class="headerlink" href="#mini-batch-aware-regularization-mbar" title="Link to this heading">#</a></h3>
<p>为解决大规模稀疏场景下，采用SGD对引入L2正则的loss进行更新时计算开销过大的问题，该方法只对每一个mini-batch中参数不为0的进行梯度更新。</p>
</section>
<section id="dice">
<h3>自适应激活函数Dice<a class="headerlink" href="#dice" title="Link to this heading">#</a></h3>
<p>PRelu激活函数如下所示：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/5f30f50e-8c69-4212-8692-940457ec8b7a.png" style="zoom: 67%; display: block; margin: auto;" />
<p>采用PRelu激活函数时，它的rectified point固定为0，这在每一层的输入分布发生变化时是不适用的，所以文章对该激活函数做了改进，平滑了rectified point附近曲线的同时，激活函数会根据每层输入数据的分布来自适应调整rectified point的位置，具体形式如下：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ccac1f77-49db-4cc2-ab87-186e5637c1f2.png" style="zoom: 67%; display: block; margin: auto;" />
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/81556967-3897-407e-8496-2d98aa7bc705.png" style="zoom: 50%; display: block; margin: auto;" /></section>
</section>
<section id="dien-target-attention-gru">
<h2>DIEN（target attention + GRU）<a class="headerlink" href="#dien-target-attention-gru" title="Link to this heading">#</a></h2>
<p>阿里妈妈的精准定向检索及基础算法团队以 Deep Interest Network(DIN) 算法为基础，进一步优化升级，产出了 Deep Interest Evolution Network(DIEN)算法，主要解决了以下两个问题：</p>
<ul class="simple">
<li><p>更加精确的刻画用户的长期兴趣和短期兴趣</p></li>
<li><p>用户的兴趣是时刻变化的，需要更加准确的刻画用户兴趣的变化</p></li>
</ul>
<section id="id2">
<h3>模型结构<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>如图所示，DIEN 由 embedding 层、Interest Extractor Layer 层，Interest Evolving Layer 层组成。Interest Extractor Layer 根据行为序列提取兴趣序列，Interest Evolving Layer 基于 Target Ad 对兴趣序列进行建模，得到用户兴趣向量表示，最终将用户兴趣向量表示与其他特征向量 concat 到一起后送到 MLP 中以进行最终预测。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/1c958050-4694-4a23-8ca2-ac929d53453a.png" style="zoom:50%; display: block; margin: auto;" />
</section>
<section id="interest-extractor-layer">
<h3>Interest Extractor Layer<a class="headerlink" href="#interest-extractor-layer" title="Link to this heading">#</a></h3>
<p>兴趣提取层的目的就是从用户的行为序列中提取出一系列的兴趣状态。为了平衡效率和性能，作者选择了GRU（Gated Recurrent Unit，门循环单元）网络来对用户行为之间的依赖进行建模。GRU既能够克服RNN梯度消失的问题，同时又比LSTM网络具有更少的参数，训练时收敛速度更快。GRU单元的表达式如下：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/225b086d-f739-42b0-bcb6-b5a1d6a2cbae.png" style="zoom: 33%; display: block; margin: auto;" />
<p>其中 <span class="math notranslate nohighlight">\(\sigma\)</span>表示sigmoid激活函数，<span class="math notranslate nohighlight">\(\circ\)</span>表示元素积(element-wise product)，<span class="math notranslate nohighlight">\(W\)</span>和<span class="math notranslate nohighlight">\(U\)</span>表示隐藏层参数，<span class="math notranslate nohighlight">\(i_t\)</span>表示GRU单元的输入（用户第t个行为的embedding向量），<span class="math notranslate nohighlight">\(h_{t}\)</span>表示第t个GRU单元的隐层状态。关于GRU结构的介绍可以参考文章：<a class="reference external" href="https://zhuanlan.zhihu.com/p/463521382">GRU学习笔记</a>，主结构如下：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6ee03222-480d-457c-9509-a6270d8a4223.png" style="zoom: 33%; display: block; margin: auto;" />
</section>
<section id="id3">
<h3>辅助损失函数<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a3592fc6-81c0-4461-b6fd-e98f90109bb6.png" style="zoom:50%; display: block; margin: auto;" />
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/44c87c81-7b82-4438-9b8c-3fddbdd69173.png" alt="辅助Loss计算示意图" style="zoom:50%; display: block; margin: auto;" />
<p>其中 <span class="math notranslate nohighlight">\(\{e_{b}^{i},\hat e_{b}^{i}\} \in D_{\beta},i=1,2,3,...,N\)</span>表示N对行为embedding序列，<span class="math notranslate nohighlight">\(e_{b}^{i} \in \mathbb{R}^{T  \times n_E}\)</span>表示用户的点击行为序列，<span class="math notranslate nohighlight">\(\hat e_{b}^{i}\in \mathbb{R}^{T  \times n_E}\)</span>表示用户没有点击的行为序列。<span class="math notranslate nohighlight">\(T\)</span>表示序列中历史行为的数量，<span class="math notranslate nohighlight">\(n_E\)</span>表示embedding向量的维度。<span class="math notranslate nohighlight">\(e_{b}^{i}[t]\)</span>表示第i个用户第t次点击行为的embedding向量。其中：</p>
<div class="math notranslate nohighlight">
\[
\sigma(x_1, x_2) = \frac{1}{exp(-[x_1, x_2])}
\]</div>
<p>DIEN使用的整体损失函数是：</p>
<div class="math notranslate nohighlight">
\[
L = L_{target} + \alpha * L_{aux}
\]</div>
</section>
<section id="interest-evolving-layer">
<h3>Interest Evolving Layer<a class="headerlink" href="#interest-evolving-layer" title="Link to this heading">#</a></h3>
<p>兴趣进化层Interest Evolution Layer的主要目标是刻画用户兴趣的进化过程。兴趣进化层的数据就是兴趣提取层的输出，兴趣进化层结合了注意力机制中的局部激活能力和GRU的序列学习能力来建模。attention部分系数计算方式如下：</p>
<div class="math notranslate nohighlight">
\[
a_t = \frac{exp(h_{t}W_{e_{a}})}{\sum_{j=1}^Texp(h_{j}W_{e_{a}})}
\]</div>
</section>
<section id="attentiongru-augru">
<h3>将attention机制融入GRU结构中(AUGRU)<a class="headerlink" href="#attentiongru-augru" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/71b8130c-bdf8-4cdf-86f4-e5eccfcdd689.png" style="zoom:50%; display: block; margin: auto;" />
<p>通过将attention部分的计算结果作用到GRU结构的更新门上将attention机制融入到GRU结构中。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/664e6fa8-e51d-4fc0-8077-3c26c81319f6.png" style="zoom: 33%; display: block; margin: auto;" /></section>
</section>
<section id="dsin">
<h2>DSIN<a class="headerlink" href="#dsin" title="Link to this heading">#</a></h2>
<p>这篇文章主要介绍阿里在2019年发表的排序阶段模型：Deep Session Interest Network for Click-Through Rate Prediction。主要思路为将用户的历史点击行为划分为不同session，而后通过Transformer结构学习每个session的向量表示，最后通过BiLSTM结构对session序列进行建模，整体来说具有一定的参考意义。</p>
<section id="id4">
<h3>模型结构（整体）<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/11a806f7-3775-44b0-83db-77a199ef130d.png" style="zoom:50%; display: block; margin: auto;" />
</section>
<section id="embedding">
<h3>embedding<a class="headerlink" href="#embedding" title="Link to this heading">#</a></h3>
<p>从图中可以看出模型主要由三大部分组成，其中第一部分（part1）主要处理用户画像特征（年龄、性别、所在城市等）和item侧特征（seller id, brand id等），处理方式也很简单，所有特征通过embedding的方式得到对应的向量表示，最终用户侧特征向量表示为<span class="math notranslate nohighlight">\(X^U \in \mathbb{R}^{N_u \times d_{model}}\)</span>，其中<span class="math notranslate nohighlight">\(N_u\)</span>表示用户侧特征个数，item侧特征的向量表示为：<span class="math notranslate nohighlight">\(X^I \in \mathbb{R}^{N_i \times d_{model}}\)</span>，其中<span class="math notranslate nohighlight">\(N_i\)</span>表示item侧特征个数。</p>
</section>
<section id="id5">
<h3>建模用户序列<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>这一部分（part2）主要用来建模用户行为序列，主要包含4部分：</p>
<ul class="simple">
<li><p>session division layer: 对用户行为序列进行划分</p></li>
<li><p>session interest extractor layer: 学习每个session的向量表示</p></li>
<li><p>session interest interacting layer: 对session序列进行建模</p></li>
<li><p>session interest activating layer: 引入attention机制</p></li>
</ul>
<section id="session-division-layer">
<h4>session division layer<a class="headerlink" href="#session-division-layer" title="Link to this heading">#</a></h4>
<p>主要作用就是将用户的历史点击行为序列划分为多个sessions。整个操作的符号表示为将用户的行为序列<span class="math notranslate nohighlight">\(S\)</span>划分为sessions <span class="math notranslate nohighlight">\(Q\)</span>，第<span class="math notranslate nohighlight">\(k\)</span>个session的表示形式为：</p>
<div class="math notranslate nohighlight">
\[
Q_k = [b_1; ...; bi; ...; b_T] \in \mathbb{R}^{T \times d_{model}}
\]</div>
<p>其中<span class="math notranslate nohighlight">\(T\)</span>为当前session对应的序列长度，<span class="math notranslate nohighlight">\(b_i\)</span>为用户在当前session中的第<span class="math notranslate nohighlight">\(i\)</span>次点击行为，文中session的划分是按照时间间隔来的，两次点击之间的间隔超过30min则将下一次点击行为计入下一个session。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/23182b90-adff-4fb1-8841-b0b18e78a747.jpg" style="zoom:50%; display: block; margin: auto;" />
</section>
<section id="session-interest-extractor-layer">
<h4>session interest extractor layer<a class="headerlink" href="#session-interest-extractor-layer" title="Link to this heading">#</a></h4>
<p>这部分的主要作用就是学习每个session的向量表示，文中使用了multi-head self-attention的结构对每个session建模。为了刻画不同session间的顺序，DSIN使用了Bias Encoding，其中<span class="math notranslate nohighlight">\(BE \in \mathbb{R}^{K \times T \times d_{model}}\)</span>。Bias Encoding 计算方式如下：</p>
<div class="math notranslate nohighlight">
\[
BE_{(k,t,c)} = w_k^K + w_t^T + w_c^C
\]</div>
<p>其中<span class="math notranslate nohighlight">\(BE_{(k,t,c)}\)</span>表示第k个session中的第t个物品的embedding向量中的第c个位置对应的bias，加入bias encoding后，用户的session表示为：</p>
<div class="math notranslate nohighlight">
\[
Q=Q+BE
\]</div>
<p>Multi-head Self-attention的计算逻辑为：
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0a020f15-557f-48a9-b49f-cd177c1af371.png" style="zoom: 50%; display: block; margin: auto;" /></p>
<div class="math notranslate nohighlight">
\[
I_k^Q = FFN(Concat(head_1, ..., head_H)W^O)
\]</div>
<div class="math notranslate nohighlight">
\[
I_k = Avg(I_k^Q)
\]</div>
</section>
<section id="session-interest-interacting-layer">
<h4>Session Interest Interacting Layer<a class="headerlink" href="#session-interest-interacting-layer" title="Link to this heading">#</a></h4>
<p>这部分主要通过Bi-LSTM结构对session序列进行建模。</p>
<div class="math notranslate nohighlight">
\[
H_t = \mathop{h_{ft}}\limits ^{\rightarrow} \oplus \mathop{h_{bt}}\limits ^{\leftarrow}
\]</div>
<p>其中<span class="math notranslate nohighlight">\(\mathop{h_{ft}}\limits ^{\rightarrow}\)</span>表示forward隐藏层状态，<span class="math notranslate nohighlight">\(\mathop{h_{bt}}\limits ^{\leftarrow}\)</span>表示backward隐藏层状态。</p>
</section>
<section id="session-interest-activating-layer">
<h4>session interest activating layer<a class="headerlink" href="#session-interest-activating-layer" title="Link to this heading">#</a></h4>
<p>这部分主要是通过attention机制刻画目标item和session之间的相关性。主要思想是若session与目标item之间的相关性越高，则应该赋予越大的权重。模型结构中共有两个Activation Unit，结构是一致的。</p>
</section>
<section id="session-interest-extractor-layer-activation-unit">
<h4>session interest extractor layer 相关 activation unit<a class="headerlink" href="#session-interest-extractor-layer-activation-unit" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/0f1a1c33-3507-4375-8fb3-8a7640b1d777.png" style="zoom: 33%; display: block; margin: auto;" />
</section>
<section id="session-interest-interacting-layer-activation-unit">
<h4>session interest interacting layer 相关 activation unit<a class="headerlink" href="#session-interest-interacting-layer-activation-unit" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a5717872-e0b2-4240-8d6a-80b5d51b4183.png" style="zoom: 33%; display: block; margin: auto;" />
</section>
</section>
<section id="mlp">
<h3>MLP<a class="headerlink" href="#mlp" title="Link to this heading">#</a></h3>
<p>这一部分（part3）为简单的MLP结构，首先将得到的所有向量表示concat到一起，而后通过2层前馈神经网络，最后通过一个softmax层得到输出。损失函数为：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/4e742bb1-d068-47c4-93c3-46351e119807.png" style="zoom:33%; display: block; margin: auto;" />
<p>其中 <span class="math notranslate nohighlight">\(\mathbb{D}\)</span>表示训练集，<span class="math notranslate nohighlight">\(y \in \{0, 1\}\)</span>，<span class="math notranslate nohighlight">\(p(\cdot)\)</span>表示网络最终的输出结果，表示用户点击目标item的概率。</p>
</section>
<section id="id6">
<h3>实验数据<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>从实验结果来看，AUC相比其他模型均有一定幅度的提升。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/f67a91a0-f19b-47b6-95ba-a1ae66ae1a0c.png" style="zoom:50%; display: block; margin: auto;" /></section>
</section>
<section id="deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender">
<h2>京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender<a class="headerlink" href="#deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>推荐阅读⭐️⭐️⭐️⭐️⭐️</p>
<ul class="simple">
<li><p>paper: Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</p></li>
<li><p><a class="reference external" href="https://blog.csdn.net/whgyxy/article/details/126023948">论文《Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender》</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/534414021">ctr预估：DMT模型</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/55752344">详解谷歌之多任务学习模型MMoE(KDD 2018)</a></p></li>
</ul>
</div></blockquote>
<section id="id7">
<h3>简介<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p>模型应用的业务场景商品搜索排序阶段，论文提出用多个Transfomer对用户多种类型的行为序列进行建模，在此基础上叠加MMOE建模多目标，最后使用一个消偏塔对数据进行消偏。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221231142523288-2501956.png" style="zoom: 67%; display: block; margin: auto;" />
</section>
<section id="deep-multifaceted-transformers-layer">
<h3>Deep Multifaceted Transformers Layer<a class="headerlink" href="#deep-multifaceted-transformers-layer" title="Link to this heading">#</a></h3>
<p>Embedding Layer对每个物料使用物料id、类目id、品牌id、商铺id分别映射成低维向量，然后concat起来，形成向量；Dense特征使用了Z-score归一化。模型使用了点击，加购，成交3个Item Sequence，分别表征短期，中期和长期兴趣；分别用3个Transformer来对点击、加入购物车、购买行为序列进行建模：</p>
<ul class="simple">
<li><p>encoder：用序列的item-embedding作为self-attention的输入</p></li>
<li><p>decoder：使用target item的embedding作为query，encoder输出的结果作为key和value。</p></li>
</ul>
</section>
<section id="bias-deep-neural-network">
<h3>Bias Deep Neural Network<a class="headerlink" href="#bias-deep-neural-network" title="Link to this heading">#</a></h3>
<p>Bais塔的输入都是偏差相关的特征，对于位置偏差输入就是展示位置索引编号或者网页索引编号；对于近邻偏差，输入就是目标物料的类目和邻近K个物料的类目。Bias建模部分使用Bias特征+MLP，输出的Logits与主网络Logits相加。</p>
</section>
<section id="multi-gate-mixture-of-experts-layers">
<h3>Multi-gate Mixture-of-Experts Layers<a class="headerlink" href="#multi-gate-mixture-of-experts-layers" title="Link to this heading">#</a></h3>
<p>多任务建模部分使用MMoE结构。</p>
</section>
</section>
<section id="search-based-user-interest-modeling-with-lifelong-sequential-behavior-data-for-click-through-rate-prediction">
<h2>阿里|Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction<a class="headerlink" href="#search-based-user-interest-modeling-with-lifelong-sequential-behavior-data-for-click-through-rate-prediction" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>推荐阅读⭐️⭐️⭐️⭐️⭐️</p>
<ul class="simple">
<li><p>paper: Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction</p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/370951728">SIM: 基于搜索的超长行为序列上的用户兴趣建模</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/216123531">学习笔记：MIMN与SIM-长用户行为序列上的处理方法</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/188755987">阿里妈妈基于检索的用户行为兴趣CTR模型——SIM</a></p></li>
</ul>
</div></blockquote>
<p>像DIN、DIEN这些模型，都使用了Target Attention，这样可以对不相关的行为信息做一下过滤。但是这种方式在处理超长行为序列时的计算延时(latency)是不能接受的。</p>
<p>针对超长行为序列两步走：</p>
<ul class="simple">
<li><p><strong>General Search Unit(GSU)</strong>：在用户行为序列中进行初筛，得到相关的item，压缩用户行为序列长度，初筛的方式又分两种：</p>
<ul>
<li><p>soft search: 用候选item的embedding去和用户行为序列中的每一项的embedding去做点积，然后取TopK，这里可以用ALSH等比较高效的方法。</p></li>
<li><p>hard search: 标签匹配的方式，比如选出用户行为序列中与target item同类别的行为。</p></li>
</ul>
</li>
<li><p><strong>Exact Search Unit(ESU)</strong>：对筛选后的用户行为序列建模。</p></li>
</ul>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20230101161905273.png" style="zoom:50%; display: block; margin: auto;" /><section id="general-search-unit">
<h3>General Search Unit<a class="headerlink" href="#general-search-unit" title="Link to this heading">#</a></h3>
<p>在做soft search操作时，采用的是另外一套embedding，和MLP的embedding不一样，主要是考虑到分布不一样。训练时，添加辅助loss：</p>
<div class="math notranslate nohighlight">
\[
Loss = \alpha Loss_{GSU} + \beta Loss_{ESU}
\]</div>
<p>使用hard serach这种方式时可以认为就是标签匹配，无参，定义好筛选规则即可。</p>
</section>
<section id="exact-search-unit">
<h3>Exact Search Unit<a class="headerlink" href="#exact-search-unit" title="Link to this heading">#</a></h3>
<p>和传统的DIN思路一致，这里使用的是multi head attention。</p>
</section>
</section>
<section id="bst-transformer">
<h2>阿里BST（Transformer处理序列特征）<a class="headerlink" href="#bst-transformer" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p>《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》</p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/338817680%E3%80%82">Transformer模型详解（图解最完整版）</a></p></li>
</ol>
</div></blockquote>
<p>阿里搜索团队在2019年发布的文章《Behavior Sequence Transformer for E-commerce Recommendation in Alibaba》中提出了BST模型，主要思路是利用Transformer结构来捕获用户行为序列中的序列信息。与WDL(wide and deep learning networks)和DIN(Deep Interest networks)模型相比：</p>
<ul class="simple">
<li><p>离线评估阶段AUC有明显的提升</p></li>
<li><p>线上A/B实验，CTR指标也有较大幅度提升</p></li>
<li><p>系统性能开销有所增加，平均响应时长指标上涨</p></li>
</ul>
<p>最终模型被实际部署到线上精排阶段，文章为序列特征的处理提供了新的思路，值得学习。</p>
<section id="id8">
<h3>模型结构<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/e651be9e-b64f-4d92-a9f5-caa02871727b.png" style="zoom:50%; display: block; margin: auto;" />
<p>模型主要分为三大部分：Embedding Layer、Transformer Layer、MLP Layers and Loss function。</p>
<section id="embedding-layer">
<h4>Embedding Layer<a class="headerlink" href="#embedding-layer" title="Link to this heading">#</a></h4>
<p>embedding层主要作用就是把输入的每个特征都转换为低维向量，文中的特征主要包括<strong>用户行为序列特征</strong>和<strong>其他特征</strong>。其中其他特征主要包括：用户画像特征、item相关特征、上下文特征、交叉特征，如下表所示：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/718a7f24-7e33-4b65-8c5c-ea15fbb7f7a2.png" style="zoom:50%; display: block; margin: auto;" />
<p>所有其他特征通过embedding层后，得到对应的低维向量表示，将所有的低维向量concat到一起。</p>
<p>用户行为序列特征主要包含两部分，一个是item相关的特征（Sequence Item Features），一个是位置特征（Positional Features）。</p>
<ul class="simple">
<li><p>item相关的特征主要包含item_id和category_id（当然也可以考虑使用其他item相关的特征，文中说明在其业务场景下使用item_id和category_id即可取得足够好的效果）。</p></li>
<li><p>位置特征主要用来刻画用户历史行为序列中的顺序信息，文中通过自定义的计算方式<span class="math notranslate nohighlight">\(pos(v_i)=t(v_t)-t(v_i)\)</span>计算每个item的位置，其中<span class="math notranslate nohighlight">\(t(v_t)\)</span>表示推荐的时间戳，<span class="math notranslate nohighlight">\(t(v_i)\)</span>表示用户点击商品<span class="math notranslate nohighlight">\(v_i\)</span>的时间戳，然后将item的位置通过embedding层投射为低维向量。</p></li>
</ul>
</section>
<section id="transformer-layer">
<h4>Transformer Layer<a class="headerlink" href="#transformer-layer" title="Link to this heading">#</a></h4>
<p>Transformer Layer 主要包含三个部分：Self-attention layer，Point-wise Feed-Forward Networks和Stacking the self-attention blocks。</p>
<section id="self-attention-layer">
<h5>Self-attention layer<a class="headerlink" href="#self-attention-layer" title="Link to this heading">#</a></h5>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/9cea67bc-dc34-40fb-abd5-bd751bf956ca.png" style="zoom:30%; display: block; margin: auto;" />
<p>self-attention主要结构如上图所示，计算时需要用到Q(查询)、K(键值)、V(值)矩阵。self-attention接收的输入是embedding层的输出矩阵<span class="math notranslate nohighlight">\(E\)</span>，<span class="math notranslate nohighlight">\(W^Q, W^K, W^V \in \mathbb{R} ^ {d \times d}\)</span>，<span class="math notranslate nohighlight">\(Q、K、V\)</span>的计算方式如下：</p>
<div class="math notranslate nohighlight">
\[
Q=EW^Q
\]</div>
<div class="math notranslate nohighlight">
\[
K=EW^K
\]</div>
<div class="math notranslate nohighlight">
\[
V=EW^V
\]</div>
<p>attention计算方式为：</p>
<div class="math notranslate nohighlight">
\[
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d}})V
\]</div>
<p>multi-head attention计算方式为：</p>
<div class="math notranslate nohighlight">
\[
S=MH(E)=Concate(head_1, head_2, ..., head_h)W^H
\]</div>
<div class="math notranslate nohighlight">
\[
head_i = Attention(EW^Q, EW^K, EW^V)
\]</div>
<p>因此，self-attention的最终输出为矩阵S（实际还会做进一步处理，后文会进一步说明）。</p>
</section>
<section id="point-wise-feed-forward-networks">
<h5>Point-wise Feed-Forward Networks<a class="headerlink" href="#point-wise-feed-forward-networks" title="Link to this heading">#</a></h5>
<p>BST模型也通过添加FFN网络来增加模型非线性，简单形式为：</p>
<div class="math notranslate nohighlight">
\[
F=FFN(S)
\]</div>
<p>本文中的FFN其实就是一个简单的二层网络，其中第一层的激活函数为为LeakyRelu，对于输入序列中每个位置上的向量x：</p>
<div class="math notranslate nohighlight">
\[
FFN(x) = LeakyRelu(xW^{(1)} + b^{(1)})W^{(2)}+b^{(2)}
\]</div>
</section>
<section id="dropoutlayernorm">
<h5>残差连接、Dropout、LayerNorm<a class="headerlink" href="#dropoutlayernorm" title="Link to this heading">#</a></h5>
<p>在Self-attention layer和Point-wise Feed-Forward Networks中均能看到残差连接、Dropout和LayerNorm的影子，其中：</p>
<ul class="simple">
<li><p>残差连接主要是为了解决多层网络训练的问题</p></li>
<li><p>Dropout主要是为了防止模型过拟合</p></li>
<li><p>LayerNorm(Layer Normalization)通常用于RNN结构，会将每一层神经元的输入都转成均值方差都一样的，可以加快模型收敛</p></li>
</ul>
<p><strong>实际上self-attention的最终输出为：</strong>
$<span class="math notranslate nohighlight">\(
S' = LayerNorm(S+Dropout(S))
\)</span>$</p>
<p><strong>Feed-Forward Networks(FFN)的最终形式为：</strong></p>
<div class="math notranslate nohighlight">
\[
F=LayerNorm(S' + Dropout(LeakyRelu(S'W^{(1)} + b^{(1)})W^{(2)}+b^{(2)}))
\]</div>
</section>
<section id="stacking-the-self-attention-blocks">
<h5>Stacking the self-attention blocks<a class="headerlink" href="#stacking-the-self-attention-blocks" title="Link to this heading">#</a></h5>
<p>这一部分即上文介绍的self-attention + FFN的堆叠结构（本文中尝试过1~3层的堆叠结构，单层结构效果是最好的）。</p>
<div class="math notranslate nohighlight">
\[
S^b = SA(F^{(b-1)})
\]</div>
<div class="math notranslate nohighlight">
\[
F^b = FFN(S^b), \forall_i \in 1,2, ..., n.
\]</div>
</section>
</section>
<section id="mlp-layers-and-loss-function">
<h4>MLP layers and Loss function<a class="headerlink" href="#mlp-layers-and-loss-function" title="Link to this heading">#</a></h4>
<p>将所有的embedding进行拼接，输入到三层的神经网络中，并最终通过sigmoid函数转换为0-1之间的值，代表用户点击目标商品的概率。loss函数为：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/82fed813-2ed5-4c65-bad0-4ed01ae9b3b6.png" style="zoom: 36%; display: block; margin: auto;" />
</section>
</section>
<section id="id9">
<h3>实验数据<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/ba1c19d6-5cb5-47e8-a9d9-624458d28f15.png" style="zoom:36%; display: block; margin: auto;" />
<p>可以看到BST模型与其他模型相比离线AUC评估指标有大幅度的提升，线上A/B实验，点击率也有大幅度的提升，但模型性能开销也有相应的提升（平均响应时长有一定程度的上涨）。</p>
</section>
</section>
<section id="dmr-attention">
<h2>阿里DMR（attention）<a class="headerlink" href="#dmr-attention" title="Link to this heading">#</a></h2>
<blockquote>
<div><p><strong>推荐阅读</strong>⭐️⭐️⭐️⭐️⭐️</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/163096847">阿里DMR：融合了匹配思想的深度排序模型-Deep Match to Rank</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/418447193">阿里巴巴DMR模型(Deep Match to Rank)</a></p></li>
<li><p><a class="reference external" href="https://developer.aliyun.com/article/898525">推荐系统序列化建模总结（二）</a></p></li>
</ol>
</div></blockquote>
<p>DIN等模型将学习到的Sequence Embedding（用户兴趣向量）、User Profile、待排序物品特征等Concat后送入最上层的MLP进行特征交叉最终输出一个CTR预估分数，作者认为在Concat特征送入MLP进行交叉前就计算一个User和Item相关性可以降低模型的学习难度。</p>
<p>DMR可以看做是另一种对DIN的改进方式。文章提出了两种网络结构，Item-to-Item网络和User-to-Item网络，来描述用户和候选目标item是否匹配。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20230101003506412-2545203.png" alt="image-20230101003506412" style="zoom:50%; display: block; margin: auto;" />
<section id="item-to-item-network">
<h3>Item-to-Item Network<a class="headerlink" href="#item-to-item-network" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(e_i\)</span>是用户行为序列中每个item的embedding，<span class="math notranslate nohighlight">\(p_i\)</span>是用户行为在序列中的位置embedding，文章通过融合行为embedding、位置embedding和target item embedding计算attention权重。</p>
<div class="math notranslate nohighlight">
\[
\hat a_t = \hat z^T tanh(\hat W_c e_c + \hat W_p p_t + \hat W_e e_t + \hat b)
\]</div>
<p>通过softmax得到最终权重，将权重作用到每个行为embedding上，通过sum pooling的方式得到用户行为序列的向量表示。</p>
<div class="math notranslate nohighlight">
\[
\hat \alpha_t = \frac {exp(\hat a_t)}{\sum_{i=1}^T exp(\hat a_i)}
\]</div>
<div class="math notranslate nohighlight">
\[
\hat u = \sum_{t=1}^T(\hat \alpha_t \hat e_t)
\]</div>
<p>此外item-to-item network子网络的输出还包括target item embedding，以及sofatmax之前各个行为embedding的attention权重和。</p>
</section>
<section id="user-to-item-network">
<h3>User-to-Item Network<a class="headerlink" href="#user-to-item-network" title="Link to this heading">#</a></h3>
<p>在计算attention的时候并不考虑target item。</p>
<p>一般来说，用户的兴趣会随着时间发生变化，距离现在更近的行为更能反映用户的真实兴趣。根据用户行为发生的时间为用户行为指定权重可以解决这个问题，采用attention机制，把用户行为出现的位置按时间排序后，用数字编码，当做query（<strong>这里也可以根据具体业务替换为其他重要的特征</strong>），自主地为每个行为计算attention权重。</p>
<div class="math notranslate nohighlight">
\[
a_t = z^T tanh(W_p p_t + W_e e_t + b)
\]</div>
<p>通过softmax得到最终的attention权重：</p>
<div class="math notranslate nohighlight">
\[
\alpha_t = \frac {exp(a_t)}{\sum_{i=1}^T exp(a_i)}
\]</div>
<p>将权重作用到每个行为embedding上，最后再通过sum pooling + 非线性变换的方式得到用户行为序列的向量表示。</p>
<div class="math notranslate nohighlight">
\[
u = g(\sum_{t=1}^T(\alpha_t e_t)) = g(\sum_{t=1}^T(h_t))
\]</div>
<p>最后跟target item embedding 做内积运算来表示用户和目标商品的匹配程度，最终输入到MLP网络(同样的问题，这种单值特征的作用大么？)。</p>
<div class="math notranslate nohighlight">
\[
r = u^T v'
\]</div>
<p>引入一个辅助训练网络来帮助训练，确保点积结果越大代表用户和target item的相关性越高。在辅助训练的时候，使用行为序列中的前T-1个行为学习用户表示，而用最后一个行为作为正样本，使用负采样的方法随机获得负样本。<span class="math notranslate nohighlight">\(p_j\)</span>表示匹配分值，<span class="math notranslate nohighlight">\(L_{NS}\)</span>表示辅助网络loss，<span class="math notranslate nohighlight">\(L_{final}\)</span>表示最终
loss。</p>
<div class="math notranslate nohighlight">
\[
p_j = \frac {exp(u_{T-1}^T v'_j)}{\sum_{i=1}^K exp(u_{T-1}^T v'_i)}
\]</div>
<div class="math notranslate nohighlight">
\[
L_{aux} = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^K y_j^i log(p_j^i)
\]</div>
<div class="math notranslate nohighlight">
\[
L_{NS} = - \frac {1}{N} \sum_{i=1}^N log(\sigma(u_{T-1}^T v'_o)) + \sum_{j=1}^k log(\sigma(-u_{T-1}^T v'_j))
\]</div>
<div class="math notranslate nohighlight">
\[
L_{final} = L_{target} + \beta L_{NS}
\]</div>
</section>
</section>
<section id="mind">
<h2>阿里MIND<a class="headerlink" href="#mind" title="Link to this heading">#</a></h2>
<p>推荐系统一般包含两个重要阶段：召回和排序，召回阶段负责从海量候选中选出用户感兴趣的候选集，排序阶段再对用户感兴趣的候选集进行排序取出TOPN。在这两个阶段一个很重要的因素就是如何建模表征用户兴趣，目前大部分模型都以单向量表征用户，MIND模型提出了一种用多个向量表征用户的思路。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/291d21a2-21fb-4d98-ba9e-966a11e9e2e0.png" style="zoom:50%; display: block; margin: auto;" />
<p>模型的核心思想：</p>
<ul class="simple">
<li><p>利用multi-interest提取层（以胶囊网络和动态路由算法为基础）结合用户历史行为，提取用户的多元兴趣。</p></li>
<li><p>通过label-aware attention机制帮助学习用户的多元向量表示。</p></li>
</ul>
<section id="id10">
<h3>模型介绍<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<section id="id11">
<h4>模型整体结构图<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d726d977-840a-47b1-b002-75d4a55fe964.png" style="zoom:50%; display: block; margin: auto;" />
</section>
<section id="id12">
<h4>模型定义<a class="headerlink" href="#id12" title="Link to this heading">#</a></h4>
<p>首先介绍一下三元组<span class="math notranslate nohighlight">\((I_u, P_u, F_i)\)</span>中各个组成部分的含义，<span class="math notranslate nohighlight">\(I_u\)</span>表示与用户交互过的商品（用户历史行为），<span class="math notranslate nohighlight">\(P_u\)</span>表示用户画像特征（例如年龄、性别等），<span class="math notranslate nohighlight">\(F_i\)</span>表示商品的特征（例如商品id、一级分类等）。</p>
<p>MIND的核心是学习以下函数表达：</p>
<div class="math notranslate nohighlight">
\[
V_u = f_{user}(I_u, P_u)
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(V_u\)</span> 是 <span class="math notranslate nohighlight">\(d \times K\)</span> 维的矩阵，d表示每个向量的维度，K表示向量个数。</p>
<p>目标商品的向量表示可以定义为：</p>
<div class="math notranslate nohighlight">
\[
\vec{e_i} = f_{item}(F_i)
\]</div>
<p>其中<span class="math notranslate nohighlight">\({e_i}\)</span>为d维向量。</p>
<p>模型候选打分计算公式：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/8db57b6c-31bc-4849-bb54-1d94980ec949.png" style="zoom:50%; display: block; margin: auto;" />
<p>即用户的每个向量与候选物料计算打分取最大值作为模型最终打分。</p>
</section>
<section id="id13">
<h4>模型结构<a class="headerlink" href="#id13" title="Link to this heading">#</a></h4>
<p>模型主要分为以下四部分：</p>
<ul class="simple">
<li><p>Embedding &amp; Pooling Layer</p></li>
<li><p>Multi-Interest Extractor Layer</p></li>
<li><p>Label-aware Attention Layer</p></li>
<li><p>Training &amp; Serving</p></li>
</ul>
<section id="embedding-pooling-layer">
<h5>Embedding &amp; Pooling Layer<a class="headerlink" href="#embedding-pooling-layer" title="Link to this heading">#</a></h5>
<p>这一层逻辑比较简单，<span class="math notranslate nohighlight">\(I_u\)</span>、<span class="math notranslate nohighlight">\(P_u\)</span>、<span class="math notranslate nohighlight">\(F_i\)</span>均通过embedding层转化为embedding向量，而后通过average pooling得到最终的向量表示。</p>
</section>
<section id="multi-interest-extractor-layer">
<h5>Multi-Interest Extractor Layer<a class="headerlink" href="#multi-interest-extractor-layer" title="Link to this heading">#</a></h5>
<p>这一层通过胶囊网络学习用户的多个兴趣向量表示，来表示用户的多种兴趣。这一层涉及到胶囊网络和动态路由算法，会在其他文章中详细展开。</p>
</section>
<section id="label-aware-attention-layer">
<h5>Label-aware Attention Layer<a class="headerlink" href="#label-aware-attention-layer" title="Link to this heading">#</a></h5>
<p>这一部分的原理其实也比较简单，这一层的输入即为通过Multi-Interest Extractor Layer提取到的多个用户兴趣向量（兴趣胶囊），每个用户兴趣向量表示不同的用户兴趣，通过Label-aware注意力机制确定通过这些用户兴趣向量计算最终的用户向量<span class="math notranslate nohighlight">\(\vec{v_{u}}\)</span>时各自的权重。</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/dc31472b-babe-411e-8aba-6de244da3a81.png" style="zoom:50%; display: block; margin: auto;" />
<p>其中p为调节参数：</p>
<ul class="simple">
<li><p>当p为0时，那么每个用户兴趣向量有相同的权重</p></li>
<li><p>当p&gt;1时，p越大，与目标商品向量点积更大的用户兴趣向量会有更大的权重</p></li>
<li><p>当p为无穷大时，实际上就相当于只使用与目标商品向量点积最大的用户兴趣向量，忽略其他用户向量，可以理解是每次只激活一个兴趣向量，文章指出使用这种方法收敛最快。</p></li>
</ul>
</section>
</section>
<section id="training-serving">
<h4>Training &amp; Serving<a class="headerlink" href="#training-serving" title="Link to this heading">#</a></h4>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/a4f337df-f794-4099-87bc-23bfcd7acd40.png" style="zoom:33%; display: block; margin: auto;" />
<p>得到用户向量<span class="math notranslate nohighlight">\(\vec{v_{u}}\)</span>和商品向量<span class="math notranslate nohighlight">\(\vec{e_{i}}\)</span>之后，通过下式可以计算得到用户对商品的交互概率：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d0ed74d8-2a9c-4b0e-9432-2149568e9951.png" style="zoom: 33%; display: block; margin: auto;" />
<p>模型的目标函数：</p>
<img src="https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/8b0c7794-2ebd-44cc-a85d-de920b7587b2.png" style="zoom:33%; display: block; margin: auto;" />
<p>模型采用了sampled softmax的方式降低开销。</p>
<p>在Serving阶段，算出每个用户的多个向量后，每个向量都可用于为用户召回商品，最终从所有召回结果中挑选出TOPN。</p>
<p>整个过程可以参考Youtube DNN。</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./02-推荐系统"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F03-%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%80%9D%E8%B7%AF.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">推荐系统03-双塔模型优化思路</p>
      </div>
    </a>
    <a class="right-next"
       href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F05-%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%B8%8E%E5%A4%9A%E5%9C%BA%E6%99%AF%E5%BB%BA%E6%A8%A1.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">推荐系统05-多目标与多场景建模</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#base-model">Base Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#din-deep-interest-network">DIN(Deep Interest Network)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#din">DIN核心思想</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-aware-regularization-mbar">mini-batch aware regularization(MBAR)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dice">自适应激活函数Dice</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dien-target-attention-gru">DIEN（target attention + GRU）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">模型结构</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-extractor-layer">Interest Extractor Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">辅助损失函数</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interest-evolving-layer">Interest Evolving Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attentiongru-augru">将attention机制融入GRU结构中(AUGRU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dsin">DSIN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">模型结构（整体）</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding">embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">建模用户序列</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-division-layer">session division layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer">session interest extractor layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer">Session Interest Interacting Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-activating-layer">session interest activating layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-extractor-layer-activation-unit">session interest extractor layer 相关 activation unit</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#session-interest-interacting-layer-activation-unit">session interest interacting layer 相关 activation unit</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp">MLP</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">实验数据</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-for-multi-objective-ranking-in-large-scale-e-commerce-recommender">京东 | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">简介</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-multifaceted-transformers-layer">Deep Multifaceted Transformers Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-deep-neural-network">Bias Deep Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gate-mixture-of-experts-layers">Multi-gate Mixture-of-Experts Layers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#search-based-user-interest-modeling-with-lifelong-sequential-behavior-data-for-click-through-rate-prediction">阿里|Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-search-unit">General Search Unit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-search-unit">Exact Search Unit</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bst-transformer">阿里BST（Transformer处理序列特征）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">模型结构</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer">Embedding Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-layer">Transformer Layer</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-layer">Self-attention layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#point-wise-feed-forward-networks">Point-wise Feed-Forward Networks</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#dropoutlayernorm">残差连接、Dropout、LayerNorm</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#stacking-the-self-attention-blocks">Stacking the self-attention blocks</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-layers-and-loss-function">MLP layers and Loss function</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">实验数据</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dmr-attention">阿里DMR（attention）</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#item-to-item-network">Item-to-Item Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#user-to-item-network">User-to-Item Network</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mind">阿里MIND</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">模型介绍</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">模型整体结构图</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">模型定义</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">模型结构</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-pooling-layer">Embedding &amp; Pooling Layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-interest-extractor-layer">Multi-Interest Extractor Layer</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#label-aware-attention-layer">Label-aware Attention Layer</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-serving">Training &amp; Serving</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yi Longhao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>