{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da866855",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#图模型总结\" data-toc-modified-id=\"图模型总结-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>图模型总结</a></span></li><li><span><a href=\"#常见应用场景：基于图神经网络的CF召回\" data-toc-modified-id=\"常见应用场景：基于图神经网络的CF召回-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>常见应用场景：基于图神经网络的CF召回</a></span></li><li><span><a href=\"#DeepWalk\" data-toc-modified-id=\"DeepWalk-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>DeepWalk</a></span></li><li><span><a href=\"#GCN\" data-toc-modified-id=\"GCN-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>GCN</a></span><ul class=\"toc-item\"><li><span><a href=\"#公式中各项的计算方式\" data-toc-modified-id=\"公式中各项的计算方式-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>公式中各项的计算方式</a></span></li><li><span><a href=\"#公式含义说明\" data-toc-modified-id=\"公式含义说明-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>公式含义说明</a></span></li><li><span><a href=\"#两层的-GCN-网络示意图\" data-toc-modified-id=\"两层的-GCN-网络示意图-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>两层的 GCN 网络示意图</a></span></li></ul></li><li><span><a href=\"#阿里EGES\" data-toc-modified-id=\"阿里EGES-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>阿里EGES</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于用户历史行为构图\" data-toc-modified-id=\"基于用户历史行为构图-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>基于用户历史行为构图</a></span></li><li><span><a href=\"#BGE\" data-toc-modified-id=\"BGE-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>BGE</a></span></li><li><span><a href=\"#GES\" data-toc-modified-id=\"GES-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>GES</a></span></li><li><span><a href=\"#EGES\" data-toc-modified-id=\"EGES-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>EGES</a></span></li><li><span><a href=\"#实验数据\" data-toc-modified-id=\"实验数据-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>实验数据</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据稀疏度\" data-toc-modified-id=\"数据稀疏度-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>数据稀疏度</a></span></li><li><span><a href=\"#离线AUC评估\" data-toc-modified-id=\"离线AUC评估-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>离线AUC评估</a></span></li><li><span><a href=\"#线上CTR对比\" data-toc-modified-id=\"线上CTR对比-5.5.3\"><span class=\"toc-item-num\">5.5.3&nbsp;&nbsp;</span>线上CTR对比</a></span></li></ul></li></ul></li><li><span><a href=\"#GraphSAGE（Transductive-Learning->Inductive-Learning）\" data-toc-modified-id=\"GraphSAGE（Transductive-Learning->Inductive-Learning）-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>GraphSAGE（Transductive Learning-&gt;Inductive Learning）</a></span><ul class=\"toc-item\"><li><span><a href=\"#整体流程\" data-toc-modified-id=\"整体流程-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>整体流程</a></span></li><li><span><a href=\"#Embedding-generation\" data-toc-modified-id=\"Embedding-generation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Embedding generation</a></span></li><li><span><a href=\"#采样\" data-toc-modified-id=\"采样-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>采样</a></span></li><li><span><a href=\"#聚合函数\" data-toc-modified-id=\"聚合函数-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>聚合函数</a></span><ul class=\"toc-item\"><li><span><a href=\"#mean-aggregator\" data-toc-modified-id=\"mean-aggregator-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>mean aggregator</a></span></li><li><span><a href=\"#convolutional-aggergator\" data-toc-modified-id=\"convolutional-aggergator-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>convolutional aggergator</a></span></li><li><span><a href=\"#LSTM-aggregator\" data-toc-modified-id=\"LSTM-aggregator-6.4.3\"><span class=\"toc-item-num\">6.4.3&nbsp;&nbsp;</span>LSTM aggregator</a></span></li><li><span><a href=\"#pooling-aggregator\" data-toc-modified-id=\"pooling-aggregator-6.4.4\"><span class=\"toc-item-num\">6.4.4&nbsp;&nbsp;</span>pooling aggregator</a></span></li></ul></li><li><span><a href=\"#模型训练\" data-toc-modified-id=\"模型训练-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>模型训练</a></span></li><li><span><a href=\"#mini-batch-版本\" data-toc-modified-id=\"mini-batch-版本-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>mini batch 版本</a></span></li><li><span><a href=\"#实验结论\" data-toc-modified-id=\"实验结论-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>实验结论</a></span></li></ul></li><li><span><a href=\"#GAT\" data-toc-modified-id=\"GAT-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>GAT</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c842003",
   "metadata": {},
   "source": [
    "# 推荐系统08-图神经网络\n",
    "\n",
    "## 图模型总结\n",
    "\n",
    "图模型适合数据稀疏的场景，图神经网络允许知识信息在图中进行远距离传递，对于用户行为较少的场景，可以形成知识补充和传递；图模型能够提高embedding表达能力，图网络能够将协同信息、用户行为信息、内容属性信息等各种异质信息在一个统一的框架里进行融合，表征为embedding。\n",
    "\n",
    "\n",
    "## 常见应用场景：基于图神经网络的CF召回\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20230219104210088.png\" alt=\"image-20230219104210088\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "step1: 基于user-item关系的游走产生序列\n",
    "\n",
    "step2: 用 skip-gram + negative sampling 算法学习每个节点的embedding表示\n",
    "\n",
    "step3: 基于 User 向量和 Item 向量分别构建 UCF 召回和 ICF 召回等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c510a",
   "metadata": {},
   "source": [
    "## DeepWalk\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. DeepWalk: Online Learning of Social Representations\n",
    "> 2. [【Graph Embedding】DeepWalk：算法原理，实现和应用](https://zhuanlan.zhihu.com/p/56380812)\n",
    "> 3. [了解图神经网络GNN和2种高级算法「DeepWalk」+ 「GraphSage」](https://easyai.tech/blog/gnn-deepwalk-graphsage/)\n",
    "\n",
    "近年来，**图神经网络 (GNN)** 在各个领域越来越受欢迎，包括社交网络、知识图谱、推荐系统等。\n",
    "\n",
    "图是由两个部件组成的一种数据结构：**顶点(vertices)/节点(nodes)** 和**边 (edges)**。一个图 G 可以用它包含的顶点 V 和边 E 的集合来描述。\n",
    "\n",
    "图又分为**有向图**和**无向图**，这取决于顶点之间是否存在方向依赖关系。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/be57cd90-b55b-4626-a031-72d6f9938215.png\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "介绍 DeepWalk 之前简单描述一下**Graph Embedding 技术**，简单点说 Graph Embedding 技术就是得到图中每个节点的向量表示，之后就可以用得到的向量做各种各样的事情。\n",
    "\n",
    "DeepWalk 的思想类似 word2vec，使用图中节点与节点的共现关系来学习节点的向量表示。那么关键的问题就是如何来描述节点与节点的共现关系，DeepWalk给出的方法是使用随机游走(RandomWalk)的方式在图中进行节点采样。\n",
    "\n",
    "算法包括两个步骤：\n",
    "\n",
    "- step1: 在图中的节点上执行随机游走生成节点序列\n",
    "- step2: 运行skip-gram，根据步骤 1 中生成的节点序列学习每个节点的嵌入\n",
    "\n",
    "在随机游走过程中，下一个节点是从前一节点的邻居统一采样。然后将每个序列截短为长度为`2|w|+1`的子序列，其中 w 表示 skip-gram 中的窗口大小。\n",
    "\n",
    "在论文中，使用分层softmax解决节点数量庞大导致softmax计算成本过高的问题。\n",
    "\n",
    "伪代码：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/4682ce0d-5c5f-4a75-8f3a-1e216baf8ee9.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "DeepWalk 的主要问题是缺乏泛化能力。**每当一个新节点出现时，它必须重新训练模型以表示这个节点**。因此这种 GNN 不适用于图中节点不断变化的动态图（需要解决 embedding 冷启动问题）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54473d22",
   "metadata": {},
   "source": [
    "## GCN\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. [《SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS》](https://arxiv.org/pdf/1609.02907.pdf)\n",
    "> 2. [【图神经网络】GNN从入门到精通](https://www.bilibili.com/video/BV1K5411H7EQ?p=9)\n",
    "\n",
    "GCN 的主要公式：\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(\\tilde{D} ^ {-\\frac{1}{2}} \\tilde{A} \\tilde{D} ^ {-\\frac{1}{2}} H^{(l)} W^{(l)})\n",
    "$$\n",
    "\n",
    "如果去掉 $ \\tilde{D} ^ {-\\frac{1}{2}} \\tilde{A} \\tilde{D} ^ {-\\frac{1}{2}}  $ 部分，GCN 主要公式可以简化为：\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma(H^{(l)} W^{(l)})\n",
    "$$\n",
    "\n",
    "可以看成一个简单的全连接神经网络。\n",
    "\n",
    "### 公式中各项的计算方式\n",
    "\n",
    "假设有一个图：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/2552d9e4-8089-4c4c-82f1-49f3de082a14.png\" alt=\"无权无向图\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "给出公式中每一部分的值，**邻接矩阵 A** 为：\n",
    "\n",
    "$$\n",
    "A = \\left[\n",
    " \\begin{matrix}\n",
    "   0 & 1 & 1 & 0 \\\\\n",
    "   1 & 0 & 1 & 0 \\\\\n",
    "   1 & 1 & 0 & 1 \\\\\n",
    "   0 & 0 & 1 & 0\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "**单位矩阵**$I_N$为：\n",
    "\n",
    "$$\n",
    "I_N = \\left[\n",
    " \\begin{matrix}\n",
    "   1 & 0 & 0 & 0 \\\\\n",
    "   0 & 1 & 0 & 0 \\\\\n",
    "   0 & 0 & 1 & 0 \\\\\n",
    "   0 & 0 & 0 & 1\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "$\\tilde{A} = A + I_N$：\n",
    "\n",
    "$$\n",
    "\\tilde{A} = \\left[\n",
    " \\begin{matrix}\n",
    "   1 & 1 & 1 & 0 \\\\\n",
    "   1 & 1 & 1 & 0 \\\\\n",
    "   1 & 1 & 1 & 1 \\\\\n",
    "   0 & 0 & 1 & 1\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "$\\tilde{A}$ 矩阵按行求和得到 $\\tilde{D}$，即 $\\tilde{D}_{ii} = \\sum_{j} \\tilde{A}_{ij}$：\n",
    "\n",
    "$$\n",
    "\\tilde{D} = \\left[\n",
    " \\begin{matrix}\n",
    "   3 & 0 & 0 & 0 \\\\\n",
    "   0 & 3 & 0 & 0 \\\\\n",
    "   0 & 0 & 4 & 0 \\\\\n",
    "   0 & 0 & 0 & 2\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{D}^{-\\frac{1}{2}} = \\left[\n",
    " \\begin{matrix}\n",
    "   0.577350 & 0 & 0 & 0 \\\\\n",
    "   0 & 0.577350 & 0 & 0 \\\\\n",
    "   0 & 0 & 0.5 & 0 \\\\\n",
    "   0 & 0 & 0 & 0.707107\n",
    "  \\end{matrix}\n",
    "  \\right]\n",
    "$$\n",
    "\n",
    "公式中各项的的计算方式如上，下面用一个更简单的实例介绍公式的实际含义。\n",
    "\n",
    "### 公式含义说明\n",
    "\n",
    "假设有一个图如下，图中每个节点对应一个 embedding 向量。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/fcefb375-9ee6-49f7-8db5-f07e57086917.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220928163412283.png\" alt=\"image-20220928163412283\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220928163433439.png\" alt=\"image-20220928163433439\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "\n",
    "### 两层的 GCN 网络示意图\n",
    "\n",
    "首先计算好 $\\hat{A} = \\tilde{D} ^ {-\\frac{1}{2}} \\tilde{A} \\tilde{D} ^ {-\\frac{1}{2}}$\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/15b780f5-524c-4a4c-804d-d238795bd25b.png\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/6236b37b-2b0a-42f8-841e-3666556ef837.png\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "假设 N 为节点数，则 $\\hat{A}$ 维度为 (N, N)。\n",
    "\n",
    "假设输入层的维度为 (N, C)，第一个隐藏层维度为 (C, H)，第二个隐藏层的输出维度为 (H, F)：\n",
    "\n",
    "第一个隐藏层的输出维度为 (N, H)：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "(N, N)(N, C)(C, H) \\\\\n",
    "= (N, H)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "第二个隐藏层的输出维度为 (N, F)：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "(N, N)(N, H)(H, F) \\\\\n",
    "= (N, N)(N, H)(H, F) \\\\\n",
    "= (N, F)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6081cc8",
   "metadata": {},
   "source": [
    "## 阿里EGES\n",
    "\n",
    "\n",
    "在推荐系统中，有三个主要挑战，分别是可扩展性（scalability）、稀疏性（sparsity）、冷启动问题（cold start）。阿里淘宝团队提出了基于图神经网络的EGES算法来应对这三个挑战。\n",
    "\n",
    "论文题目是《Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba》，通常推荐系统都包含召回、排序等几个阶段，这篇文章主要聚焦召回阶段。\n",
    "\n",
    "文章共涉及到三个模型：\n",
    "\n",
    "- Base Graph Embedding（BGE）\n",
    "- Graph Embedding with Side Information（GES）\n",
    "- Enhanced Graph Embedding with Side Information（EGES）\n",
    "\n",
    "### 基于用户历史行为构图\n",
    "\n",
    "第一步是基于用户历史行为构图，考虑到性能开销，不可能根据用户整个的历史行为构图，其次考虑到用户的兴趣有随着时间迁移的情况，因此在基于用户历史行为构图时，设定了一个时间窗口，只考虑时间窗口内的用户历史行为（session-based users' behaviors）。\n",
    "\n",
    "获取指定时间窗口内的用户历史行为后，例如用户U1的历史行为序列为DAB，对应图中的两条有向边D->A和A->B，如图中的(a)、(b)所示。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/124614fd-0f02-48aa-8061-e4a0670b69e5.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "实际业务场景中存在大量的脏数据，如果不做数据清洗，很可能影响到到模型效果，所以在构图前也做了一些数据清洗工作：\n",
    "\n",
    "- 用户点击后在相关页面停留时间少于1s，则认为点击行为不置信，构图前需要清洗相关的行为。\n",
    "- 3个月内用户累计购买超过1000次或点击超过3500次则认为用户是过度活跃的用户，构图前需要清洗此类用户。\n",
    "- 淘宝上的零售商一直在更新商品的细节，在极端的情况下，同一标识符的商品在淘宝上经过长时间的更新后可能会变成完全不同的商品，构图前需要清洗相关商品的行为。\n",
    "\n",
    "### BGE\n",
    "\n",
    "基于用户历史行为构图，得到一张带权有向图，入上图中的(b)所示，用$\\mathcal{G}=(\\mathcal{V}, \\mathscr{E})$。$M$为图$\\mathcal{G}$的邻接矩阵，其中$M_{ij}$表示图中节点i到节点j的边的权重。首先使用random walk的产出序列，如上图中的(c)所示。最后基于Skip-Gram算法学习每个节点的embedding表示。其中random walk时的转移概率的计算方式如下：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/4881e024-66ba-4f37-97d1-32d6e19a3f23.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "其中$N_{+}(v_i)$表示$v_i$节点的邻居节点集合。\n",
    "\n",
    "### GES\n",
    "\n",
    "BGE模型对于交互行为少商品不太友好，会出现对应商品的embedding向量表示学习不够充分的问题，模型存在比较严重的冷启动问题，为了解决冷启问题，在BGE模型的基础上提出了GES模型。与BGE模型，改进模型的基本思路是为图中的每个节点增加额外信息（Side Information），例如Category、Brand、Price等来强化item的向量表示。\n",
    "\n",
    "我们定义矩阵$W$表示物品及其side information的embedding向量表示，$W_v^0$表示物品$v$自身的向量表示，$W_v^s$表示物品$v$第s类side information的向量表示。\n",
    "\n",
    "最终物品的向量表示为：\n",
    "\n",
    "$$\n",
    "H_v = \\frac{1}{n+1} \\sum_{s=0}^n W_v^s\n",
    "$$\n",
    "\n",
    "### EGES\n",
    "\n",
    "考虑到在计算物品的最终向量时，不同类型的side information的贡献应该是不同的，所以在聚合side information时可以添加attention机制。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/fc2afdfc-2359-4354-9225-c8264561d146.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "定义一个矩阵 $A \\in \\mathbb{R}^{V \\times (n+1)} $，$V$表示的是图中所有节点的数量，n表示side information的类型数，$A_{ij}$表示第i个item的第j类side information的权重，$A_{i0}$表示第i个item自身的权重，依次类推。方便起见使用$a_v^s$表示物品$v$的第$s$类side information的权重。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/06a41077-0dc8-498a-b9a3-e443c0878359.png\" style=\"zoom: 50%; display: block; margin: auto;\" />\n",
    "\n",
    "在这里我们使用$e^{a_v^j}$代替$a_v^j$来确保每个side information的权重都大于0，并使用$\\sum_{j=0}^n e^{a_v^j}$对每个side information的权重做了标准化处理。\n",
    "\n",
    "伪代码：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/7de04266-26e0-4821-bb40-d44f4c163053.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "涉及到的结构与超参：\n",
    "\n",
    "- $\\mathcal{G}=(\\mathcal{V}, \\mathscr{E})$：图结构\n",
    "- S：side information\n",
    "- $w$：每个节点的游走次数(number of walks per node)\n",
    "- $l$：随机游走的序列长度(walk length)\n",
    "- $k$：Skip-Gram算法窗口大小(Skip-Gram window size k)\n",
    "- $\\#ns$：负样本数量(正负样本比，number of negatives samples)\n",
    "- $d$：embedding向量维度(embedding dimension)\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/99afcba2-03cd-491d-96b6-2f9712ac47a9.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "### 实验数据\n",
    "\n",
    "#### 数据稀疏度\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/45c0811d-965f-427f-b520-fd5bb720131e.png\" style=\"zoom: 67%; display: block; margin: auto;\" />\n",
    "\n",
    "计算方式如下：\n",
    "\n",
    "$$\n",
    "1 - \\frac{\\#Edges}{\\#Nodes \\times (\\#Nodes - 1)}\n",
    "$$\n",
    "\n",
    "其中$\\#Edges$表示图中边的数量，$\\#Nodes$表示图中节点数。\n",
    "\n",
    "#### 离线AUC评估\n",
    "\n",
    "离线评估对比了BGE、LINE、GES和EGES的AUC，其中EGES在亚马逊数据集和淘宝的数据集上的表现都是最好的（关于这个评估指标个人或多或少存在一些疑问，召回阶段的模型评估离线AUC是否有实际的参考价值，比较好理解的是评估准召指标，期待与大佬交流学习~）。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/d0e557dd-801e-45c8-bf67-dfa9266d9146.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "#### 线上CTR对比\n",
    "\n",
    "基于淘宝真实的数据流量进行了线上的CTR对比，EGES的表现是最好的。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/826af54f-bd66-4034-a565-431eaec09916.png\" style=\"zoom:50%; display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352e7a9",
   "metadata": {},
   "source": [
    "## GraphSAGE（Transductive Learning->Inductive Learning）\n",
    "\n",
    "在介绍GraphSAGE之前先介绍下直推式学习（Transductive Learning）和归纳学习（Inductive Learning），二者的最大区别在于直推式学习（Transductive Learning）要求在一个确定的图中去学习顶点的embedding，无法直接泛化到在训练过程没有出现过的顶点，而归纳学习（Inductive Learning）则是要求能够利用已知顶点的embedding高效的产生未知顶点的embedding，要求对未知数据有很好的泛化能力。\n",
    "\n",
    "GraphSAGE的核心思想是：GraphSAGE不是学习一个图上所有节点的embedding，而是学习一个为每个节点产生embedding的映射（一组aggregator functions，通过对邻居节点进行聚合产生目标节点的embedding）。\n",
    "\n",
    "### 整体流程\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220825081235384.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "GraphSAGE的整体流程主要包含以上三步：\n",
    "\n",
    "- Sample neighborhood：对每个节点的邻居节点进行**采样**，为每个节点采样固定数量的邻居\n",
    "- Aggregate feature information from neighbors：通过聚合函数**聚合邻居节点包含的信息**\n",
    "- Predict graph context and label using aggregated information：**得到图中各个顶点的向量表示**\n",
    "\n",
    "### Embedding generation\n",
    "\n",
    "GraphSAGE的前向传播算法如下，前向传播描述了如何使用聚合函数对节点的邻居信息进行聚合，从而生成节点embedding：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220825193432068.png\" alt=\"image-20220825193432068\" style=\"zoom:67%; display: block; margin: auto;\" />\n",
    "\n",
    "其中：\n",
    "\n",
    "- $\\mathcal G(\\mathcal V, \\mathcal E)$表示一个图\n",
    "- $x_v$表示节点的向量表示\n",
    "- depth K表示网络的层数，也代表着每个顶点能够聚合到的邻接节点的跳数\n",
    "- $h_u^{k-1}$表示在k−1层中节点$v$的邻居**节点$u$**的向量表示，$\\mathcal N(v)$表示节点$v$的所有邻居节点\n",
    "- $h_{\\mathcal N(v)}^{k}$表示在第k层节点$v$的**所有邻居节点**的向量表示\n",
    "- $h_v^{k}$表示在第k层节点$v$的向量表示\n",
    "\n",
    "假设有一个图如下所示：\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/8b906935-bf00-4ba2-95c5-650f858068bf.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "- 计算节点 1 的相邻节点 $N(1) = \\{ 3, 4, 5, 6 \\}$\n",
    "\n",
    "- 伪代码第 4 步：$h_{N(1)}^1 = AGGREGAGE(\\{h_3^0, h_4^0, h_5^0, h_6^0\\}$，假设这里使用的 AGGREGATE 方法为均值函数(mean)，则 $h_{N(1)}^1 = mean([0.3, 0.4], [0.2, 0.2], [0.7, 0.8], [0.5, 0.6]) = [0.425, 0.5] $\n",
    "\n",
    "- 伪代码第 5 步：$h_1^1 = \\sigma(W^1 \\cdot CONCAT(h_1^0, h_{N(1)}^1)) = \\sigma(W^1 \\cdot [0.1, 0.2, 0.425, 0.5])$\n",
    "\n",
    "\n",
    "### 采样 \n",
    "\n",
    "出于对计算效率的考虑，需要对每个节点的邻居节点进行采样，采样时主要包含两个要素：层数K（跳数）和每一层上每个节点要采样的数量，训练时可以认为每个batch的时空复杂度为$\\prod_{i}^{K}S_i$，实验发现，K不必取很大的值，当K=2时，以及两次扩展的邻居节点总数$S_1 \\cdot S_2$小于等于500时，效果就很好了。\n",
    "\n",
    "由于要对每个顶点采样一定数量的邻居节点，设需要的邻居节点数量为S，若顶点邻居节点数小于S，则采用有放回的抽样方法，直到采样出S个顶点，若顶点邻居数大于S，则采用无放回的抽样。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/c3b99238-867a-4dd3-a4a5-6bc31344cad0.png\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "### 聚合函数\n",
    "\n",
    "这里主要对应红框部分。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220825193825253.png\" alt=\"image-20220825193432068\" style=\"zoom:67%; display: block; margin: auto;\" />\n",
    "\n",
    "#### mean aggregator\n",
    "\n",
    "先对邻居节点向量表示取平均，再与目标节点向量表示拼接。\n",
    "\n",
    "$$\n",
    "h_{\\mathcal N(v)}^{k} \\leftarrow mean(\\{h_u^{k-1}, u \\in N(v)\\})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_v^k \\leftarrow \\sigma (W^k \\cdot CONCAT(h_v^{k-1}, h_{\\mathcal N(v)}^{k})\n",
    "$$\n",
    "\n",
    "#### convolutional aggergator\n",
    "\n",
    "直接对目标节点和邻居节点的向量表示取平均。\n",
    "\n",
    "$$\n",
    "h_v^k \\leftarrow \\sigma (W^k \\cdot mean( \\{h_v^{k-1}\\} \\cup \\{h_u^{k-1}, u \\in N(v)\\}))\n",
    "$$\n",
    "\n",
    "#### LSTM aggregator\n",
    "\n",
    "需要先获得邻居节点的顺序，再将邻居节点的向量表示序列作为LSTM网络的输入。\n",
    "\n",
    "\n",
    "\n",
    "#### pooling aggregator\n",
    "\n",
    "其中max表示element-wise最大值操作，即取对应维度的最大值\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220825200051643.png\" alt=\"image-20220825200051643\" style=\"zoom: 33%; display: block; margin: auto;\" />\n",
    "\n",
    "### 模型训练\n",
    "\n",
    "损失函数倾向于使得相邻节点的向量表示相似度高，互相远离的节点相似度低。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220825200421157.png\" alt=\"image-20220825200421157\" style=\"zoom:33%; display: block; margin: auto;\" />\n",
    "\n",
    "其中：\n",
    "\n",
    "- $z_u$表示节点$u$的向量表示\n",
    "\n",
    "- 节点$v$是节点$u$随机游走到达的邻居\n",
    "\n",
    "- $P_n(v)$表示负采样概率分布\n",
    "\n",
    "- $Q$是负样本的数目\n",
    "\n",
    "### mini batch 版本\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/8950626a-f856-41af-ae7d-5bdd6442b3a5.png\" style=\"zoom: 50%; display: block; margin: auto;\" />\n",
    "\n",
    "假设节点 a 是当前 batch 内的一个节点，可以计算出，a 节点参与训练依赖的的节点集合为：{a, c, f, j, d, e, i, h, k, l}，训练时只需要存储 a 以及训练 a 时依赖的节点，batch 内的其他节点依此类推。GraphSAGE 部分即正常的训练部分。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20221007230600284.png\" alt=\"image-20221007230600284\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "### 实验结论\n",
    "\n",
    "GraphSAGE相对于其他模型指标有较大幅度提升，LSTM aggregator与pooling aggregator效果较好，LSTM aggregator耗时最严重。\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220825201156385.png\" alt=\"image-20220825201156385\" style=\"zoom:30%; display: block; margin: auto;\" />\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20220825201258061.png\" alt=\"image-20220825201258061\" style=\"zoom:30%; display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca474e1",
   "metadata": {},
   "source": [
    "## GAT\n",
    "\n",
    "> **推荐阅读**⭐️⭐️⭐️⭐️⭐️\n",
    "> 1. 原文：《GRAPH ATTENTION NETWORKS》\n",
    "> 2. [向往的GAT(图注意力网络的原理、实现及计算复杂度)](https://zhuanlan.zhihu.com/p/81350196)\n",
    "\n",
    "<img src=\"https://yilonghao-picgo.oss-cn-hangzhou.aliyuncs.com/image-20230219135055379.png\" alt=\"image-20230219135055379\" style=\"zoom:50%; display: block; margin: auto;\" />\n",
    "\n",
    "对邻居节点利用Attention机制来学习贡献度。\n",
    "\n",
    "和所有的attention mechanism一样，GAT的计算也分为两步走：\n",
    "\n",
    "（1）计算注意力系数：$e_{ij} = a([Wh_i||Wh_j]),j\\in \\mathcal N_i$，首先一个共享参数W的线性映射对于顶点的特征进行了增维，当然这是一种常见的特征增强方法。$||$对于顶点$i,j$的变换后的特征进行了拼接（concatenate），最后$a(\\cdot)$把拼接后的高维特征映射到一个实数上，论文中是通过single-layer feedforward neural network实现的。最后通过softmax做归一化得到注意力系数。\n",
    "\n",
    "（2）加权求和：\n",
    "\n",
    "- 根据计算好的注意力系数，把特征加权求和（aggregate）一下：$h_{i}^{'}=\\sigma(\\sum_{j\\in\\mathcal N_i} \\alpha_{ij}Wh_j)$，$h_{i}^{'}$就是GTA输出的对于每个顶点$i$的新特征（融合了邻域信息），$\\sigma(\\cdot)$是激活函数。\n",
    "- 进一步使用multi-head的方式：$h_{i}^{'}(K)=||_{k=1}^K \\sigma(\\sum_{j \\in\\mathcal N{i}} \\alpha_{ij}^k W^{k} h_j) $\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
